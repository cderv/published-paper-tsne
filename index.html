<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Laurens van der Maaten">
<meta name="author" content="Geoffrey Hinton">
<meta name="dcterms.date" content="2023-01-05">
<meta name="keywords" content="visualization, dimensionality reduction, manifold learning, embedding algorithms, multidimensional scaling">
<meta name="description" content="This page is a reworking of the original t-SNE article using the Computo template. It aims to help authors submitting to the journal by using some advanced formatting features. We warmly thank the authors of t-SNE and the editor of JMLR for allowing us to use their work to illustrate the Computo spirit.">

<title>Visualizing Data using t-SNE</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="published-paper-tsne_files/libs/clipboard/clipboard.min.js"></script>
<script src="published-paper-tsne_files/libs/quarto-html/quarto.js"></script>
<script src="published-paper-tsne_files/libs/quarto-html/popper.min.js"></script>
<script src="published-paper-tsne_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="published-paper-tsne_files/libs/quarto-html/anchor.min.js"></script>
<link href="published-paper-tsne_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="published-paper-tsne_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="published-paper-tsne_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="published-paper-tsne_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="published-paper-tsne_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="published-paper-tsne_files/libs/quarto-contrib/pseudocode-1.0.0/pseudocode.min.js"></script>
<link href="published-paper-tsne_files/libs/quarto-contrib/pseudocode-1.0.0/pseudocode.min.css" rel="stylesheet">
<style>
    .quarto-title-block .quarto-title-banner {
      color: #FFFFFF;
background: #034E79;
    }
    </style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body>

<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title"><a href="https://computo.sfds.asso.fr">
        <img src="https://computo.sfds.asso.fr/assets/img/logo_notext_white.png" height="60px">
      </a> &nbsp; Visualizing Data using t-SNE</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> source</button></div></div>
            <p class="subtitle lead">A practical computo example</p>
            <p><a href="http://creativecommons.org/licenses/by/4.0/"><img src="https://i.creativecommons.org/l/by/4.0/80x15.png" alt="Creative Commons BY License"></a>
ISSN 2824-7795</p>
            <div>
        <div class="description">
          <p>This page is a reworking of the original t-SNE article using the Computo template. It aims to help authors submitting to the journal by using some advanced formatting features. We warmly thank the authors of t-SNE and the editor of JMLR for allowing us to use their work to illustrate the Computo spirit.</p>
        </div>
      </div>
                </div>
  </div>
    
    <div class="quarto-title-meta-author">
      <div class="quarto-title-meta-heading">Authors</div>
      <div class="quarto-title-meta-heading">Affiliations</div>
          
          <div class="quarto-title-meta-contents">
        <a href="https://lvdmaaten.github.io/">Laurens van der Maaten</a> <a href="https://orcid.org/0000-0002-1931-6828" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  <a href="https://www.tilburguniversity.edu/">
                  TiCC, Tilburg University
                  </a>
                </p>
            </div>
            <div class="quarto-title-meta-contents">
        <a href="https://www.cs.toronto.edu/~hinton/">Geoffrey Hinton</a> <a href="https://orcid.org/0000-0002-8063-7209" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  <a href="https://web.cs.toronto.edu/">
                  Department of Computer Science, University of Toronto
                  </a>
                </p>
            </div>
        </div>
                    
  <div class="quarto-title-meta">
                                
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 5, 2023</p>
      </div>
    </div>
                                    
      
                  
      <div>
      <div class="quarto-title-meta-heading">Keywords</div>
      <div class="quarto-title-meta-contents">
        <p class="date">visualization, dimensionality reduction, manifold learning, embedding algorithms, multidimensional scaling</p>
      </div>
    </div>
    
    <div>
      <div class="quarto-title-meta-heading">Status</div>
      <div class="quarto-title-meta-contents">
              <a href="https://github.com/computorg/published-paper-tsne"><img src="https://github.com/computorg/published-paper-tsne/workflows/build/badge.svg" alt="build status"></a>
                    <p class="date"></p>
        <a href="https://github.com/computorg/published-paper-tsne/issues/1"><img src="https://img.shields.io/badge/review-report%201-blue" alt="review 1"></a>
        <a href="https://github.com/computorg/published-paper-tsne/issues/2"><img src="https://img.shields.io/badge/review-report%202-blue" alt="review 2"></a>
            </div>
    </div>

  </div>
                                                
  <div>
    <div class="abstract">
    <div class="abstract-title">Abstract</div>
      <p>We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding hinton:stochastic that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualization produced by t-SNE are significantly better than those produced by other techniques on almost all of the data sets. <br></p>
    </div>
  </div>

  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="toc-section-number">1</span>  Introduction</a></li>
  <li><a href="#sec-sne" id="toc-sec-sne" class="nav-link" data-scroll-target="#sec-sne"><span class="toc-section-number">2</span>  Stochastic Neighbor Embedding</a></li>
  <li><a href="#sec-tsne" id="toc-sec-tsne" class="nav-link" data-scroll-target="#sec-tsne"><span class="toc-section-number">3</span>  t-Distributed Stochastic Neighbor Embedding</a>
  <ul class="collapse">
  <li><a href="#sec-symmetric_sne" id="toc-sec-symmetric_sne" class="nav-link" data-scroll-target="#sec-symmetric_sne"><span class="toc-section-number">3.1</span>  Symmetric SNE</a></li>
  <li><a href="#sec-crowding" id="toc-sec-crowding" class="nav-link" data-scroll-target="#sec-crowding"><span class="toc-section-number">3.2</span>  The Crowding Problem</a></li>
  <li><a href="#sec-heavy_tail" id="toc-sec-heavy_tail" class="nav-link" data-scroll-target="#sec-heavy_tail"><span class="toc-section-number">3.3</span>  Mismatched tails can compensate for mismatched dimensionalities</a></li>
  <li><a href="#sec-optimization_methods_for_tsne" id="toc-sec-optimization_methods_for_tsne" class="nav-link" data-scroll-target="#sec-optimization_methods_for_tsne"><span class="toc-section-number">3.4</span>  Optimization methods for t-SNE</a></li>
  </ul></li>
  <li><a href="#sec-experiments" id="toc-sec-experiments" class="nav-link" data-scroll-target="#sec-experiments"><span class="toc-section-number">4</span>  Experiments</a>
  <ul class="collapse">
  <li><a href="#sec-datasets" id="toc-sec-datasets" class="nav-link" data-scroll-target="#sec-datasets"><span class="toc-section-number">4.1</span>  Data Sets</a></li>
  <li><a href="#sec-experimental_setup" id="toc-sec-experimental_setup" class="nav-link" data-scroll-target="#sec-experimental_setup"><span class="toc-section-number">4.2</span>  Experimental Setup</a></li>
  <li><a href="#sec-results" id="toc-sec-results" class="nav-link" data-scroll-target="#sec-results"><span class="toc-section-number">4.3</span>  Results</a></li>
  </ul></li>
  <li><a href="#sec-large-data" id="toc-sec-large-data" class="nav-link" data-scroll-target="#sec-large-data"><span class="toc-section-number">5</span>  Applying t-SNE to Large Data Sets</a></li>
  <li><a href="#sec-discussion" id="toc-sec-discussion" class="nav-link" data-scroll-target="#sec-discussion"><span class="toc-section-number">6</span>  Discussion</a>
  <ul class="collapse">
  <li><a href="#sec-comparison" id="toc-sec-comparison" class="nav-link" data-scroll-target="#sec-comparison"><span class="toc-section-number">6.1</span>  Comparison with Related Techniques</a></li>
  <li><a href="#sec-weakness" id="toc-sec-weakness" class="nav-link" data-scroll-target="#sec-weakness"><span class="toc-section-number">6.2</span>  Weakness</a></li>
  </ul></li>
  <li><a href="#sec-conclusion" id="toc-sec-conclusion" class="nav-link" data-scroll-target="#sec-conclusion"><span class="toc-section-number">7</span>  Conclusions</a></li>
  <li><a href="#appendix-a.-derivation-of-the-t-sne-gradient" id="toc-appendix-a.-derivation-of-the-t-sne-gradient" class="nav-link" data-scroll-target="#appendix-a.-derivation-of-the-t-sne-gradient">Appendix A. Derivation of the t-SNE gradient</a></li>
  <li><a href="#appendix-b.-analytical-solution-to-random-walk-probabilities" id="toc-appendix-b.-analytical-solution-to-random-walk-probabilities" class="nav-link" data-scroll-target="#appendix-b.-analytical-solution-to-random-walk-probabilities">Appendix B. Analytical Solution to Random Walk Probabilities</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">Bibliography</a></li>
  </ul>
</nav>
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">



<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Visualization of high-dimensional data is an important problem in many different domains, and deals with data of widely varying dimensionality. Cell nuclei that are relevant to breast cancer, for example, are described by approximately 30 variables <span class="citation" data-cites="street:nuclear">Street, Wolberg, and Mangasarian (<a href="#ref-street:nuclear" role="doc-biblioref">1993</a>)</span>, whereas the pixel intensity vectors used to represent images or the word-count vectors used to represent documents typically have thousands of dimensions. Over the last few decades, a variety of techniques for the visualization of such high-dimensional data have been proposed, many of which are reviewed by <span class="citation" data-cites="ferreira:visual">Ferreira de Oliveira and Levkowitz (<a href="#ref-ferreira:visual" role="doc-biblioref">2003</a>)</span>. Important techniques include iconographic displays such as Chernoff faces <span class="citation" data-cites="chernoff:use">Chernoff (<a href="#ref-chernoff:use" role="doc-biblioref">1973</a>)</span>, pixel-based techniques <span class="citation" data-cites="keim:designing">Keim (<a href="#ref-keim:designing" role="doc-biblioref">2000</a>)</span>, and techniques that represent the dimensions in the data as vertices in a graph <span class="citation" data-cites="battista:algorithms">Di Battista et al. (<a href="#ref-battista:algorithms" role="doc-biblioref">1994</a>)</span>. Most of these techniques simply provide tools to display more than two data dimensions, and leave the interpretation of the data to the human observer. This severely limits the applicability of these techniques to real-world data sets that contain thousands of high-dimensional datapoints.</p>
<p>In contrast to the visualization techniques discussed above, dimensionality reduction methods convert the high-dimensional data set <span class="math inline">\mathcal{X} = {x_1, x_2, \dots, x_n}</span> into two or three-dimensional data <span class="math inline">\mathcal{Y} = {y_1, y_2, \dots, y_n}</span> that can be displayed in a scatterplot. In the paper, we refer to the low-dimensional data representation <span class="math inline">\mathcal{Y}</span> as a map, and to the low-dimensional representations <span class="math inline">y_i</span> of individual datapoints as map points. The aim of dimensionality reduction is to preserve as much of the significant structure of the high-dimensional data as possible in the low-dimensional map. Various techniques for this problem have been proposed that differ in the type of structure they preserve. Traditional dimensionality reduction techniques such as Principal Components Analysis <span class="citation" data-cites="hotelling:analysis">Hotelling (<a href="#ref-hotelling:analysis" role="doc-biblioref">1933</a>)</span> and classical multidimensional scaling <span class="citation" data-cites="torgerson:multidimensional">Torgerson (<a href="#ref-torgerson:multidimensional" role="doc-biblioref">1952</a>)</span> are linear techniques that focus on keeping the low-dimensional representations of dissimilar datapoints far apart. For high-dimensional data that lies on or near a low-dimensional, non-linear manifold it is usually more important to keep the low-dimensional representations of very similar datapoints close together, which is typically not possible with a linear mapping.</p>
<p>A large number of nonlinear dimensionality reduction techniques that aim to preserve the local structure of data have been proposed, many of which are reviewed by <span class="citation" data-cites="lee:nonlinear">John A. Lee and Verleysen (<a href="#ref-lee:nonlinear" role="doc-biblioref">2007</a>)</span>. In particular, we mention the following seven techniques: (1) Sammon mapping <span class="citation" data-cites="sammon:nonlinear">Sammon (<a href="#ref-sammon:nonlinear" role="doc-biblioref">1969</a>)</span>, (2) curvilinear components analysis <span class="citation" data-cites="demartines:curvilinear">Demartines and Herault (<a href="#ref-demartines:curvilinear" role="doc-biblioref">1997</a>)</span>, (3) Stochastic Neighbor Embedding <span class="citation" data-cites="hinton:stochastic">Hinton and Roweis (<a href="#ref-hinton:stochastic" role="doc-biblioref">2003</a>)</span>; (4) Isomap <span class="citation" data-cites="tenenbaum:global">Tenenbaum, Silva, and Langford (<a href="#ref-tenenbaum:global" role="doc-biblioref">2000</a>)</span>, (5) Maximum Variance Unfolding <span class="citation" data-cites="weinberger:learning">Kilian Q. Weinberger, Sha, and Saul (<a href="#ref-weinberger:learning" role="doc-biblioref">2004</a>)</span>; (6) Locally Linear Embedding <span class="citation" data-cites="roweis:nonlinear">Roweis and Saul (<a href="#ref-roweis:nonlinear" role="doc-biblioref">2000</a>)</span>, and (7) Laplacian Eigenmaps <span class="citation" data-cites="belkin:laplacian">Belkin and Niyogi (<a href="#ref-belkin:laplacian" role="doc-biblioref">2001</a>)</span>. Despite the strong performance of these techniques on artificial data sets, they are often not very successful at visualizing real, high-dimensional data. In particular, most of the techniques are not capable of retaining both the local and the global structure of the data in a single map. For instance, a recent study reveals that even a semi-supervised variant of MVU is not capable of separating handwritten digits into their natural clusters <span class="citation" data-cites="song:colored">Song et al. (<a href="#ref-song:colored" role="doc-biblioref">2008</a>)</span>.</p>
<p>In this paper, we describe a way of converting a high-dimensional data set into a matrix of pairwise similarities and we introduce a new technique, called “t-SNE”, for visualizing the resulting similarity data. t-SNE is capable of capturing much of the local structure of the high-dimensional data very well, while also revealing global structure such as the presence of clusters at several scales. We illustrate the performance of t-SNE by comparing it to the seven dimensionality reduction techniques mentioned above on five data sets from a variety of domains. Because of space limitations, most of the <span class="math inline">(7+1)\times5=40</span> maps are presented in the supplemental material, but the maps that we present in the paper are sufficient to demonstrate the superiority of t-SNE.</p>
<p>The outline of the paper is as follows. In <a href="#sec-sne">Section&nbsp;2</a>, we outline SNE as presented by <span class="citation" data-cites="hinton:stochastic">Hinton and Roweis (<a href="#ref-hinton:stochastic" role="doc-biblioref">2003</a>)</span>, which forms the basis for t-SNE. In <a href="#sec-tsne">Section&nbsp;3</a>, we present t-SNE, which has two important differences from SNE. In Section <a href="#sec-experiments">Section&nbsp;4</a>, we describe the experimental setup and the results of our experiments. Subsequently, <a href="#sec-large-data">Section&nbsp;5</a> shows how t-SNE can be modified to visualize real-world data sets that contain many more than 10,000 datapoints. The results of our experiments are discussed in more detail in <a href="#sec-discussion">Section&nbsp;6</a>. Our conclusions and suggestions for future work are presented in <a href="#sec-conclusion">Section&nbsp;7</a>.</p>
</section>
<section id="sec-sne" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Stochastic Neighbor Embedding</h1>
<p>Stochastic Neighbor Embedding (SNE) starts by converting the high-dimensional Euclidean distances between datapoints into conditional probabilities that represent similarities.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> The similarity of datapoint <span class="math inline">x_j</span> to datapoint <span class="math inline">x_i</span> is the conditional probabilities, <span class="math inline">p_{j|i}</span>, that <span class="math inline">x_i</span> would pick <span class="math inline">x_j</span> as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian centered at <span class="math inline">x_i</span>. For nearby datapoints, <span class="math inline">p_{j|i}</span> is relatively high, whereas for widely separated datapoints, <span class="math inline">p_{j|i}</span> will be almost infinitesimal (for reasonable values of the variance of the Gaussian, <span class="math inline">\sigma_i</span>). Mathematically, the conditional probability <span class="math inline">p_{j|i}</span> is given by</p>
<p><span id="eq-sne_large_space"><span class="math display">
p_{j|i} = \frac{\exp(-\|x_i - x_j\|^2/(2\sigma_i^2))}{\sum_{k\neq i} \exp(-\|x_i
- x_k\|^2 / 2 \sigma_i^2)}\,.
\tag{1}</span></span></p>
<p>where <span class="math inline">\sigma_i</span> is the variance of the Gaussian that is centered on datapoint <span class="math inline">x_i</span>. The method for determining the value of <span class="math inline">\sigma_i</span> is presented later in this section. Because we are only interested in modeling pairwise similarities, we set the value of <span class="math inline">p_{i|i}</span> to zero. For the low-dimensional counterparts <span class="math inline">y_i</span> and <span class="math inline">y_j</span> of the high-dimensional datapoints <span class="math inline">x_i</span> and <span class="math inline">x_j</span>, it is possible to compute a similar conditional probability, which we denote by <span class="math inline">q_{j|i}</span>. We set <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> the variance of the Gaussian that is employed in the computation of the conditional probabilities <span class="math inline">q_{j|i}</span> to <span class="math inline">\frac{1}{\sqrt{2}}</span>. Hence, we model the similarity of a map point <span class="math inline">y_j</span> to map point <span class="math inline">y_i</span> by</p>
<p><span class="math display">
q_{j|i} = \frac{\exp(-\|y_i - y_j\|^2)}{\sum_{k \neq i} \exp(-\|y_i
-y_k\|^2)}\,.
</span></p>
<p>Again, since we are only interested in modeling pairwise similarities, we set <span class="math inline">q_{i|i}=0</span>.</p>
<p>If the map points <span class="math inline">y_i</span> and <span class="math inline">y_j</span> correctly model the similarity between the high-dimensional data-points <span class="math inline">x_i</span> and <span class="math inline">x_j</span>, the conditional probabilities <span class="math inline">p_{j|i}</span> and <span class="math inline">q_{j|i}</span> will be equal. Motivated by this observation, SNE aims to find a low-dimensional data representation that minimizes the mismatch between <span class="math inline">p_{j|i}</span> and <span class="math inline">q_{j|i}</span>. A natural measure of the faithfulness with which <span class="math inline">q_{j|i}</span> models <span class="math inline">p_{j|i}</span> is the Kullback-Leibler divergence (which is in the case equal to the cross-entropy up to an additive constant). SNE minimizes the sum of Kullback-Leibler divergences over all datapoints using a gradient descent method. The cost function <span class="math inline">C</span> is given by</p>
<p><span id="eq-sne_cost_function"><span class="math display">
C = \sum_i KL(P_i\|Q_i) = \sum_i \sum_j p_{j|i} \log
\frac{p_{j|i}}{q_{j|i}}\,,
\tag{2}</span></span></p>
<p>in which <span class="math inline">P_i</span> represents the conditional probability distribution over all other datapoints given datapoint <span class="math inline">x_i</span>, and <span class="math inline">Q_i</span> represents the conditional probability distribution over all other map points given map point <span class="math inline">y_i</span>. Because the Kullback-Liebler divergence is not symmetric, different types of error in the pairwise distances in the low-dimensional map are not weighted equally. In particular, there is a large cost for using widely separated map points to represent nearby datapoints (i.e, for using a small <span class="math inline">q_{j|i}</span> to model a large <span class="math inline">p_{j|i}</span>), but there is only a small cost for using nearby map points to represent widely separated datapoints. This small cost comes from wasting some of the probability mass in the relevant <span class="math inline">Q</span> distributions. In other words, the SNE cost function focuses on retaining the local structure of the data in the map (for reasonable values of the variance of the Gaussian in the high-dimensional space, <span class="math inline">\sigma_i</span>).</p>
<p>The remaining parameter to be selected the variance <span class="math inline">\sigma_i</span> of the Gaussian that is centered over each high-dimensional datapoint, <span class="math inline">x_i</span>. It is not likely that there is a single value of <span class="math inline">\sigma_i</span> that is optimal for all datapoints in the data set because the density of the data is likely to vary. In dense regions, a smaller value of <span class="math inline">\sigma_i</span> is usually more appropriate than in sparser regions. Any particular value of <span class="math inline">\sigma_i</span> induces a probability distribution, <span class="math inline">P_i</span>, over all of the other datapoints. This distribution has an entropy which increases as <span class="math inline">\sigma_i</span> increases. SNE performs a binary search for the value of <span class="math inline">\sigma_i</span> that produces a <span class="math inline">P_i</span> with a fixed perplexity that is specified by the user<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. The perplexity is defined as</p>
<p><span class="math display">
Perp(P_i) = 2^{H(P_i)}\,,
</span></p>
<p>where <span class="math inline">H(P_i)</span> is the Shannon entropy of <span class="math inline">P_i</span> measured in bits</p>
<p><span class="math display">
H(P_i) = - \sum_j p_{j|i} \log_2 p_{j|i}\,.
</span></p>
<p>The perplexity can be interpreted as a smooth measure of the effective number of neighbors. The performance of SNE is fairly robust to changes in the perplexity, and typical values are between 5 and 50.</p>
<p>The minimization of the cost function in <a href="#eq-sne_cost_function">Equation&nbsp;2</a> is performed using a gradient descent method. The gradient has a surprisingly simple form</p>
<p><span class="math display">
\frac{\partial C}{\partial y_i} = 2 \sum_j (p_{j|i} - q_{j|i} + p_{i|j} -
q_{i|j})(y_i - y_j)\,.
</span></p>
<p>Physically, the gradient may be interpreted as the resultant force created by a set of springs between the map point <span class="math inline">y_i</span> and all other map points <span class="math inline">y_j</span>. All springs exert a force along the direction <span class="math inline">(y_i - y_j)</span>. The spring between <span class="math inline">y_i</span> and <span class="math inline">y_j</span> repels or attracts the map points depending on whether the distance between the two in the map is too small or too large to represent the similarities between the two high-dimensional datapoints. The force exerted by the spring between <span class="math inline">y_i</span> and <span class="math inline">y_j</span> is proportional to its length, and also proportional to its stiffness, which is the mismatch <span class="math inline">(p_{j|i} - q_{j|i} + p_{i|j} + q_{i|j})</span> between the pairwise similarities of the data points.</p>
<p>The gradient descent is initialized by sampling map points randomly from an isotropic Gaussian with small variance that is centered around the origin. In order to speed up the optimization and to avoid poor local minima, a relatively large momentum term is added to the gradient. In other words, the current gradient is added to an exponentially decaying sum of previous gradients in order to determine the changes in the coordinates of the map points at each iteration of the gradient search. Mathematically, the gradient update with a momentum term is given by</p>
<p><span class="math display">
\mathcal{Y}^{(t)} = \mathcal{Y}^{(t-1)} + \eta \frac{\partial C}{\partial
\mathcal{Y}} + \alpha(t) \left( \mathcal{Y}^{(t-1)} - \mathcal{Y}^{(t-2)}\right)\,,
</span></p>
<p>where <span class="math inline">\mathcal{Y}^{(t)}</span> indicates the solution at iteration <span class="math inline">t</span>, <span class="math inline">\eta</span> indicates the learning rate, and <span class="math inline">\alpha(t)</span> represents the momentum at iteration <span class="math inline">t</span>.</p>
<p>In addition, in the early stages of the optimization, Gaussian noise is added to the map points after each iteration. Gradually reducing the variance of this noise performs a type of simulated annealing that helps the optimization to escape from poor local minima in the cost function. If the variance of the noise changes very slowly at the critical point at which the global structure of the map starts to form, SNE tends to find maps with a better global organization. Unfortunately, this requires sensible choices of the initial amount of Gaussian noise and the rate at which it decays. Moreover, these choices interact with the amount of momentum and the step size that are employed in the gradient descent. It is therefore common to run the optimization several times on a data set to find appropriate values for the parameters.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> In this respect, SNE is inferior to methods that allow convex optimization and it would be useful to find an optimization method that gives good results without requiring the extra computation time and parameter choices introduced by the simulated annealing.</p>
</section>
<section id="sec-tsne" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> t-Distributed Stochastic Neighbor Embedding</h1>
<p><a href="#sec-sne">Section&nbsp;2</a> discussed SNE as it was presented by <span class="citation" data-cites="hinton:stochastic">Hinton and Roweis (<a href="#ref-hinton:stochastic" role="doc-biblioref">2003</a>)</span>. Although SNE constructs reasonably good visualizations, it is hampered by a cost function that is difficult to optimize and by a problem we refer to as the “crowding problem.” In this section, we present a new technique called “t-Distributed Stochastic Neighbor Embedding” or “t-SNE” that aims to alleviate these problems. The cost function used by t-SNE differs from the one used by SNE in two ways: (1) it uses a symmetrized version of the SNE cost function with simpler gradients that was briefly introduced by <span class="citation" data-cites="cook:visualizing">Cook et al. (<a href="#ref-cook:visualizing" role="doc-biblioref">2007</a>)</span> and (2) it uses a Student-t distribution rather than a Gaussian to compute the similarity between two points <em>in the low dimensional space</em>. t-SNE employs a heavy-tailed distribution in the low-dimensional space to alleviate both the crowding problem and the optimization problems of SNE.</p>
<p>In this section, we first discuss the symmetric version of SNE (<a href="#sec-symmetric_sne">Section&nbsp;3.1</a>). Subsequently, we discuss the crowding problem (<a href="#sec-crowding">Section&nbsp;3.2</a>), and the use of heavy-tailed distributions to address this problem (<a href="#sec-heavy_tail">Section&nbsp;3.3</a>). We conclude the by describing our approach to the optimization of the t-SNE cost function (<a href="#sec-optimization_methods_for_tsne">Section&nbsp;3.4</a>).</p>
<section id="sec-symmetric_sne" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="sec-symmetric_sne"><span class="header-section-number">3.1</span> Symmetric SNE</h2>
<p>As an alternative to minimizing the sum of the Kullback-Leibler divergences between the conditional probabilities <span class="math inline">p_{j|i}</span> and <span class="math inline">q_{j|i}</span>, it is also possible to minimize a single Kullback-Leibler divergence between a joint probability distribution, <span class="math inline">P</span>, in the high-dimensional space and a joint probability distribution, <span class="math inline">Q</span>, in the low-dimensional space:</p>
<p><span class="math display">
C = KL(P\|Q) = \sum_i \sum_j p_{ij} \log \frac{p_{ij}}{q_{ij}}\,,
</span></p>
<p>where again, we set <span class="math inline">p_{ij}</span> and <span class="math inline">q_{ii}</span> to zero. We refer to this type of SNE as symmetric SNE, because it has the property that <span class="math inline">p_{ij} = p_{ji}</span> and <span class="math inline">q_{ij} = q_{ji}</span> for all <span class="math inline">i, j</span>. In symmetric SNE, the pairwise similarities in the low-dimensional map <span class="math inline">q_{ij}</span> are given by</p>
<p><span id="eq-pairwise_similarities"><span class="math display">
q_{ij} = \frac{\exp(-\|x_i - x_j\|^2)}{\sum_{k\neq l} \exp(-\|x_k -
x_l\|^2)}\,.
\tag{3}</span></span></p>
<p>The obvious way to define the pairwise similarities in the high-dimensional space <span class="math inline">p_{ij}</span> is</p>
<p><span class="math display">
p_{ij} = \frac{\exp(-\|x_i - x_j\|^2/2\sigma^2)}{\sum_{k\neq l} \exp(-\|x_k -
x_l\|^2/2\sigma^2)}\,
</span></p>
<p>but this causes problems when a high-dimensional datapoint <span class="math inline">x_i</span> is an outlier (i.e., all pairwise distances <span class="math inline">\|x_i - x_j\|^2</span> are large for <span class="math inline">x_i</span>). For such an outlier, the values of <span class="math inline">p_{ij}</span> are extremely small for all <span class="math inline">j</span>, so the location of its low-dimensional map point <span class="math inline">y_i</span> has very little effect on the cost function. As a result, the position of the map point is not well determined by the positions of the other map points. We circumvent this problem by defining the joint probabilities <span class="math inline">p_{ij}</span> in the high dimensional space to be symmetrized conditional probabilities, that is, we set <span class="math inline">p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}</span>. This ensures that <span class="math inline">\sum_j p_{ij} &gt; \frac{1}{2n}</span> for all datapoints <span class="math inline">x_i</span>, as a result of which each datapoint <span class="math inline">x_i</span> makes a significant contribution to the cost function. In the low-dimensional space, symmetric SNE simply uses <a href="#eq-pairwise_similarities">Equation&nbsp;3</a>. The main advantage of the symmetric version of SNE is the simpler form of its gradient, which is faster to compute. The gradient of symmetric SNE is fairly similar to that of asymmetric SNE, and is given by</p>
<p><span class="math display">
\frac{\partial C}{\partial y_i} = 4 \sum_j (p_{ij} - q_{ij})(y_i - y_j)\,.
</span></p>
<p>In preliminary experiments, we observed that symmetric SNE seems to produce maps that are just as good as asymmetric SNE, and sometimes even a little better.</p>
</section>
<section id="sec-crowding" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="sec-crowding"><span class="header-section-number">3.2</span> The Crowding Problem</h2>
<p>Consider a set of datapoints that lie on a two-dimensional curved manifold which is approximately linear on a small scale, and which is embedded within a higher-dimensional space. It is possible to model the small pairwise distances between datapoints fairly well in a two-dimensional map, which is often illustrated on toy examples such as the “Swiss roll” data set. Now suppose that the manifold has ten intrinsic dimensions[^dataset] and is embedded within a space of much higher dimensionality. There are several reasons why the pairwise distances in a two-dimensional map cannot faithfully model distances between points on the ten-dimensional manifold. For instance, in ten dimensions, it is possible to have 11 datapoints that are mutually equidistant and there is no way to model this faithfully in a two-dimensional map. A related problem is the very different distribution of pairwise distances in the two spaces. The volume of a sphere centered on datapoint <span class="math inline">i</span> scales as <span class="math inline">r^m</span>, where <span class="math inline">r</span> is the radius and <span class="math inline">m</span> the dimensionality of the sphere. So if the datapoints are approximately uniformly distributed in the region around <span class="math inline">i</span> on the ten-dimensional manifold, we get the following “crowding problem:” the area of the two-dimensional map that is available to accommodate moderately distant datapoints will not be nearly large enough compared with the area available to accommodate nearby datapoints. Hence, if we want to model the small distances accurately in the map, most of the points that are at a moderate distance from datapoint <span class="math inline">i</span> will have to be placed much too far away in the two-dimensional map. In SNE, the spring connecting datapoint <span class="math inline">i</span> to each of these too-distant map points will thus exert a very small attractive force. Although these attractive forces are very small, the very large number of such forces crushes together the points in the center of the map, which prevents gaps from forming between the natural clusters. Note that the crowding problem is not specific to SNE, but that it occurs in other local techniques for multidimensional scaling such as Sammon mapping.</p>
<p>An attempt to address the crowding problem by adding a slight repulsion to all springs was presented by <span class="citation" data-cites="cook:visualizing">Cook et al. (<a href="#ref-cook:visualizing" role="doc-biblioref">2007</a>)</span>. The slight repulsion is created by introducing a uniform background model with a small mixing proportion, <span class="math inline">\rho</span>. So however far apart two map points are, <span class="math inline">q_{ij}</span> can never fall below <span class="math inline">\frac{2\rho}{n(n-1)}</span> (because the uniform background distribution is over <span class="math inline">n(n-1)/2</span> pairs). As a result, for datapoints that are far apart in the high-dimensional space, <span class="math inline">q_{ij}</span> will always be larger than <span class="math inline">p_{ij}</span>, leading to a slight repulsion. This technique is called UNI-SNE and although it usually outperforms standard SNE, the optimization of the UNI-SNE cost function is tedious. The best optimization method known is to start by setting the background mixing proportion to zero (i.e., by performing standard SNE). Once the SNE cost function has been optimized using simulated annealing, the background mixing proportion can be increased to allow some gaps to form between natural clusters as shown by <span class="citation" data-cites="cook:visualizing">Cook et al. (<a href="#ref-cook:visualizing" role="doc-biblioref">2007</a>)</span>. Optimizing the UNI-SNE cost function directly does not work because two map points that are far apart will get almost all of their <span class="math inline">q_{i}</span> from the uniform background. So even if their <span class="math inline">p_{ij}</span> is large, there will be no attractive force between them, because a small change in their separation will have a vanishingly small <em>proportional</em> effect on <span class="math inline">q_{ij}</span>. This means that if two parts of a cluster get separated early on in the optimization, there is no force to pull them back together.</p>
</section>
<section id="sec-heavy_tail" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="sec-heavy_tail"><span class="header-section-number">3.3</span> Mismatched tails can compensate for mismatched dimensionalities</h2>
<p>Since symmetric SNE is actually matching the joint probabilities of pairs of datapoints in the high-dimensional and the low-dimensional spaces rather than their distances, we have a natural way of alleviating the crowing problem that works as follows. In the high-dimensional space, we convert distances into probabilities using a Gaussian distribution. In the low-dimensional map, we can use a probability distribution that has a much heavier tails than a Gaussian to convert distances into probabilities. This allows a moderate distance in the high-dimensional space to be faithfully modeled by a much larger distance in the map and, as a result, it eliminates the unwanted attractive forces between map points that represent moderately dissimilar datapoints.</p>
<p>In t-SNE, we employ a Student <span class="math inline">t</span>-distribution with a single degree of freedom (which is the same as a Cauchy distribution) as the heavy-tailed distribution in the low-dimensional map. Using this distribution, the joint probabilities <span class="math inline">q_{ij}</span> are defined as</p>
<p><span id="eq-joint_probabilities"><span class="math display">
q_{ij} = \frac{(1+\|y_i - y_j\|^2)^{-1}}{\sum_{k \neq l}(1+\|y_k -
y_t\|^2)^{-1}}
\tag{4}</span></span></p>
<p>We use a Student t-distribution with a single degree of freedom, because it has the particularly nice property that <span class="math inline">\left(1+\|y_i - y_j\|^2\right)^{-1}</span> approaches an inverse square law for large pairwise distances <span class="math inline">\|y_i - y_j\|</span> in the low-dimensional map. This makes the map’s representation of joint probabilities (almost) invariant to changes in the scale of the map for map points that are far apart. It also means that large clusters of points that are far apart interact in just the same way as individual points, so the optimization operates in the same way at all but the finest scales. A theoretical justification for our selection of the Student <span class="math inline">t</span>-distribution is that it is closely related to the Gaussian distribution, as the Student <span class="math inline">t</span>-distribution is an infinite mixture of Gaussians. A computationally convenient property is that it is much faster to evaluate the density of a point under a Student <span class="math inline">t</span>-distribution than under a Gaussian because it does not involve an exponential, even though the Student <span class="math inline">t</span>-distribution is equivalent to an infinite mixture of Gaussians with different variances.</p>
<p>The gradient of the Kullback-Leibler divergence between <span class="math inline">P</span> and the Student-<span class="math inline">t</span> based joint probability distribution <span class="math inline">Q</span> (computed using <a href="#eq-joint_probabilities">Equation&nbsp;4</a>) is derived in Appendix A, and is given by</p>
<p><span id="eq-gradient-tsne"><span class="math display">
\frac{\partial C}{\partial y_i} = 4 \sum_j (p_{ij} - q_{ij})(y_i -
y_j)(1+\|y_i - y_j\|^2)^{-1}\,.
\tag{5}</span></span></p>
<div id="fig-gradients" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/gradients.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Gradients of three types of SNE as a function of the pairwise Euclidean distance between two points in the high-dimensional and the pairwise distance between the points in the low-dimensional data representation.</figcaption><p></p>
</figure>
</div>
<p>In <a href="#fig-gradients">Figure&nbsp;1</a>, we show the gradients between the low-dimensional datapoints <span class="math inline">y_i</span> and <span class="math inline">y_j</span> as a function of their pairwise Euclidean distances in the high-dimensional and the low-dimensional space (i.e., as a function of <span class="math inline">\|x_i-x_j\|</span> and <span class="math inline">\|y_i-y_j\|</span>) for the symmetric versions of SNE, UNI-SNE, and t-SNE. In the figures, positive values of the gradient represent an attraction between the low-dimensional datapoints <span class="math inline">y_i</span> and <span class="math inline">y_j</span>, whereas negative values represent a repulsion between the two datapoints. From the figures, we observe two main advantages of the t-SNE gradient over the gradients of SNE and UNI-SNE.</p>
<p>First, the t-SNE gradient strongly repels dissimilar datapoints that are modeled by a small pairwise distance in the low-dimensional representation. SNE has such repulsion as well, but its effect is minimal compared to the strong attractions elsewhere in the gradient (the largest attraction in our graphical representation of the gradient is approximately 19, whereas the largest repulsion is approximately 1). In UNI-SNE, the amount of repulsion between dissimilar datapoints is slightly larger, however, this repulsion is only strong when the pairwise distance between the points in the low-dimensional representation is already large (which is often not the case, since the low-dimensional representation is initialized by sampling from a Gaussian with a very small variance that is centered around the origin).</p>
<p>Second, although t-SNE introduces strong repulsions between dissimilar datapoints that are modeled by small pairwise distances, these repulsions do not go to infinity. In this respect, t-SNE differs from UNI-SNE, in which the strength of the repulsion between very dissimilar datapoints is proportional to their pairwise distance in the low-dimensional map, which may cause dissimilar datapoints to move much too far away from each other.</p>
<p>Taken together, t-SNE puts emphasis on (1) modeling dissimilar datapoints by means of large pairwise distances, and (2) modeling similar datapoints by means of small pairwise distances. Moreover, as a result of these characteristics of the t-SNE cost function (and as a result of the approximate scale invariance of the Student t-distribution), the optimization of the t-SNE cost function is much easier than the optimization of the cost functions of SNE and UNI-SNE. Specifically, t-SNE introduces long-range forces in the low-dimensional map that can pull back together two (clusters of) similar points that get separated early on in the optimization. SNE and UNI-SNE do not have such long-range forces, as a result of which SNE and UNI-SNE need to use simulated annealing to obtain reasonable solutions. Instead, the long-range forces in t-SNE facilitate the identification of good local optima without resorting to simulated annealing</p>
<pre class="pseudocode"><code>\begin{algorithm}
\caption{Simple version of t-Distributed Stochastic Neighbor Embeding}
\begin{algorithmic}
\State \textbf{Data}: high-dimensional representation $\mathcal{X} = \{x_1, \dots, x_n\}$
\State cost function parameters: perplexity  $Perp$
\State optimization parameters: number of iterations $T$, learning rate $\eta$, momentum $\alpha(t)$
\State \textbf{Result}: low-dimensional data representation $\mathcal{Y}^{(T)} = \{y_1, \dots, y_n\}$
\Procedure{t-sne}{$Perp, T, \eta, \alpha(t)$}
    \State compute pairwise affinities $p_{j|i}$ with perplexity $Perp$ (using Equation 1)
    \State set $p_{ij} = \frac{1}{2n} (p_{j|i} + p_{i|j})$
    \State sample initial solution $\mathcal{Y}^{(0)} = \{y_1,\dots,y_n\}$ from $\mathcal{N}(0,1e^{-4} I)$
    \For{$t = 0$\dots$T$}
        \State compute low-dimensional affinities $q_{ij}$ (using Equation 4)
        \State compute gradient $\frac{\partial C}{\partial \mathcal{Y}}$ (using Equation 5)
        \State et $\mathcal{Y}^{t} = \mathcal{Y}^{t-1} + \eta \frac{\partial C}{\partial \mathcal{Y}} + \alpha(t) \left(\mathcal{Y}^{t-1} - \mathcal{Y}^{t-2}\right)$ 
    \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}</code></pre>
</section>
<section id="sec-optimization_methods_for_tsne" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="sec-optimization_methods_for_tsne"><span class="header-section-number">3.4</span> Optimization methods for t-SNE</h2>
<p>We start by presenting a relatively simple, gradient descent procedure for optimizing the t-SNE cost function. This simple procedure uses a momentum term to reduce the number of iterations required and it works best if the momentum term is small until the map points have become moderately well organized. Pseudocode for this simple algorithm is presented in Algorithm 1 (FIXME: ref not working). The simple algorithm can be sped up using the adaptive learning rate scheme that is described by <span class="citation" data-cites="jacobs:rates">Jacobs (<a href="#ref-jacobs:rates" role="doc-biblioref">1988</a>)</span>, which gradually increases the learning rate in directions in which the gradient is stable.</p>
<p>Although the simple algorithm produces visualizations that are often much better than those produced by other non-parametric dimensionality reduction techniques, the results can be improved further by using either of two tricks. The first trick, which we call “early compression,” is to force the map points to stay close together at the start of the optimization. When the distances between map points are small, it is easy for clusters to move through one another so it is much easier to explore the space of possible global organizations of the data. Early compression is implemented by adding an additional L2-penalty to the cost function that is proportional to the sum of squared distances of the map points from the origin. The magnitude of this penalty term and the iteration at which it is removed are set by hand, but the behavior is fairly robust across variations in these two additional optimization parameters.</p>
<p>A less obvious way to improve the optimization, which we call “early exaggeration,” is to multiply all of the <span class="math inline">p_{ij}</span>’s by, for example, 4, in the initial stages of the optimization. This means that almost all of the <span class="math inline">q_{ij}</span>’s, which still add up to 1, are much too small to model their corresponding <span class="math inline">p_{ij}</span>’s. As a result, the optimization is encouraged to focus on modeling the large <span class="math inline">p_{ij}</span>’s by fairly large <span class="math inline">q {ij}</span>’s. The effect is that the natural clusters in the data tend to form tight widely separated clusters in the map. This creates a lot of relatively empty space in the map, which makes it much easier for the clusters to move around relative to one another in order to find a good global organization.</p>
<p>In all the visualizations presented in this paper and in the supporting material, we used exactly the same optimization procedure. We used the early exaggeration method with an exaggeration of 4 for the first 50 iterations (note that early exaggeration is not included in the pseudocode in Algorithm 1). The number of gradient descent iterations <span class="math inline">T</span> was set 1000, and the momentum term was set to <span class="math inline">\alpha^{(t)} = 0.5</span> for <span class="math inline">t&lt;250</span> and <span class="math inline">\alpha^{(t)}=0.8</span> for <span class="math inline">t \geq 250</span>. The learning rate <span class="math inline">\eta</span> is initially set to 100 and it is updated after every iteration by means of the adaptive learning rate scheme described by <span class="citation" data-cites="jacobs:rates">Jacobs (<a href="#ref-jacobs:rates" role="doc-biblioref">1988</a>)</span>. A Matlab implementation of the resulting algorithm is available at <a href="https://lvdmaaten.github.io/tsne/" class="uri">https://lvdmaaten.github.io/tsne/</a>.</p>
</section>
</section>
<section id="sec-experiments" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Experiments</h1>
<p>To evaluate t-SNE, we present experiments in which t-SNE is compared to seven other non-parametric techniques for dimensionality reduction. Because of space limitations, in the paper, we only compare t-SNE with: (1) Sammon mapping, (2) Isomap, and (3) LLE. In the supporting material, we also compare t-SNE with: (4) CCA, (5) SNE, (6) MVU, and (7) Laplacian Eigenmaps. We performed experiments on five data sets that represent a variety of application domains. Again, because of space limitations, we restrict ourselves to three data sets in the paper. The results of our experiments on the remaining two data sets are presented in the supplementary material.</p>
<p>In <a href="#sec-datasets">Section&nbsp;4.1</a>, the data sets that we employed in our experiments are introduced. The setup of the experiments is presented in <a href="#sec-experimental_setup">Section&nbsp;4.2</a>. In <a href="#sec-results">Section&nbsp;4.3</a>, we present the results of our experiments.</p>
<section id="sec-datasets" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="sec-datasets"><span class="header-section-number">4.1</span> Data Sets</h2>
<p>The five data sets we employed in our experiments are: (1) the MNIST data set, (2) the Olivetti faces data set, (3) the COIL-20 data set, (4) the word-features data set, and (5) the Netflix data set. We only present results on the first three data sets in this section. The results on the remaining two data sets are presented in the supporting material. The first three data sets are introduced below.</p>
<p>The MNIST data set<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> contains 60,000 grayscale images of handwritten digits. For our experiments, we randomly selected 6,000 of the images for computational reasons. The digit images have <span class="math inline">28 \times 28 = 784</span> pixels (i.e., dimensions). The Olivetti faces data set<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> consists of images of 40 individuals with small variations in viewpoint, large variations in expression, and occasional addition of glasses. The data set consists of 400 images (10 per individual) of size <span class="math inline">92\times 112=10,304</span> pixels, and is labeled according to identity. The COIL-20 data set <span class="citation" data-cites="nene:coil20">Nene, Nayar, and Murase (<a href="#ref-nene:coil20" role="doc-biblioref">1996</a>)</span> contains images of 20 different objects viewed from 72 equally spaced orientations, yielding a total of 1,440 images. The images contain <span class="math inline">32 \times 32 = 1,024</span> pixels.</p>
</section>
<section id="sec-experimental_setup" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="sec-experimental_setup"><span class="header-section-number">4.2</span> Experimental Setup</h2>
<p>In all of our experiments, we start by using PCA to reduce the dimensionality of the data to 30. This speeds up the computation of pairwise distances between the datapoints and suppresses some noise without severely distorting the interpoint distances. We then use each of the dimensionality reduction techniques to convert the 30-dimensional representation to a two-dimensional map and we show the resulting map as a scatterplot. For all of the data sets, there is information about the class of each datapoint, but the class information is only used to select a color and/or symbol for the map points. The class information is not used to determine the spatial coordinates of the map points. The coloring thus provides a way of evaluating how well the map preserves the similarities within each class.</p>
<p>The cost function parameter settings we employed in our experiments are listed in <a href="#tbl-cost-function-parameters">Table&nbsp;1</a>. In the table, <span class="math inline">Perp</span> represents the perplexity of the conditional probability distribution induced by a Gaussian kernel and <span class="math inline">k</span> represents the number of nearest neighbors employed in a neighborhood graph. In the experiments with Isomap and LLE, we only visualize datapoints that correspond to vertices in the largest connected component of the neighborhood graph. <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> For the Sammon mapping optimization, we performed Newton’s method for 500 iterations.</p>
<div id="tbl-cost-function-parameters" class="anchored">
<table class="table">
<caption>Table&nbsp;1: Cost function parameter settings for the experiments</caption>
<thead>
<tr class="header">
<th>Technique</th>
<th style="text-align: right;">Cost function parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>t-SNE</td>
<td style="text-align: right;"><span class="math inline">Perp = 40</span></td>
</tr>
<tr class="even">
<td>Sammon mapping</td>
<td style="text-align: right;">none</td>
</tr>
<tr class="odd">
<td>Isomap</td>
<td style="text-align: right;"><span class="math inline">k=12</span></td>
</tr>
<tr class="even">
<td>LLE</td>
<td style="text-align: right;"><span class="math inline">k=12</span></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="sec-results" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="sec-results"><span class="header-section-number">4.3</span> Results</h2>
<div class="cell" data-execution_count="1">
<details>
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 2. Visualization by t-SNE and Sammon mapping</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_digits</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> manifold</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> joblib</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>mem <span class="op">=</span> joblib.Memory(<span class="st">".joblib"</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>digits <span class="op">=</span> load_digits()</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> digits[<span class="st">"data"</span>]</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> digits[<span class="st">"target"</span>]</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>n_components <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> {</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>: <span class="st">"C0"</span>,</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span>: <span class="st">"C1"</span>,</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    <span class="dv">2</span>: <span class="st">"C2"</span>,</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    <span class="dv">3</span>: <span class="st">"C3"</span>,</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    <span class="dv">4</span>: <span class="st">"C4"</span>,</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    <span class="dv">5</span>: <span class="st">"C5"</span>,</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    <span class="dv">6</span>: <span class="st">"C6"</span>,</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    <span class="dv">7</span>: <span class="st">"C7"</span>,</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    <span class="dv">8</span>: <span class="st">"C8"</span>,</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    <span class="dv">9</span>: <span class="st">"C9"</span>,</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">12</span>))</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="co"># t-SNE manifold learning</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>tsne <span class="op">=</span> manifold.TSNE(n_components<span class="op">=</span>n_components, init<span class="op">=</span><span class="st">"pca"</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>X_transformed <span class="op">=</span> mem.cache(tsne.fit_transform)(X)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label <span class="kw">in</span> np.unique(y):</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> y <span class="op">==</span> label</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>    ax.scatter(</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">0</span>],</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">1</span>],</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>        c<span class="op">=</span>colors[label], marker<span class="op">=</span><span class="st">"."</span>,</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span>label)</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"left"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"right"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"top"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"bottom"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([])</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>ax.set_yticks([])</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"t-SNE"</span>)</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Sammon mapping</span></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">FIXME</span><span class="co"> I think Sammon mapping has weights?</span></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> manifold.MDS(n_components<span class="op">=</span>n_components)</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>X_transformed <span class="op">=</span> mem.cache(embedding.fit_transform)(X)</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label <span class="kw">in</span> np.unique(y):</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> y <span class="op">==</span> label</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>    ax.scatter(</span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">0</span>],</span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">1</span>],</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>        c<span class="op">=</span>colors[label], marker<span class="op">=</span><span class="st">"."</span>,</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span>label)</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"left"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"right"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"top"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"bottom"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([])</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>ax.set_yticks([])</span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Sammon mapping"</span>)</span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>________________________________________________________________________________
[Memory] Calling sklearn.manifold._t_sne.fit_transform...
fit_transform(array([[0., ..., 0.],
       ...,
       [0., ..., 0.]]))</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>____________________________________________________fit_transform - 7.7s, 0.1min
________________________________________________________________________________
[Memory] Calling sklearn.manifold._mds.fit_transform...
fit_transform(array([[0., ..., 0.],
       ...,
       [0., ..., 0.]]))</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>___________________________________________________fit_transform - 67.8s, 1.1min</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="fig-tsne-sammon" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-paper-tsne_files/figure-html/fig-tsne-sammon-output-4.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: Visualization by t-SNE and Sammon mapping</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="2">
<details>
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 3. Visualization by Isomap and LLE</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">12</span>))</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># ISOMAP</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>isomap <span class="op">=</span> manifold.Isomap(n_components<span class="op">=</span>n_components, n_neighbors<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>X_transformed <span class="op">=</span> mem.cache(isomap.fit_transform)(X)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label <span class="kw">in</span> np.unique(y):</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> y <span class="op">==</span> label</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    ax.scatter(</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">0</span>],</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">1</span>],</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        c<span class="op">=</span>colors[label], marker<span class="op">=</span><span class="st">"."</span>,</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span>label)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"left"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"right"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"top"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"bottom"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([])</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>ax.set_yticks([])</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Isomap"</span>)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a><span class="co"># LLE</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> manifold.LocallyLinearEmbedding(</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    n_components<span class="op">=</span>n_components,</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    n_neighbors<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>X_transformed <span class="op">=</span> mem.cache(embedding.fit_transform)(X)</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label <span class="kw">in</span> np.unique(y):</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> y <span class="op">==</span> label</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>    ax.scatter(</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">0</span>],</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">1</span>],</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>        c<span class="op">=</span>colors[label], marker<span class="op">=</span><span class="st">"."</span>,</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span>label)</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"left"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"right"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"top"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"bottom"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([])</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>ax.set_yticks([])</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"LLE"</span>)</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>________________________________________________________________________________
[Memory] Calling sklearn.manifold._isomap.fit_transform...
fit_transform(array([[0., ..., 0.],
       ...,
       [0., ..., 0.]]))</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>____________________________________________________fit_transform - 1.7s, 0.0min
________________________________________________________________________________
[Memory] Calling sklearn.manifold._locally_linear.fit_transform...
fit_transform(array([[0., ..., 0.],
       ...,
       [0., ..., 0.]]))</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>____________________________________________________fit_transform - 0.2s, 0.0min</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="fig-isomap-lle" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-paper-tsne_files/figure-html/fig-isomap-lle-output-4.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3: Visualization by Isomap and LLE</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="3">
<details>
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 4. Visualization of the Olivetti faces data set</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># First, load the olivetti datasets</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_olivetti_faces</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> itertools</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> fetch_olivetti_faces()</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[<span class="st">"data"</span>]</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">"target"</span>]</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>markers <span class="op">=</span> [<span class="st">"+"</span>, <span class="st">"."</span>, <span class="st">"v"</span>, <span class="st">"^"</span>, <span class="st">"&gt;"</span>, <span class="st">"&lt;"</span>, <span class="st">"d"</span>, <span class="st">"*"</span>]</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">"black"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span>, <span class="st">"pink"</span>]</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">2</span>, ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="co"># t-SNE manifold learning</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>tsne <span class="op">=</span> manifold.TSNE(n_components<span class="op">=</span>n_components, init<span class="op">=</span><span class="st">"pca"</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>X_transformed <span class="op">=</span> mem.cache(tsne.fit_transform)(X)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label, (m, c) <span class="kw">in</span> <span class="bu">zip</span>(np.unique(y), itertools.product(markers, colors)):</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> y <span class="op">==</span> label</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>    ax.scatter(</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">0</span>],</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">1</span>],</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>    marker<span class="op">=</span>m, c<span class="op">=</span>c,</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    s<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span>label)</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"left"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"right"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"top"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"bottom"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([])</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>ax.set_yticks([])</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"t-SNE"</span>)</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Sammon mapping</span></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> manifold.MDS(n_components<span class="op">=</span>n_components)</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>X_transformed <span class="op">=</span> mem.cache(embedding.fit_transform)(X)</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label, (m, c) <span class="kw">in</span> <span class="bu">zip</span>(np.unique(y), itertools.product(markers, colors)):</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> y <span class="op">==</span> label</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>    ax.scatter(</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">0</span>],</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">1</span>],</span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>    marker<span class="op">=</span>m, c<span class="op">=</span>c,</span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>    s<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span>label)</span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"left"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"right"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"top"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"bottom"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([])</span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>ax.set_yticks([])</span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Sammon mapping"</span>)</span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a><span class="co"># ISOMAP</span></span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>, <span class="dv">0</span>]</span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a>isomap <span class="op">=</span> manifold.Isomap(n_components<span class="op">=</span>n_components, n_neighbors<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>X_transformed <span class="op">=</span> mem.cache(isomap.fit_transform)(X)</span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label, (m, c) <span class="kw">in</span> <span class="bu">zip</span>(np.unique(y), itertools.product(markers, colors)):</span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> y <span class="op">==</span> label</span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>    ax.scatter(</span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">0</span>],</span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">1</span>],</span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a>    marker<span class="op">=</span>m, c<span class="op">=</span>c,</span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a>    s<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span>label)</span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"left"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"right"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"top"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"bottom"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([])</span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a>ax.set_yticks([])</span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Isomap"</span>)</span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a><span class="co"># LLE</span></span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> manifold.LocallyLinearEmbedding(</span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a>    n_components<span class="op">=</span>n_components,</span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a>    n_neighbors<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb10-92"><a href="#cb10-92" aria-hidden="true" tabindex="-1"></a>X_transformed <span class="op">=</span> mem.cache(embedding.fit_transform)(X)</span>
<span id="cb10-93"><a href="#cb10-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-94"><a href="#cb10-94" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label, (m, c) <span class="kw">in</span> <span class="bu">zip</span>(np.unique(y), itertools.product(markers, colors)):</span>
<span id="cb10-95"><a href="#cb10-95" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> y <span class="op">==</span> label</span>
<span id="cb10-96"><a href="#cb10-96" aria-hidden="true" tabindex="-1"></a>    ax.scatter(</span>
<span id="cb10-97"><a href="#cb10-97" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">0</span>],</span>
<span id="cb10-98"><a href="#cb10-98" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">1</span>],</span>
<span id="cb10-99"><a href="#cb10-99" aria-hidden="true" tabindex="-1"></a>    marker<span class="op">=</span>m, c<span class="op">=</span>c,</span>
<span id="cb10-100"><a href="#cb10-100" aria-hidden="true" tabindex="-1"></a>    s<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb10-101"><a href="#cb10-101" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span>label)</span>
<span id="cb10-102"><a href="#cb10-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-103"><a href="#cb10-103" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"left"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb10-104"><a href="#cb10-104" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"right"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb10-105"><a href="#cb10-105" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"top"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb10-106"><a href="#cb10-106" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"bottom"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb10-107"><a href="#cb10-107" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([])</span>
<span id="cb10-108"><a href="#cb10-108" aria-hidden="true" tabindex="-1"></a>ax.set_yticks([])</span>
<span id="cb10-109"><a href="#cb10-109" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"LLE"</span>)</span>
<span id="cb10-110"><a href="#cb10-110" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>downloading Olivetti faces from https://ndownloader.figshare.com/files/5976027 to /home/runner/scikit_learn_data</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>________________________________________________________________________________
[Memory] Calling sklearn.manifold._t_sne.fit_transform...
fit_transform(array([[0.309917, ..., 0.157025],
       ...,
       [0.516529, ..., 0.384298]], dtype=float32))</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>____________________________________________________fit_transform - 1.6s, 0.0min
________________________________________________________________________________
[Memory] Calling sklearn.manifold._mds.fit_transform...
fit_transform(array([[0.309917, ..., 0.157025],
       ...,
       [0.516529, ..., 0.384298]], dtype=float32))</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>____________________________________________________fit_transform - 2.0s, 0.0min
________________________________________________________________________________
[Memory] Calling sklearn.manifold._isomap.fit_transform...
fit_transform(array([[0.309917, ..., 0.157025],
       ...,
       [0.516529, ..., 0.384298]], dtype=float32))
____________________________________________________fit_transform - 0.1s, 0.0min</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>________________________________________________________________________________
[Memory] Calling sklearn.manifold._locally_linear.fit_transform...
fit_transform(array([[0.309917, ..., 0.157025],
       ...,
       [0.516529, ..., 0.384298]], dtype=float32))
____________________________________________________fit_transform - 0.1s, 0.0min</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="fig-olivetti" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="published-paper-tsne_files/figure-html/fig-olivetti-output-6.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4: Visualization of the Olivetti faces data set</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>In <a href="#fig-tsne-sammon">Figure&nbsp;2</a> and <a href="#fig-isomap-lle">Figure&nbsp;3</a>, we show the results of our experiments with t-SNE, Sammon mapping, Isomap, and LLE on the MNIST data set. The results reveal the strong performance of t-SNE compared to the other techniques. In particular, Sammon mapping constructs a “ball” in which only three classes (representing the digits 0, 1, and 7) are somewhat separated from the other classes. Isomap and LLE produce solutions in which there are large overlaps between the digit classes. In contrast, tSNE constructs a map in which the separation between the digit classes is almost perfect. Moreover, detailed inspection of the t-SNE map reveals that much of the local structure of the data (such as the orientation of the ones) is captured as well. This is illustrated in more detail in <a href="#sec-large-data">Section&nbsp;5</a> (see <a href="#fig-random_walk_tsne">Figure&nbsp;6</a>). The map produced by t-SNE contains some points that are clustered with the wrong class, but most of these points correspond to distorted digits many of which are difficult to identify. <a href="#fig-olivetti">Figure&nbsp;4</a> shows the results of applying t-SNE, Sammon mapping, Isomap, and LLE to the Olivetti faces data set. Again, Isomap and LLE produce solutions that provide little insight into the class structure of the data. The map constructed by Sammon mapping is significantly better, since it models many of the members of each class fairly close together, but none of the classes are clearly separated in the Sammon map. In contrast, t-SNE does a much better job of revealing the natural classes in the data. Some individuals have their ten images split into two clusters, usually because a subset of the images have the head facing in a significantly different direction, or because they have a very different expression or glasses. For these individuals, it is not clear that their ten images form a natural class when using Euclidean distance in pixel space.</p>
<p>Figure 5 shows the results of applying t-SNE, Sammon mapping, Isomap, and LLE to the COIL20 data set. For many of the 20 objects, t-SNE accurately represents the one-dimensional manifold of viewpoints as a closed loop. For objects which look similar from the front and the back, t-SNE distorts the loop so that the images of front and back are mapped to nearby points. For the four types of toy car in the COIL-20 data set (the four aligned “sausages” in the bottom-left of the tSNE map), the four rotation manifolds are aligned by the orientation of the cars to capture the high similarity between different cars at the same orientation. This prevents t-SNE from keeping the four manifolds clearly separate. Figure 5 also reveals that the other three techniques are not nearly as good at cleanly separating the manifolds that correspond to very different objects. In addition, Isomap and LLE only visualize a small number of classes from the COIL-20 data set, because the data set comprises a large number of widely separated submanifolds that give rise to small connected components in the neighborhood graph.</p>
</section>
</section>
<section id="sec-large-data" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Applying t-SNE to Large Data Sets</h1>
<p>Like many other visualization techniques, t-SNE has a computational and memory complexity that is quadratic in the number of datapoints. This makes it infeasible to apply the standard version of t-SNE to data sets that contain many more than, say, 10,000 points. Obviously, it is possible to pick a random subset of the datapoints and display them using t-SNE, but such an approach fails to make use of the information that the undisplayed datapoints provide about the underlying manifolds. Suppose, for example, that A, B, and C are all equidistant in the high-dimensional space. If there are many undisplayed datapoints between A and B and none between A and C, it is much more likely that A and B are part of the same cluster than A and C. This is illustrated in <a href="#fig-random-walk">Figure&nbsp;5</a>. In this section, we show how t-SNE can be modified to display a random subset of the datapoints (so-called landmark points) in a way that uses information from the entire (possibly very large) data set.</p>
<div id="fig-random-walk" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/random_walk.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5: An illustration of the advantage of the random walk version of t-SNE over a standard landmark approach. The shaded points A, B, and C are three (almost) equidistant landmark points, whereas the non-shaded datapoints are non-landmark points. The arrows represent a directed neighborhood graph where <span class="math inline">k = 3</span>. In a standard landmark approach, the pairwise affinity between A and B is approximately equal to the pairwise affinity between A and C. In the random walk version of t-SNE, the pairwise affinity between A and B is much larger than the pairwise affinity between A and C, and therefore, it reflects the structure of the data much better.</figcaption><p></p>
</figure>
</div>
<p>We start by choosing a desired number of neighbors and creating a neighborhood graph for all of the datapoints. Although this is computationally intensive, it is only done once. Then, for each of the landmark points, we define a random walk starting at that landmark point and terminating as soon as it lands on another landmark point. During a random walk, the probability of choosing an edge emanating from node xi to node x j is proportional to <span class="math inline">e^{-\|x_i−x_j \|^2}</span> . We define <span class="math inline">p_{j|i}</span> to be the fraction of random walks starting at landmark point <span class="math inline">x_i</span> that terminate at landmark point <span class="math inline">x_j</span> . This has some resemblance to the way Isomap measures pairwise distances between points. However, as in diffusion maps <span class="citation" data-cites="lafon:diffusion">Lafon and Lee (<a href="#ref-lafon:diffusion" role="doc-biblioref">2006</a>)</span>,<span class="citation" data-cites="nadler:diffusion">Nadler et al. (<a href="#ref-nadler:diffusion" role="doc-biblioref">2006</a>)</span>, rather than looking for the shortest path through the neighborhood graph, the random walk-based affinity measure integrates over all paths through the neighborhood graph. As a result, the random walk-based affinity measure is much less sensitive to “short-circuits” <span class="citation" data-cites="lee:nonlinear2005">John Aldo Lee and Verleysen (<a href="#ref-lee:nonlinear2005" role="doc-biblioref">2005</a>)</span>, in which a single noisy datapoint provides a bridge between two regions of dataspace that should be far apart in the map. Similar approaches using random walks have also been successfully applied to, for example, semi-supervised learning <span class="citation" data-cites="jaakola:partially">Szummer and Jaakkola (<a href="#ref-jaakola:partially" role="doc-biblioref">2002</a>)</span>,<span class="citation" data-cites="zhu:semi">Zhu, Ghahramani, and Lafferty (<a href="#ref-zhu:semi" role="doc-biblioref">2003</a>)</span> and image segmentation <span class="citation" data-cites="grady:random">Grady (<a href="#ref-grady:random" role="doc-biblioref">2006</a>)</span>.</p>
<p>The most obvious way to compute the random walk-based similarities <span class="math inline">p_{j|i}</span> is to explicitly perform the random walks on the neighborhood graph, which works very well in practice, given that one can easily perform one million random walks per second. Alternatively, <span class="citation" data-cites="grady:random">Grady (<a href="#ref-grady:random" role="doc-biblioref">2006</a>)</span> presents an analytical solution to compute the pairwise similarities <span class="math inline">p_{j|i}</span> that involves solving a sparse linear system. The analytical solution to compute the similarities <span class="math inline">p_{j|i}</span> is sketched in Appendix B (FIXME). In preliminary experiments, we did not find significant differences between performing the random walks explicitly and the analytical solution. In the experiment we present below, we explicitly performed the random walks because this is computationally less expensive. However, for very large data sets in which the landmark points are very sparse, the analytical solution may be more appropriate.</p>
<div id="fig-random_walk_tsne" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/random_walk_tSNE.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6: Visualization of 6,000 digits from the MNIST data set produced by the random walk version of t-SNE (employing all 60,000 digit images).</figcaption><p></p>
</figure>
</div>
<p><a href="#fig-random_walk_tsne">Figure&nbsp;6</a> shows the results of an experiment, in which we applied the random walk version of t-SNE to 6,000 randomly selected digits from the MNIST data set, using all 60,000 digits to compute the pairwise affinities <span class="math inline">p_{j|i}</span>. In the experiment, we used a neighborhood graph that was constructed using a value of <span class="math inline">k = 20</span> nearest neighbors.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> The inset of the figure shows the same visualization as a scatterplot in which the colors represent the labels of the digits. In the t-SNE map, all classes are clearly separated and the “continental” sevens form a small separate cluster. Moreover, t-SNE reveals the main dimensions of variation within each class, such as the orientation of the ones, fours, sevens, and nines, or the “loopiness” of the twos. The strong performance of t-SNE is also reflected in the generalization error of nearest neighbor classifiers that are trained on the low-dimensional representation. Whereas the generalization error (measured using 10-fold cross validation) of a 1-nearest neighbor classifier trained on the original 784-dimensional datapoints is 5.75%, the generalization error of a 1-nearest neighbor classifier trained on the two-dimensional data representation produced by t-SNE is only 5.13%. The computational requirements of random walk t-SNE are reasonable: it took only one hour of CPU time to construct the map in <a href="#fig-random_walk_tsne">Figure&nbsp;6</a>.</p>
</section>
<section id="sec-discussion" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Discussion</h1>
<p>The results in the previous two sections (and those in the supplemental material) demonstrate the performance of t-SNE on a wide variety of data sets. In this section, we discuss the differences between t-SNE and other non-parametric techniques (<a href="sec-comparison">6.1</a>), and we also discuss a number of weaknesses and possible improvements of t-SNE (<a href="sec-weakness">6.2</a>).</p>
<section id="sec-comparison" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="sec-comparison"><span class="header-section-number">6.1</span> Comparison with Related Techniques</h2>
<p>Classical scaling <span class="citation" data-cites="torgerson:multidimensional">Torgerson (<a href="#ref-torgerson:multidimensional" role="doc-biblioref">1952</a>)</span>, which is closely related to PCA <span class="citation" data-cites="mardia:multivariate">Mardia and Bibby (<a href="#ref-mardia:multivariate" role="doc-biblioref">1979</a>)</span> <span class="citation" data-cites="williams:connection">Williams (<a href="#ref-williams:connection" role="doc-biblioref">2002</a>)</span>, finds a linear transformation of the data that minimizes the sum of the squared errors between high-dimensional pairwise distances and their low-dimensional representatives. A linear method such as classical scaling is not good at modeling curved manifolds and it focuses on preserving the distances between widely separated datapoints rather than on preserving the distances between nearby datapoints. An important approach that attempts to address the problems of classical scaling is the Sammon mapping <span class="citation" data-cites="sammon:nonlinear">Sammon (<a href="#ref-sammon:nonlinear" role="doc-biblioref">1969</a>)</span> which alters the cost function of classical scaling by dividing the squared error in the representation of each pairwise Euclidean distance by the original Euclidean distance in the high-dimensional space. The resulting cost function is given by</p>
<p><span class="math display">
    C = \frac{1}{\sum_{ij} \|x_i - x_j\|} \sum_{i \neq j} \frac{\left(\|x_i - x_j\| - \|x_i - x_j\|\right)^2}{\|x_i - x_j\|}\,
</span></p>
<p>where the constant outside of the sum is added in order to simplify the derivation of the gradient. The main weakness of the Sammon cost function is that the importance of retaining small pairwise distances in the map is largely dependent on small differences in these pairwise distances. In particular, a small error in the model of two high-dimensional points that are extremely close together results in a large contribution to the cost function. Since all small pairwise distances constitute the local structure of the data, it seems more appropriate to aim to assign approximately equal importance to all small pairwise distances.</p>
<p>In contrast to Sammon mapping, the Gaussian kernel employed in the high-dimensional space by t-SNE defines a soft border between the local and global structure of the data and for pairs of datapoints that are close together relative to the standard deviation of the Gaussian, the importance of modeling their separations is almost independent of the magnitudes of those separations. Moreover, t-SNE determines the local neighborhood size for each datapoint separately based on the local density of the data (by forcing each conditional probability distribution <span class="math inline">P_i</span> to have the same perplexity).</p>
<p>The strong performance of t-SNE compared to Isomap is partly explained by Isomap’s susceptibility to “short-circuiting”. Also, Isomap mainly focuses on modeling large geodesic distances rather than small ones.</p>
<p>The strong performance of t-SNE compared to LLE is mainly due to a basic weakness of LLE: the only thing that prevents all datapoints from collapsing onto a single point is a constraint on the covariance of the low-dimensional representation. In practice, this constraint is often satisfied by placing most of the map points near the center of the map and using a few widely scattered points to create large covariance (see Figure FIXME). For neighborhood graphs that are almost disconnected, the covariance constraint can also be satisfied by a “curdled” map in which there are a few widely separated, collapsed subsets corresponding to the almost disconnected components. Furthermore, neighborhood-graph based techniques (such as Isomap and LLE) are not capable of visualizing data that consists of two or more widely separated submanifolds, because such data does not give rise to a connected neighborhood graph. It is possible to produce a separate map for each connected component, but this loses information about the relative similarities of the separate components.</p>
<p>Like Isomap and LLE, the random walk version of t-SNE employs neighborhood graphs, but it does not suffer from short-circuiting problems because the pairwise similarities between the highdimensional datapoints are computed by integrating over all paths through the neighborhood graph. Because of the diffusion-based interpretation of the conditional probabilities underlying the random walk version of t-SNE, it is useful to compare t-SNE to diffusion maps. Diffusion maps define a “diffusion distance” on the high-dimensional datapoints that is given by</p>
<p><span class="math display">
D^{(t)}(x_i,x_j) = \sqrt{ \sum_{k} \frac{\left(p^{(t)}_{ik} -
p^{(t)}_{jk}\right)^2)}{\psi(x_k)^{(0)}} }\,
</span></p>
<p>where <span class="math inline">p^{(t)}_{ij}</span> represents the probability of a particle traveling from <span class="math inline">x_i</span> to <span class="math inline">x_j</span> in <span class="math inline">t</span> timesteps through a graph on the data with Gaussian emission probabilities. The term <span class="math inline">\psi(x_k)^{(0)}</span> is a measure for the local density of the points, and serves a similar purpose to the fixed perplexity Gaussian kernel that is employed in SNE. The diffusion map is formed by the principal non-trivial eigenvectors of the Markov matrix of the random walks of length <span class="math inline">t</span>. It can be shown that when all <span class="math inline">(n−1)</span> non-trivial eigenvectors are employed, the Euclidean distances in the diffusion map are equal to the diffusion distances in the high-dimensional data representation <span class="citation" data-cites="lafon:diffusion">Lafon and Lee (<a href="#ref-lafon:diffusion" role="doc-biblioref">2006</a>)</span>. Mathematically, diffusion maps minimize</p>
<p><span class="math display">
C = \sum_i \sum_j \left(D^{(t)}(x_i,x_j) - \|y_i-y_j\|\right)^2
</span></p>
<p>As a result, diffusion maps are susceptible to the same problems as classical scaling: they assign much higher importance to modeling the large pairwise diffusion distances than the small ones and as a result, they are not good at retaining the local structure of the data. Moreover, in contrast to the random walk version of t-SNE, diffusion maps do not have a natural way of selecting the length, <span class="math inline">t</span>, of the random walks.</p>
<p>In the supplemental material, we present results that reveal that t-SNE outperforms CCA <span class="citation" data-cites="demartines:curvilinear">Demartines and Herault (<a href="#ref-demartines:curvilinear" role="doc-biblioref">1997</a>)</span>, MVU <span class="citation" data-cites="weinberger:learning">Kilian Q. Weinberger, Sha, and Saul (<a href="#ref-weinberger:learning" role="doc-biblioref">2004</a>)</span>, and Laplacian Eigenmaps <span class="citation" data-cites="belkin:laplacian">Belkin and Niyogi (<a href="#ref-belkin:laplacian" role="doc-biblioref">2001</a>)</span> as well. For CCA and the closely related CDA <span class="citation" data-cites="lee:robust">John Aldo Lee et al. (<a href="#ref-lee:robust" role="doc-biblioref">2000</a>)</span>, these results can be partially explained by the hard border <span class="math inline">\lambda</span> that these techniques define between local and global structure, as opposed to the soft border of t-SNE. Moreover, within the range <span class="math inline">\lambda</span>, CCA suffers from the same weakness as Sammon mapping: it assigns extremely high importance to modeling the distance between two datapoints that are extremely close.</p>
<p>Like t-SNE, MVU <span class="citation" data-cites="weinberger:learning">Kilian Q. Weinberger, Sha, and Saul (<a href="#ref-weinberger:learning" role="doc-biblioref">2004</a>)</span> tries to model all of the small separations well but MVU insists on modeling them perfectly (i.e., it treats them as constraints) and a single erroneous constraint may severely affect the performance of MVU. This can occur when there is a short-circuit between two parts of a curved manifold that are far apart in the intrinsic manifold coordinates. Also, MVU makes no attempt to model longer range structure: It simply pulls the map points as far apart as possible subject to the hard constraints so, unlike t-SNE, it cannot be expected to produce sensible large-scale structure in the map.</p>
<p>For Laplacian Eigenmaps, the poor results relative to t-SNE may be explained by the fact that Laplacian Eigenmaps have the same covariance constraint as LLE, and it is easy to cheat on this constraint.</p>
</section>
<section id="sec-weakness" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="sec-weakness"><span class="header-section-number">6.2</span> Weakness</h2>
<p>Although we have shown that t-SNE comparesfavorably to other techniquesfor data visualization, tSNE has three potential weaknesses: (1) it is unclear how t-SNE performs on general dimensionality reduction tasks, (2) the relatively local nature of t-SNE makes it sensitive to the curse of the intrinsic dimensionality of the data, and (3) t-SNE is not guaranteed to converge to a global optimum of its cost function. Below, we discuss the three weaknesses in more detail.</p>
<ol type="1">
<li><p><em>Dimensionality reduction for other purposes.</em> It is not obvious how t-SNE will perform on the more general task of dimensionality reduction (i.e., when the dimensionality of the data is not reduced to two or three, but to <span class="math inline">d &gt; 3</span> dimensions). To simplify evaluation issues, this paper only considers the use of t-SNE for data visualization. The behavior of t-SNE when reducing data to two or three dimensions cannot readily be extrapolated to <span class="math inline">d &gt; 3</span> dimensions because of the heavy tails of the Student-t distribution. In high-dimensional spaces, the heavy tails comprise a relatively large portion of the probability mass under the Student-t distribution, which might lead to d-dimensional data representations that do not preserve the local structure of the data as well. Hence, for tasks in which the dimensionality of the data needs to be reduced to a dimensionality higher than three, Student t-distributions with more than one degree of freedom10 are likely to be more appropriate.</p></li>
<li><p><em>Curse of intrinsic dimensionality.</em> t-SNE reduces the dimensionality of data mainly based on local properties of the data, which makes t-SNE sensitive to the curse of the intrinsic dimensionality of the data <span class="citation" data-cites="bengio:learning">Bengio (<a href="#ref-bengio:learning" role="doc-biblioref">2009</a>)</span>. In data sets with a high intrinsic dimensionality and an underlying manifold that is highly varying, the local linearity assumption on the manifold that t-SNE implicitly makes (by employing Euclidean distances between near neighbors) may be violated. As a result, t-SNE might be less successful if it is applied on data sets with a very high intrinsic dimensionality (for instance, a recent study by <span class="citation" data-cites="meytlis:face">Meytlis and Sirovich (<a href="#ref-meytlis:face" role="doc-biblioref">2007</a>)</span> estimates the space of images of faces to be constituted of approximately 100 dimensions). Manifold learners such as Isomap and LLE suffer from exactly the same problems (see, e.g., <span class="citation" data-cites="bengio:learning">Bengio (<a href="#ref-bengio:learning" role="doc-biblioref">2009</a>)</span>; <span class="citation" data-cites="vandermaaten:comparison">Van Der Maaten et al. (<a href="#ref-vandermaaten:comparison" role="doc-biblioref">2009</a>)</span> ). A possible way to (partially) address this issue is by performing t-SNE on a data representation obtained from a model that represents the highly varying data manifold efficiently in a number of nonlinear layers such as an autoencoder <span class="citation" data-cites="hinton:reducing">Hinton and Salakhutdinov (<a href="#ref-hinton:reducing" role="doc-biblioref">2006</a>)</span>. Such deep-layer architectures can represent complex nonlinear functions in a much simpler way, and as a result, require fewer datapoints to learn an appropriate solution (as is illustrated for a d-bits parity task by <span class="citation" data-cites="bengio:learning">Bengio (<a href="#ref-bengio:learning" role="doc-biblioref">2009</a>)</span>). Performing t-SNE on a data representation produced by, for example, an autoencoder is likely to improve the quality of the constructed visualizations, because autoencoders can identify highly-varying manifolds better than a local method such as t-SNE. However, the reader should note that it is by definition impossible to fully represent the structure of intrinsically high-dimensional data in two or three dimensions.</p></li>
<li><p><em>Non-convexity of the t-SNE cost function.</em> A nice property of most state-of-the-art dimensionality reduction techniques (such as classical scaling, Isomap, LLE, and diffusion maps) is the convexity of their cost functions. A major weakness of t-SNE is that the cost function is not convex, as a result of which several optimization parameters need to be chosen. The constructed solutions depend on these choices of optimization parameters and may be different each time t-SNE is run from an initial random configuration of map points. We have demonstrated that the same choice of optimization parameters can be used for a variety of different visualization tasks, and we found that the quality of the optima does not vary much from run to run. Therefore, we think that the weakness of the optimization method is insufficient reason to reject t-SNE in favor of methods that lead to convex optimization problems but produce noticeably worse visualizations. A local optimum of a cost function that accurately captures what we want in a visualization is often preferable to the global optimum of a cost function that fails to capture important aspects of what we want. Moreover, the convexity of cost functions can be misleading, because their optimization is often computationally infeasible for large real-world data sets, prompting the use of approximation techniques <span class="citation" data-cites="desilva:global">De Silva and Tenenbaum (<a href="#ref-desilva:global" role="doc-biblioref">2002</a>)</span>; <span class="citation" data-cites="weinberger:graph">Kilian Q. Weinberger et al. (<a href="#ref-weinberger:graph" role="doc-biblioref">2007</a>)</span>. Even for LLE and Laplacian Eigenmaps, the optimization is performed using iterative Arnoldi <span class="citation" data-cites="arnoldi:principle">Arnoldi (<a href="#ref-arnoldi:principle" role="doc-biblioref">1951</a>)</span> or Jacobi-Davidson <span class="citation" data-cites="fokkema:jacobi">Fokkema, Sleijpen, and Van der Vorst (<a href="#ref-fokkema:jacobi" role="doc-biblioref">1998</a>)</span> methods, which may fail to find the global optimum due to convergence problems.</p></li>
</ol>
</section>
</section>
<section id="sec-conclusion" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Conclusions</h1>
<p>The paper presents a new technique for the visualization of similarity data that is capable of retaining the local structure of the data while also revealing some important global structure (such as clusters at multiple scales). Both the computational and the memory complexity of t-SNE are <span class="math inline">\mathcal{O}(n^2)</span>, but we present a landmark approach that makes it possible to successfully visualize large real-world data sets with limited computational demands. Our experiments on a variety of data sets show that t-SNE outperforms existing state-of-the-art techniques for visualizing a variety of real-world data sets. Matlab implementations of both the normal and the random walk version of t-SNE are available for download at <a href="https://lvdmaaten.github.io/tsne/" class="uri">https://lvdmaaten.github.io/tsne/</a>. In future work we plan to investigate the optimization of the number of degrees of freedom of the Student-t distribution used in t-SNE. This may be helpful for dimensionality reduction when the low-dimensional representation has many dimensions. We will also investigate the extension of t-SNE to models in which each high-dimensional datapoint is modeled by several low-dimensional map points as in <span class="citation" data-cites="cook:visualizing">Cook et al. (<a href="#ref-cook:visualizing" role="doc-biblioref">2007</a>)</span>. Also, we aim to develop a parametric version of t-SNE that allows for generalization to held-out test data by using the t-SNE objective function to train a multilayer neural network that provides an explicit mapping to the low-dimensional space</p>

</section>


<section id="bibliography" class="level1 unnumbered">


</section>


<div id="quarto-appendix" class="default"><section id="acknowledgments" class="level4 appendix unnumbered"><h2 class="quarto-appendix-heading">Acknowledgments</h2><div class="quarto-appendix-contents">

<p>The authors thank Sam Roweis for many helpful discussions, Andriy Mnih for supplying the wordfeatures data set, Ruslan Salakhutdinov for help with the Netflix data set (results for these data sets are presented in the supplemental material), and Guido de Croon for pointing us to the analytical solution of the random walk probabilities.</p>
<p>Laurens van der Maaten is supported by the CATCH-programme of the Dutch Scientific Organization (NWO), project RICH (grant 640.002.401), and cooperates with RACM. Geoffrey Hinton is a fellow of the Canadian Institute for Advanced Research, and is also supported by grants from NSERC and CFI and gifts from Google and Microsoft.</p>
</div></section><section id="appendix-a.-derivation-of-the-t-sne-gradient" class="level1 appendix unnumbered"><h2 class="quarto-appendix-heading">Appendix A. Derivation of the t-SNE gradient</h2><div class="quarto-appendix-contents">

<p>t-SNE minimizes the Kullback-Leibler divergence between the joint probabilities <span class="math inline">p_{ij}</span> in the highdimensional space and the joint probabilities <span class="math inline">q_{ij}</span> in the low-dimensional space. The values of <span class="math inline">p_{ij}</span> are defined to be the symmetrized conditional probabilities, whereas the values of <span class="math inline">q_{ij}</span> are obtained by means of a Student-t distribution with one degree of freedom</p>
<p><span class="math display">
\begin{aligned}
p_{ij} &amp; = \frac{p_{j|i} + p_{i|j}}{2 n} \\
q_{ij} &amp; = \frac{\left(1 + \|y_i - y_j \|^2\right)^{-1}}{\sum_{k\neq \ell} \left(1 + \|y_k - y_\ell \|^2\right)^{-1}}
\end{aligned}
</span></p>
<p>where <span class="math inline">p_{j|i}</span> and <span class="math inline">p_{i|j}</span> are either obtained from Equation 1 or from the random walk procedure described in <a href="#sec-large-data">Section&nbsp;5</a>. The values of <span class="math inline">p_{ii}</span> and <span class="math inline">q_{ii}</span> are set to zero. The Kullback-Leibler divergence between the two joint probability distributions <span class="math inline">P</span> and <span class="math inline">Q</span> is given by</p>
<p><span id="eq-appendixa1"><span class="math display">
C = KL(P \| Q) = \sum_i \sum_j p_{ij} \log \frac{p_{ij}}{q_{ij}}  = \sum_i \sum_j p_{ij} \log p_{ij} - p_{ij} \log q_{ij}.
\tag{6}</span></span></p>
<p>In order to make the derivation less cluttered, we define two auxiliary variables <span class="math inline">d_{ij}</span> and <span class="math inline">Z</span> as follows</p>
<p><span class="math display">
\begin{aligned}
d_{ij} &amp; = \|y_i - y_j\|, \\
Z &amp; = \sum_{k\neq \ell} \left(1 + d_{k\ell}^2 \right)^{-1}.
\end{aligned}
</span></p>
<p>Note that if <span class="math inline">y_i</span> changes, the only pairwise distances that change are <span class="math inline">d_{ij}</span> and <span class="math inline">d_{ji}</span> for all <span class="math inline">j</span>. Hence, the gradient of the cost function <span class="math inline">C</span> with respect to <span class="math inline">y_i</span> is given by</p>
<p><span id="eq-appendixa2"><span class="math display">
\frac{\partial C}{\partial y_i} = \sum_j \left(\frac{\partial C}{\partial d_{ij}} + \frac{\partial C}{\partial d_{ji}}\right) (y_i - y_j) = 2 \sum_j \frac{\partial C}{\partial d_{ij}} (y_i - y_j)
\tag{7}</span></span></p>
<p>The gradient <span class="math inline">\frac{\partial C}{\partial d_{ji}}</span> is computed from the definition of the Kullback-Leibler divergence in <a href="#eq-appendixa1">Equation&nbsp;6</a> (note that he first part of this equation is a constant).</p>
<p><span class="math display">
\begin{aligned}
\frac{\partial C}{\partial d_{ij}}
&amp; = - \sum_{k\neq\ell} p_{k\ell} \frac{\partial \log q_{k\ell}}{\partial d_{ij}} \\
&amp; = - \sum_{k\neq\ell} p_{k\ell} \frac{\partial \log q_{k\ell}Q - \log Z}{\partial d_{ij}} \\
&amp; = - \sum_{k\neq\ell} p_{k\ell} \left(\frac{1}{q_{k\ell}Z} \frac{\partial ((1 - d_{k\ell}^2)^{-1})}{\partial d_{ij}} - \frac{1}{Z}\frac{\partial Z}{\partial d_{ij}} \right) )
\end{aligned}
</span></p>
<p>The gradient <span class="math inline">\frac{\partial ((1 - d_{k\ell}^2)^{-1})}{\partial d_{ij}}</span> is only onzero when <span class="math inline">k=i</span> and <span class="math inline">\ell = j</span>. Hence, the gradient <span class="math inline">\frac{\partial C}{\partial d_{ij}}</span> is given by</p>
<p><span class="math display">
\frac{\partial C}{\partial d_{ij}} + 2 \frac{p{ij}}{q_{ij}Z} (1 = d_{ij}^2)^{-2} - 2 \sum_{k\neq \ell} p_{k\ell} \frac{(1+d_{ij}^2)^{-2}}{Z}.
</span></p>
<p>Noting that <span class="math inline">\sum_{k\neq \ell} p_{k\ell} = 1</span>, we see that the gradients simplifies to</p>
<p><span class="math display">
\begin{aligned}
\frac{\partial C}{\partial d_{ij}} &amp; = 2 p{ij} (1 + d_{ij}^2)^{-1} - 2 q_{ij}(1 + d_{ij}^2)^{-1} \\
&amp; = 2 (p{ij} - q_{ij}) (1 + d_{ij}^2)^{-1}.
\end{aligned}
</span></p>
<p>Substituting this term into <a href="#eq-appendixa2">Equation&nbsp;7</a>, we obtain the gradient</p>
<p><span class="math display">
\frac{\partial C}{\partial y_i} = 4 \sum_j (p{ij} - q_{ij}) (1 + \|y_i - y_j\|^2)^{-1} (y_i - y_j).
</span></p>
</div></section><section id="appendix-b.-analytical-solution-to-random-walk-probabilities" class="level1 appendix unnumbered"><h2 class="quarto-appendix-heading">Appendix B. Analytical Solution to Random Walk Probabilities</h2><div class="quarto-appendix-contents">

<p>Below, we describe the analytical solution to the random walk probabilities that are employed in the random walk version of t-SNE (<a href="#sec-large-data">Section&nbsp;5</a>). The solution is described in more detail <span class="citation" data-cites="grady:random">Grady (<a href="#ref-grady:random" role="doc-biblioref">2006</a>)</span>.</p>
<p>It can be shown that computing the probability that a random walk initiated from a non-landmark point (on a graph that is specified by adjacency matrix W) first reaches a specific landmark point is equal to computing the solution to the combinatorial Dirichlet problem in which the boundary conditions are at the locations of the landmark points, the considered landmark point is fixed to unity, and the other landmarks points are set to zero <span class="citation" data-cites="kakutani:dirichlet">Kakutani (<a href="#ref-kakutani:dirichlet" role="doc-biblioref">1945</a>)</span> ; <span class="citation" data-cites="doyle:random">Doyle and Snell (<a href="#ref-doyle:random" role="doc-biblioref">1984</a>)</span>. In practice, the solution can thus be obtained by minimizing the combinatorial formulation of the Dirichlet integral</p>
<p><span class="math display">
D[x] = \frac12 x^\top L x,
</span></p>
<p>where <span class="math inline">L</span> represents the graph Laplacian. Mathematically, the graph Laplacian is given by <span class="math inline">L = D−W</span>, where <span class="math inline">D = \mathrm{diag} (\sum_j w_{1j}, \sum_j w_{2j}, \dots, \sum_j w_{nj} )</span>. Without loss of generality, we may reorder the landmark points such that the landmark points come first. As a result, the combinatorial Dirichlet integral decomposes into</p>
<p><span class="math display">
\begin{aligned}
D_{x_N} &amp; = \frac12
\left[\begin{array}{cc}
x_L^\top &amp; x_N^\top \end{array}\right] \,
\left[\begin{array}{cc}  L_L &amp; B \\ B^\top &amp; L_N \end{array}\right] \,
\left[\begin{array}{c}
x_L \\ x_N \end{array}\right] \\
&amp; = \frac12 (x_L^\top L_L x_L + 2 x_N^\top B^\top x_L + x_N^\top L_N x_N),
\end{aligned}
</span></p>
<p>where we use the subscript <span class="math inline">{\cdot}_L</span> to indicate the landmark points, and the subscript <span class="math inline">{\cdot}_N</span> to indicate the non-landmark points. Differentiating <span class="math inline">D[x_N]</span> with respect to <span class="math inline">x_N</span> and finding its critical points amounts to solving the linear systems</p>
<p><span id="eq-appendixb1"><span class="math display">
L_N x_N = −B^\top.
\tag{8}</span></span></p>
<p>Please note that in this linear system, <span class="math inline">B^\top</span> is a matrix containing the columns from the graph Laplacian <span class="math inline">L</span> that correspond to the landmark points (excluding the rows that correspond to landmark points). After normalization of the solutions to the systems <span class="math inline">X_N</span>, the column vectors of <span class="math inline">X_N</span> contain the probability that a random walk initiated from a non-landmark point terminates in a landmark point. One should note that the linear system in <a href="#eq-appendixb1">Equation&nbsp;8</a> is only nonsingular if the graph is completely connected, or if each connected component in the graph contains at least one landmark point <span class="citation" data-cites="biggs:algebraic">Biggs (<a href="#ref-biggs:algebraic" role="doc-biblioref">1993</a>)</span>.</p>
<p>Because we are interested in the probability of a random walk initiated from a landmark point terminating at another landmark point, we duplicate all landmark points in the neighborhood graph, and initiate the random walks from the duplicate landmarks. Because of memory constraints, it is not possible to store the entire matrix <span class="math inline">X_N</span> into memory (note that we are only interested in a small number of rows from this matrix, viz., in the rows corresponding to the duplicate landmark points). Hence, we solve the linear systems defined by the columns of <span class="math inline">−B^\top</span> one-by-one, and store only the parts of the solutions that correspond to the duplicate landmark points. For computational reasons, we first perform a Cholesky factorization of <span class="math inline">L_N</span>, such that <span class="math inline">L_N = C C^\top</span>, where <span class="math inline">C</span> is an upper-triangular matrix. Subsequently, the solution to the linear system in Equation <a href="#eq-appendixb1">Equation&nbsp;8</a> is obtained by solving the linear systems <span class="math inline">Cy = −B^\top</span> and <span class="math inline">C x_N = y</span> using a fast backsubstitution method.</p>
<!-- -->

</div></section><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">Bibliography</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-arnoldi:principle" class="csl-entry" role="doc-biblioentry">
Arnoldi, Walter Edwin. 1951. <span>“The Principle of Minimized Iterations in the Solution of the Matrix Eigenvalue Problem.”</span> <em>Quarterly of Applied Mathematics</em> 9 (1): 17–29.
</div>
<div id="ref-belkin:laplacian" class="csl-entry" role="doc-biblioentry">
Belkin, Mikhail, and Partha Niyogi. 2001. <span>“Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering.”</span> In <em>Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic</em>, 585–91. Advances in Neural Information Processing Systems. Cambridge, MA, USA: MIT Press.
</div>
<div id="ref-bengio:learning" class="csl-entry" role="doc-biblioentry">
Bengio, Yoshua. 2009. <em>Learning Deep Architectures for AI</em>. Now Publishers Inc.
</div>
<div id="ref-biggs:algebraic" class="csl-entry" role="doc-biblioentry">
Biggs, Norman. 1993. <em>Algebraic Graph Theory</em>. 67. Cambridge university press.
</div>
<div id="ref-chernoff:use" class="csl-entry" role="doc-biblioentry">
Chernoff, Herman. 1973. <span>“The Use of Faces to Represent Points in k-Dimensional Space Graphically.”</span> <em>Journal of the American Statistical Association</em> 68 (342): 361–68. <a href="http://www.jstor.org/stable/2284077">http://www.jstor.org/stable/2284077</a>.
</div>
<div id="ref-cook:visualizing" class="csl-entry" role="doc-biblioentry">
Cook, James, Ilya Sutskever, Andriy Mnih, and Geoffrey Hinton. 2007. <span>“Visualizing Similarity Data with a Mixture of Maps.”</span> In <em>In Proceedings of the 11th International Conference on Artificial Intelligence and Statistics</em>, 2:67–74. PMLR.
</div>
<div id="ref-desilva:global" class="csl-entry" role="doc-biblioentry">
De Silva, Vin, and Joshua B Tenenbaum. 2002. <span>“Global Versus Local Methods in Nonlinear Dimensionality Reduction.”</span> In <em>Advances in Neural Information Processing Systems</em>, 15:705–12.
</div>
<div id="ref-demartines:curvilinear" class="csl-entry" role="doc-biblioentry">
Demartines, P., and J. Herault. 1997. <span>“Curvilinear Component Analysis: A Self-Organizing Neural Network for Nonlinear Mapping of Data Sets.”</span> <em>IEEE Transactions on Neural Networks</em> 8 (1): 148–54. <a href="https://doi.org/10.1109/72.554199">https://doi.org/10.1109/72.554199</a>.
</div>
<div id="ref-battista:algorithms" class="csl-entry" role="doc-biblioentry">
Di Battista, Giuseppe, Peter Eades, Roberto Tamassia, and Ioannis G Tollisi. 1994. <span>“Algorithms for Drawing Graphs: An Annotated Bibliography.”</span> <em>Computational Geometry</em> 4 (5): 235–82. https://doi.org/<a href="https://doi.org/10.1016/0925-7721(94)00014-X">https://doi.org/10.1016/0925-7721(94)00014-X</a>.
</div>
<div id="ref-doyle:random" class="csl-entry" role="doc-biblioentry">
Doyle, Peter G, and J Laurie Snell. 1984. <em>Random Walks and Electric Networks</em>. Vol. 22. American Mathematical Soc.
</div>
<div id="ref-ferreira:visual" class="csl-entry" role="doc-biblioentry">
Ferreira de Oliveira, M. C., and H. Levkowitz. 2003. <span>“From Visual Data Exploration to Visual Data Mining: A Survey.”</span> <em>IEEE Transactions on Visualization and Computer Graphics</em> 9 (3): 378–94. <a href="https://doi.org/10.1109/TVCG.2003.1207445">https://doi.org/10.1109/TVCG.2003.1207445</a>.
</div>
<div id="ref-fokkema:jacobi" class="csl-entry" role="doc-biblioentry">
Fokkema, Diederik R, Gerard LG Sleijpen, and Henk A Van der Vorst. 1998. <span>“Jacobi–Davidson Style QR and QZ Algorithms for the Reduction of Matrix Pencils.”</span> <em>SIAM Journal on Scientific Computing</em> 20 (1): 94–125.
</div>
<div id="ref-grady:random" class="csl-entry" role="doc-biblioentry">
Grady, Leo. 2006. <span>“Random Walks for Image Segmentation.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 28 (11): 1768–83.
</div>
<div id="ref-hinton:stochastic" class="csl-entry" role="doc-biblioentry">
Hinton, Geoffrey E, and Sam Roweis. 2003. <span>“Stochastic Neighbor Embedding.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by S. Becker, S. Thrun, and K. Obermayer. Vol. 15. MIT Press. <a href="https://proceedings.neurips.cc/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf">https://proceedings.neurips.cc/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf</a>.
</div>
<div id="ref-hinton:reducing" class="csl-entry" role="doc-biblioentry">
Hinton, Geoffrey E, and Ruslan R Salakhutdinov. 2006. <span>“Reducing the Dimensionality of Data with Neural Networks.”</span> <em>Science</em> 313 (5786): 504–7.
</div>
<div id="ref-hotelling:analysis" class="csl-entry" role="doc-biblioentry">
Hotelling, H. 1933. <span>“Analysis of a Complex of Statistical Variables into Principal Components.”</span> <em>Journal of Educational Psychology</em> 24: 498–520.
</div>
<div id="ref-jacobs:rates" class="csl-entry" role="doc-biblioentry">
Jacobs, Robert A. 1988. <span>“Increased Rates of Convergence Through Learning Rate Adaptation.”</span> <em>Neural Networks</em> 1 (4): 295–307.
</div>
<div id="ref-kakutani:dirichlet" class="csl-entry" role="doc-biblioentry">
Kakutani, S. 1945. <span>“Markov Processes and the Dirichlet Problem.”</span> In <em>Proceedings of the Japan Academy</em>, 21:227–33.
</div>
<div id="ref-keim:designing" class="csl-entry" role="doc-biblioentry">
Keim, Daniel A. 2000. <span>“Designing Pixel-Oriented Visualization Techniques: Theory and Applications.”</span> <em>IEEE Transactions on Visualization and Computer Graphics</em> 6 (1): 59–78. <a href="https://doi.org/10.1109/2945.841121">https://doi.org/10.1109/2945.841121</a>.
</div>
<div id="ref-lafon:diffusion" class="csl-entry" role="doc-biblioentry">
Lafon, Stephane, and Ann B Lee. 2006. <span>“Diffusion Maps and Coarse-Graining: A Unified Framework for Dimensionality Reduction, Graph Partitioning, and Data Set Parameterization.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 28 (9): 1393–403.
</div>
<div id="ref-lee:robust" class="csl-entry" role="doc-biblioentry">
Lee, John Aldo, Amaury Lendasse, Nicolas Donckers, and Michel Verleysen. 2000. <span>“A Robust Nonlinear Projection Method.”</span> In <em>Proceedings of the 8th European Symposium on Artificial Neural Networks</em>, 13–20.
</div>
<div id="ref-lee:nonlinear2005" class="csl-entry" role="doc-biblioentry">
Lee, John Aldo, and Michel Verleysen. 2005. <span>“Nonlinear Dimensionality Reduction of Data Manifolds with Essential Loops.”</span> <em>Neurocomputing</em> 67: 29–53.
</div>
<div id="ref-lee:nonlinear" class="csl-entry" role="doc-biblioentry">
Lee, John A., and Michel Verleysen. 2007. <em>Nonlinear Dimensionality Reduction</em>. 1st ed. Springer Publishing Company, Incorporated.
</div>
<div id="ref-mardia:multivariate" class="csl-entry" role="doc-biblioentry">
Mardia, Kent, KV, and J Bibby. 1979. <em>Multivariate Analysis</em>. Academic Press Amsterdam.
</div>
<div id="ref-meytlis:face" class="csl-entry" role="doc-biblioentry">
Meytlis, Marsha, and Lawrence Sirovich. 2007. <span>“On the Dimensionality of Face Space.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 29 (7): 1262–67.
</div>
<div id="ref-nadler:diffusion" class="csl-entry" role="doc-biblioentry">
Nadler, B, S Lafon, RR Coifman, and IG Kevrekidis. 2006. <span>“Diffusion Maps, Spectral Clustering, and the Reaction Coordinates of Dynamical Systems.”</span> <em>Applied and Computational Harmonic Analysis: Special Issue on Diffusion Maps and Wavelets</em> 21: 113–27.
</div>
<div id="ref-nene:coil20" class="csl-entry" role="doc-biblioentry">
Nene, Sameer A, Shree K Nayar, and Hiroshi Murase. 1996. <span>“Columbia Object Image Library (COIL-20).”</span> CUCS-005-96. Columbia University.
</div>
<div id="ref-roweis:nonlinear" class="csl-entry" role="doc-biblioentry">
Roweis, Sam T., and Lawrence K. Saul. 2000. <span>“<span class="nocase">Nonlinear Dimensionality Reduction by Locally Linear Embedding</span>.”</span> <em>Science</em> 290 (5500): 2323–26. <a href="https://doi.org/10.1126/science.290.5500.2323">https://doi.org/10.1126/science.290.5500.2323</a>.
</div>
<div id="ref-sammon:nonlinear" class="csl-entry" role="doc-biblioentry">
Sammon, J. W. 1969. <span>“A Nonlinear Mapping for Data Structure Analysis.”</span> <em>IEEE Transactions on Computers</em> C-18 (5): 401–9. <a href="https://doi.org/10.1109/T-C.1969.222678">https://doi.org/10.1109/T-C.1969.222678</a>.
</div>
<div id="ref-song:colored" class="csl-entry" role="doc-biblioentry">
Song, Le, Alexander J Smola, Karsten M Borgwardt, and Arthur Gretton. 2008. <span>“Colored Maximum Variance Unfolding.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by J. Platt, D. Koller, Y. Singer, and S. Roweis. Vol. 20. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2007/file/55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf">https://proceedings.neurips.cc/paper/2007/file/55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf</a>.
</div>
<div id="ref-street:nuclear" class="csl-entry" role="doc-biblioentry">
Street, W. Nick, W. H. Wolberg, and O. L. Mangasarian. 1993. <span>“<span class="nocase">Nuclear feature extraction for breast tumor diagnosis</span>.”</span> In <em>Biomedical Image Processing and Biomedical Visualization</em>, edited by Raj S. Acharya and Dmitry B. Goldgof, 1905:861–70. International Society for Optics; Photonics; SPIE. <a href="https://doi.org/10.1117/12.148698">https://doi.org/10.1117/12.148698</a>.
</div>
<div id="ref-jaakola:partially" class="csl-entry" role="doc-biblioentry">
Szummer, Martin, and Tommi Jaakkola. 2002. <span>“Partially Labeled Classification with Markov Random Walks.”</span> <em>Advances in Neural Information Processing Systems</em> 14: 945–52.
</div>
<div id="ref-tenenbaum:global" class="csl-entry" role="doc-biblioentry">
Tenenbaum, Joshua B., Vin de Silva, and John C. Langford. 2000. <span>“A Global Geometric Framework for Nonlinear Dimensionality Reduction.”</span> <em>Science</em> 290 (5500): 2319.
</div>
<div id="ref-torgerson:multidimensional" class="csl-entry" role="doc-biblioentry">
Torgerson, W. S. 1952. <span>“Multidimensional Scaling: I. Theory and Method.”</span> <em>Psychometrika</em> 17: 401–19.
</div>
<div id="ref-vandermaaten:comparison" class="csl-entry" role="doc-biblioentry">
Van Der Maaten, Laurens, Eric Postma, Jaap Van den Herik, et al. 2009. <span>“Dimensionality Reduction: A Comparative.”</span> <em>J Mach Learn Res</em> 10 (66-71): 13.
</div>
<div id="ref-weinberger:learning" class="csl-entry" role="doc-biblioentry">
Weinberger, Kilian Q., Fei Sha, and Lawrence K. Saul. 2004. <span>“Learning a Kernel Matrix for Nonlinear Dimensionality Reduction.”</span> In <em>Proceedings of the Twenty-First International Conference on Machine Learning</em>, 106. ICML ’04. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/1015330.1015345">https://doi.org/10.1145/1015330.1015345</a>.
</div>
<div id="ref-weinberger:graph" class="csl-entry" role="doc-biblioentry">
Weinberger, Kilian Q, Fei Sha, Qihui Zhu, and Lawrence K Saul. 2007. <span>“Graph Laplacian Regularization for Large-Scale Semidefinite Programming.”</span> In <em>Advances in Neural Information Processing Systems</em>, 1489–96.
</div>
<div id="ref-williams:connection" class="csl-entry" role="doc-biblioentry">
Williams, Christopher KI. 2002. <span>“On a Connection Between Kernel PCA and Metric Multidimensional Scaling.”</span> <em>Machine Learning</em> 46 (1): 11–19.
</div>
<div id="ref-zhu:semi" class="csl-entry" role="doc-biblioentry">
Zhu, Xiaojin, Zoubin Ghahramani, and John D Lafferty. 2003. <span>“Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions.”</span> In <em>Proceedings of the 20th International Conference on Machine Learning (ICML-03)</em>, 912–19.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>SNE can also be applied to data sets that consist of pairwise similarities between objects rather than high-dimensional vector representations of each object, provided these similarities can be interpreted as conditional probabilities. For example, human word associations data consists of the probability of producing each possible word in response to a given word, as a result of which it is already in the form required by SNE.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Setting the variance in the low-dimensional Gaussians to another value only results in a rescaled version of the final map. Note that by using the same variance for every datapoint in the low-dimensional map, we lose the property that the data is a perfect model of itself if we embed it in a space of the same dimensionality, because in the high-dimensional space, we used a different variance <span class="math inline">\sigma_i</span> in each Gaussian.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Note that the perplexity increases monotonically with the variance <span class="math inline">\sigma_i</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Picking the best map after several runs as visualization of the data is not nearly as problematic as picking the model that does best on a test set during supervised learning. In visualization, the aim is to see the structure in the training data, not to generalize to held out test data.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>The MNIST data set is publicly available from <a href="http://yann.lecun.com/exdb/mnist/index.html" class="uri">http://yann.lecun.com/exdb/mnist/index.html</a>.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>The Olivetti data set is publicly available from <a href="http://mambo.ucsc.edu/psl/olivetti.html" class="uri">http://mambo.ucsc.edu/psl/olivetti.html</a>.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Isomap and LLE require data that gives rise to a neighborhood graph that is connected.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>In preliminary experiments, we found the performance of random walk t-SNE to be very robust under changes of <span class="math inline">k</span>.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
    var links = window.document.querySelectorAll('a:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb16" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Visualizing Data using t-SNE"</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "A practical computo example"</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> last-modified</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> |</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co">  This page is a reworking of the original t-SNE article using the Computo template. It aims to help authors submitting to the journal by using some advanced formatting features. We warmly thank the authors of t-SNE and the editor of JMLR for allowing us to use their work to illustrate the Computo spirit.</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> </span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Laurens van der Maaten</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="co">    url: https://lvdmaaten.github.io/</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation: TiCC, Tilburg University</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation-url: https://www.tilburguniversity.edu/</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="co">    orcid: 0000-0002-1931-6828</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Geoffrey Hinton</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="co">    url: https://www.cs.toronto.edu/~hinton/</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation: Department of Computer Science, University of Toronto</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation-url: https://web.cs.toronto.edu/</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="co">    orcid: 0000-0002-8063-7209</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a><span class="an">abstract:</span><span class="co"> |</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="co">  We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map.  The technique is a variation of Stochastic Neighbor Embedding hinton:stochastic that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualization produced by t-SNE are significantly better than those produced by other techniques on almost all of the data sets. &lt;br/&gt;</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="an">keywords:</span><span class="co"> [visualization, dimensionality reduction, manifold learning, embedding algorithms, multidimensional scaling]</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="an">github-user:</span><span class="co"> computorg</span></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a><span class="an">repo:</span><span class="co"> "published-paper-tsne"</span></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a><span class="an">draft:</span><span class="co"> false</span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a><span class="an">published:</span><span class="co"> true</span></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span><span class="co"> </span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a><span class="co">  computo-html: default</span></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a><span class="co">  computo-pdf: default</span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a><span class="co">  keep-ipynb: true</span></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a><span class="fu"># Introduction</span></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>Visualization of high-dimensional data is an important problem in many</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>different domains, and deals with data of widely varying dimensionality. Cell</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>nuclei that are relevant to breast cancer, for example, are described by</span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>approximately 30 variables @street:nuclear, whereas the pixel</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>intensity vectors used to represent images or the word-count vectors used to</span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>represent documents typically have thousands of dimensions. Over the last few</span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>decades, a variety of techniques for the visualization of such high-dimensional</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>data have been proposed, many of which are reviewed by @ferreira:visual.</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>Important techniques include iconographic displays such as Chernoff faces @chernoff:use, </span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>pixel-based techniques @keim:designing, and techniques that represent the dimensions in the </span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>data as vertices in a graph @battista:algorithms. Most of these techniques simply</span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>provide tools to display more than two data dimensions, and leave the</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>interpretation of the data to the human observer. This severely limits the</span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a>applicability of these techniques to real-world data sets that contain</span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>thousands of high-dimensional datapoints.</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>In contrast to the visualization techniques discussed above, dimensionality</span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a>reduction methods convert the high-dimensional data set $\mathcal{X} = {x_1,</span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a>x_2, \dots, x_n}$ into two or three-dimensional data $\mathcal{Y} = {y_1, y_2,</span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a>\dots, y_n}$ that can be displayed in a scatterplot. In the paper, we refer to</span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>the low-dimensional data representation $\mathcal{Y}$ as a map, and to the</span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a>low-dimensional representations $y_i$ of individual datapoints as map</span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a>points. The aim of dimensionality reduction is to preserve as much of the</span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a>significant structure of the high-dimensional data as possible in the</span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a>low-dimensional map. Various techniques for this problem have been proposed</span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a>that differ in the type of structure they preserve. Traditional dimensionality</span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a>reduction techniques such as Principal Components Analysis</span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a>@hotelling:analysis  and classical multidimensional scaling</span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a>@torgerson:multidimensional are linear techniques that focus on keeping</span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a>the low-dimensional representations of dissimilar datapoints far apart. For</span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a>high-dimensional data that lies on or near a low-dimensional, non-linear</span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a>manifold it is usually more important to keep the low-dimensional</span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a>representations of very similar datapoints close together, which is typically</span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a>not possible with a linear mapping.</span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a>A large number of nonlinear dimensionality reduction techniques that aim to</span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a>preserve the local structure of data have been proposed, many of which are</span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true" tabindex="-1"></a>reviewed by @lee:nonlinear. In particular, we mention the following</span>
<span id="cb16-74"><a href="#cb16-74" aria-hidden="true" tabindex="-1"></a>seven techniques: (1) Sammon mapping @sammon:nonlinear, (2) curvilinear</span>
<span id="cb16-75"><a href="#cb16-75" aria-hidden="true" tabindex="-1"></a>components analysis @demartines:curvilinear, (3) Stochastic Neighbor</span>
<span id="cb16-76"><a href="#cb16-76" aria-hidden="true" tabindex="-1"></a>Embedding @hinton:stochastic; (4) Isomap @tenenbaum:global, (5)</span>
<span id="cb16-77"><a href="#cb16-77" aria-hidden="true" tabindex="-1"></a>Maximum Variance Unfolding @weinberger:learning; (6) Locally Linear</span>
<span id="cb16-78"><a href="#cb16-78" aria-hidden="true" tabindex="-1"></a>Embedding @roweis:nonlinear, and (7) Laplacian Eigenmaps</span>
<span id="cb16-79"><a href="#cb16-79" aria-hidden="true" tabindex="-1"></a>@belkin:laplacian. Despite the strong performance of these techniques</span>
<span id="cb16-80"><a href="#cb16-80" aria-hidden="true" tabindex="-1"></a>on artificial data sets, they are often not very successful at visualizing</span>
<span id="cb16-81"><a href="#cb16-81" aria-hidden="true" tabindex="-1"></a>real, high-dimensional data. In particular, most of the techniques are not</span>
<span id="cb16-82"><a href="#cb16-82" aria-hidden="true" tabindex="-1"></a>capable of retaining both the local and the global structure of the data in a</span>
<span id="cb16-83"><a href="#cb16-83" aria-hidden="true" tabindex="-1"></a>single map. For instance, a recent study reveals that even a semi-supervised</span>
<span id="cb16-84"><a href="#cb16-84" aria-hidden="true" tabindex="-1"></a>variant of MVU is not capable of separating handwritten digits into their</span>
<span id="cb16-85"><a href="#cb16-85" aria-hidden="true" tabindex="-1"></a>natural clusters @song:colored.</span>
<span id="cb16-86"><a href="#cb16-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-87"><a href="#cb16-87" aria-hidden="true" tabindex="-1"></a>In this paper, we describe a way of converting a high-dimensional data</span>
<span id="cb16-88"><a href="#cb16-88" aria-hidden="true" tabindex="-1"></a>set into a matrix of pairwise similarities and we introduce a new</span>
<span id="cb16-89"><a href="#cb16-89" aria-hidden="true" tabindex="-1"></a>technique, called "t-SNE", for visualizing the resulting similarity</span>
<span id="cb16-90"><a href="#cb16-90" aria-hidden="true" tabindex="-1"></a>data. t-SNE is capable of capturing much of the local structure of the</span>
<span id="cb16-91"><a href="#cb16-91" aria-hidden="true" tabindex="-1"></a>high-dimensional data very well, while also revealing global structure</span>
<span id="cb16-92"><a href="#cb16-92" aria-hidden="true" tabindex="-1"></a>such as the presence of clusters at several scales. We illustrate the</span>
<span id="cb16-93"><a href="#cb16-93" aria-hidden="true" tabindex="-1"></a>performance of t-SNE by comparing it to the seven dimensionality</span>
<span id="cb16-94"><a href="#cb16-94" aria-hidden="true" tabindex="-1"></a>reduction techniques mentioned above on five data sets from a variety</span>
<span id="cb16-95"><a href="#cb16-95" aria-hidden="true" tabindex="-1"></a>of domains. Because of space limitations, most of the</span>
<span id="cb16-96"><a href="#cb16-96" aria-hidden="true" tabindex="-1"></a>$(7+1)\times5=40$ maps are presented in the supplemental material, but</span>
<span id="cb16-97"><a href="#cb16-97" aria-hidden="true" tabindex="-1"></a>the maps that we present in the paper are sufficient to demonstrate</span>
<span id="cb16-98"><a href="#cb16-98" aria-hidden="true" tabindex="-1"></a>the superiority of t-SNE.</span>
<span id="cb16-99"><a href="#cb16-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-100"><a href="#cb16-100" aria-hidden="true" tabindex="-1"></a>The outline of the paper is as follows. In  @sec-sne, we</span>
<span id="cb16-101"><a href="#cb16-101" aria-hidden="true" tabindex="-1"></a>outline SNE as presented by @hinton:stochastic, which forms</span>
<span id="cb16-102"><a href="#cb16-102" aria-hidden="true" tabindex="-1"></a>the basis for t-SNE. In  @sec-tsne, we present t-SNE, which</span>
<span id="cb16-103"><a href="#cb16-103" aria-hidden="true" tabindex="-1"></a>has two important differences from SNE. In Section</span>
<span id="cb16-104"><a href="#cb16-104" aria-hidden="true" tabindex="-1"></a>@sec-experiments, we describe the experimental setup and the</span>
<span id="cb16-105"><a href="#cb16-105" aria-hidden="true" tabindex="-1"></a>results of our experiments.  Subsequently, @sec-large-data shows how t-SNE can be modified to visualize</span>
<span id="cb16-106"><a href="#cb16-106" aria-hidden="true" tabindex="-1"></a>real-world data sets that contain many more than 10,000</span>
<span id="cb16-107"><a href="#cb16-107" aria-hidden="true" tabindex="-1"></a>datapoints. The results of our experiments are discussed in more</span>
<span id="cb16-108"><a href="#cb16-108" aria-hidden="true" tabindex="-1"></a>detail in  @sec-discussion. Our conclusions and suggestions</span>
<span id="cb16-109"><a href="#cb16-109" aria-hidden="true" tabindex="-1"></a>for future work are presented in  @sec-conclusion.</span>
<span id="cb16-110"><a href="#cb16-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-111"><a href="#cb16-111" aria-hidden="true" tabindex="-1"></a><span class="fu"># Stochastic Neighbor Embedding {#sec-sne}</span></span>
<span id="cb16-112"><a href="#cb16-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-113"><a href="#cb16-113" aria-hidden="true" tabindex="-1"></a>Stochastic Neighbor Embedding (SNE) starts by converting the</span>
<span id="cb16-114"><a href="#cb16-114" aria-hidden="true" tabindex="-1"></a>high-dimensional Euclidean distances between datapoints into</span>
<span id="cb16-115"><a href="#cb16-115" aria-hidden="true" tabindex="-1"></a>conditional probabilities that represent similarities.<span class="ot">[^SNE]</span> The</span>
<span id="cb16-116"><a href="#cb16-116" aria-hidden="true" tabindex="-1"></a>similarity of datapoint $x_j$ to datapoint $x_i$ is the conditional</span>
<span id="cb16-117"><a href="#cb16-117" aria-hidden="true" tabindex="-1"></a>probabilities, $p_{j|i}$, that $x_i$ would pick $x_j$ as its neighbor</span>
<span id="cb16-118"><a href="#cb16-118" aria-hidden="true" tabindex="-1"></a>if neighbors were picked in proportion to their probability density</span>
<span id="cb16-119"><a href="#cb16-119" aria-hidden="true" tabindex="-1"></a>under a Gaussian centered at $x_i$. For nearby datapoints, $p_{j|i}$</span>
<span id="cb16-120"><a href="#cb16-120" aria-hidden="true" tabindex="-1"></a>is relatively high, whereas for widely separated datapoints, $p_{j|i}$</span>
<span id="cb16-121"><a href="#cb16-121" aria-hidden="true" tabindex="-1"></a>will be almost infinitesimal (for reasonable values of the variance of</span>
<span id="cb16-122"><a href="#cb16-122" aria-hidden="true" tabindex="-1"></a>the Gaussian, $\sigma_i$). Mathematically, the conditional probability</span>
<span id="cb16-123"><a href="#cb16-123" aria-hidden="true" tabindex="-1"></a>$p_{j|i}$ is given by</span>
<span id="cb16-124"><a href="#cb16-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-125"><a href="#cb16-125" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-126"><a href="#cb16-126" aria-hidden="true" tabindex="-1"></a>p_{j|i} = \frac{\exp(-\|x_i - x_j\|^2/(2\sigma_i^2))}{\sum_{k\neq i} \exp(-\|x_i</span>
<span id="cb16-127"><a href="#cb16-127" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>x_k\|^2 / 2 \sigma_i^2)}\,.</span>
<span id="cb16-128"><a href="#cb16-128" aria-hidden="true" tabindex="-1"></a>$$ {#eq-sne_large_space}</span>
<span id="cb16-129"><a href="#cb16-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-130"><a href="#cb16-130" aria-hidden="true" tabindex="-1"></a>where $\sigma_i$ is the variance of the Gaussian that is centered on datapoint</span>
<span id="cb16-131"><a href="#cb16-131" aria-hidden="true" tabindex="-1"></a>$x_i$. The method for determining the value of $\sigma_i$ is presented later</span>
<span id="cb16-132"><a href="#cb16-132" aria-hidden="true" tabindex="-1"></a>in this section. Because we are only interested in modeling pairwise</span>
<span id="cb16-133"><a href="#cb16-133" aria-hidden="true" tabindex="-1"></a>similarities, we set the value of $p_{i|i}$ to zero. For the low-dimensional</span>
<span id="cb16-134"><a href="#cb16-134" aria-hidden="true" tabindex="-1"></a>counterparts $y_i$ and $y_j$ of the high-dimensional datapoints $x_i$ and $x_j$,</span>
<span id="cb16-135"><a href="#cb16-135" aria-hidden="true" tabindex="-1"></a>it is possible to compute a similar conditional probability, which we denote</span>
<span id="cb16-136"><a href="#cb16-136" aria-hidden="true" tabindex="-1"></a>by $q_{j|i}$. We set <span class="ot">[^variance]</span> the variance of the Gaussian that is employed</span>
<span id="cb16-137"><a href="#cb16-137" aria-hidden="true" tabindex="-1"></a>in the computation of the conditional probabilities $q_{j|i}$ to</span>
<span id="cb16-138"><a href="#cb16-138" aria-hidden="true" tabindex="-1"></a>$\frac{1}{\sqrt{2}}$. Hence, we model the similarity of a map point $y_j$ to</span>
<span id="cb16-139"><a href="#cb16-139" aria-hidden="true" tabindex="-1"></a>map point $y_i$ by</span>
<span id="cb16-140"><a href="#cb16-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-141"><a href="#cb16-141" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-142"><a href="#cb16-142" aria-hidden="true" tabindex="-1"></a>q_{j|i} = \frac{\exp(-\|y_i - y_j\|^2)}{\sum_{k \neq i} \exp(-\|y_i</span>
<span id="cb16-143"><a href="#cb16-143" aria-hidden="true" tabindex="-1"></a>-y_k\|^2)}\,.</span>
<span id="cb16-144"><a href="#cb16-144" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-145"><a href="#cb16-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-146"><a href="#cb16-146" aria-hidden="true" tabindex="-1"></a><span class="ot">[^SNE]: </span>SNE can also be applied to data sets that consist of pairwise</span>
<span id="cb16-147"><a href="#cb16-147" aria-hidden="true" tabindex="-1"></a>  similarities between objects rather than high-dimensional vector</span>
<span id="cb16-148"><a href="#cb16-148" aria-hidden="true" tabindex="-1"></a>  representations of each object, provided these similarities can be</span>
<span id="cb16-149"><a href="#cb16-149" aria-hidden="true" tabindex="-1"></a>  interpreted as conditional probabilities. For example, human word</span>
<span id="cb16-150"><a href="#cb16-150" aria-hidden="true" tabindex="-1"></a>  associations data consists of the probability of producing each possible</span>
<span id="cb16-151"><a href="#cb16-151" aria-hidden="true" tabindex="-1"></a>  word in response to a given word, as a result of which it is already in the</span>
<span id="cb16-152"><a href="#cb16-152" aria-hidden="true" tabindex="-1"></a>  form required by SNE.</span>
<span id="cb16-153"><a href="#cb16-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-154"><a href="#cb16-154" aria-hidden="true" tabindex="-1"></a><span class="ot">[^variance]: </span>Setting the variance in the low-dimensional Gaussians to another</span>
<span id="cb16-155"><a href="#cb16-155" aria-hidden="true" tabindex="-1"></a>  value only results in a rescaled version of the final map. Note that by</span>
<span id="cb16-156"><a href="#cb16-156" aria-hidden="true" tabindex="-1"></a>  using the same variance for every datapoint in the low-dimensional map, we</span>
<span id="cb16-157"><a href="#cb16-157" aria-hidden="true" tabindex="-1"></a>  lose the property that the data is a perfect model of itself if we embed it</span>
<span id="cb16-158"><a href="#cb16-158" aria-hidden="true" tabindex="-1"></a>  in a space of the same dimensionality, because in the high-dimensional</span>
<span id="cb16-159"><a href="#cb16-159" aria-hidden="true" tabindex="-1"></a>  space, we used a different variance $\sigma_i$ in each Gaussian.</span>
<span id="cb16-160"><a href="#cb16-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-161"><a href="#cb16-161" aria-hidden="true" tabindex="-1"></a>Again, since we are only interested in modeling pairwise similarities, we set</span>
<span id="cb16-162"><a href="#cb16-162" aria-hidden="true" tabindex="-1"></a>$q_{i|i}=0$.</span>
<span id="cb16-163"><a href="#cb16-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-164"><a href="#cb16-164" aria-hidden="true" tabindex="-1"></a>If the map points $y_i$ and $y_j$ correctly model the similarity between the</span>
<span id="cb16-165"><a href="#cb16-165" aria-hidden="true" tabindex="-1"></a>high-dimensional data-points $x_i$ and $x_j$, the conditional probabilities</span>
<span id="cb16-166"><a href="#cb16-166" aria-hidden="true" tabindex="-1"></a>$p_{j|i}$ and $q_{j|i}$ will be equal. Motivated by this observation, SNE aims</span>
<span id="cb16-167"><a href="#cb16-167" aria-hidden="true" tabindex="-1"></a>to find a low-dimensional data representation that minimizes the mismatch</span>
<span id="cb16-168"><a href="#cb16-168" aria-hidden="true" tabindex="-1"></a>between $p_{j|i}$ and $q_{j|i}$. A natural measure of the faithfulness with</span>
<span id="cb16-169"><a href="#cb16-169" aria-hidden="true" tabindex="-1"></a>which $q_{j|i}$ models $p_{j|i}$ is the Kullback-Leibler divergence (which is</span>
<span id="cb16-170"><a href="#cb16-170" aria-hidden="true" tabindex="-1"></a>in the case equal to the cross-entropy up to an additive constant). SNE</span>
<span id="cb16-171"><a href="#cb16-171" aria-hidden="true" tabindex="-1"></a>minimizes the sum of Kullback-Leibler divergences over all datapoints using a</span>
<span id="cb16-172"><a href="#cb16-172" aria-hidden="true" tabindex="-1"></a>gradient descent method. The cost function $C$ is given by</span>
<span id="cb16-173"><a href="#cb16-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-174"><a href="#cb16-174" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-175"><a href="#cb16-175" aria-hidden="true" tabindex="-1"></a>C = \sum_i KL(P_i\|Q_i) = \sum_i \sum_j p_{j|i} \log</span>
<span id="cb16-176"><a href="#cb16-176" aria-hidden="true" tabindex="-1"></a>\frac{p_{j|i}}{q_{j|i}}\,,</span>
<span id="cb16-177"><a href="#cb16-177" aria-hidden="true" tabindex="-1"></a>$$ {#eq-sne_cost_function}</span>
<span id="cb16-178"><a href="#cb16-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-179"><a href="#cb16-179" aria-hidden="true" tabindex="-1"></a>in which $P_i$ represents the conditional probability distribution</span>
<span id="cb16-180"><a href="#cb16-180" aria-hidden="true" tabindex="-1"></a>over all other datapoints given datapoint $x_i$, and $Q_i$ represents</span>
<span id="cb16-181"><a href="#cb16-181" aria-hidden="true" tabindex="-1"></a>the conditional probability distribution over all other map points</span>
<span id="cb16-182"><a href="#cb16-182" aria-hidden="true" tabindex="-1"></a>given map point $y_i$.  Because the Kullback-Liebler divergence is not</span>
<span id="cb16-183"><a href="#cb16-183" aria-hidden="true" tabindex="-1"></a>symmetric, different types of error in the pairwise distances in the</span>
<span id="cb16-184"><a href="#cb16-184" aria-hidden="true" tabindex="-1"></a>low-dimensional map are not weighted equally. In particular, there is</span>
<span id="cb16-185"><a href="#cb16-185" aria-hidden="true" tabindex="-1"></a>a large cost for using widely separated map points to represent nearby</span>
<span id="cb16-186"><a href="#cb16-186" aria-hidden="true" tabindex="-1"></a>datapoints (i.e, for using a small $q_{j|i}$ to model a large</span>
<span id="cb16-187"><a href="#cb16-187" aria-hidden="true" tabindex="-1"></a>$p_{j|i}$), but there is only a small cost for using nearby map points</span>
<span id="cb16-188"><a href="#cb16-188" aria-hidden="true" tabindex="-1"></a>to represent widely separated datapoints. This small cost comes from</span>
<span id="cb16-189"><a href="#cb16-189" aria-hidden="true" tabindex="-1"></a>wasting some of the probability mass in the relevant $Q$</span>
<span id="cb16-190"><a href="#cb16-190" aria-hidden="true" tabindex="-1"></a>distributions. In other words, the SNE cost function focuses on</span>
<span id="cb16-191"><a href="#cb16-191" aria-hidden="true" tabindex="-1"></a>retaining the local structure of the data in the map (for reasonable</span>
<span id="cb16-192"><a href="#cb16-192" aria-hidden="true" tabindex="-1"></a>values of the variance of the Gaussian in the high-dimensional space,</span>
<span id="cb16-193"><a href="#cb16-193" aria-hidden="true" tabindex="-1"></a>$\sigma_i$).</span>
<span id="cb16-194"><a href="#cb16-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-195"><a href="#cb16-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-196"><a href="#cb16-196" aria-hidden="true" tabindex="-1"></a>The remaining parameter to be selected the variance $\sigma_i$ of the Gaussian</span>
<span id="cb16-197"><a href="#cb16-197" aria-hidden="true" tabindex="-1"></a>that is centered over each high-dimensional datapoint, $x_i$.</span>
<span id="cb16-198"><a href="#cb16-198" aria-hidden="true" tabindex="-1"></a>It is not likely that there is a single value of $\sigma_i$ that is optimal</span>
<span id="cb16-199"><a href="#cb16-199" aria-hidden="true" tabindex="-1"></a>for all datapoints in the data set because the density of the data is likely</span>
<span id="cb16-200"><a href="#cb16-200" aria-hidden="true" tabindex="-1"></a>to vary. In dense regions, a smaller value of $\sigma_i$ is usually more</span>
<span id="cb16-201"><a href="#cb16-201" aria-hidden="true" tabindex="-1"></a>appropriate than in sparser regions. Any particular value of $\sigma_i$</span>
<span id="cb16-202"><a href="#cb16-202" aria-hidden="true" tabindex="-1"></a>induces a probability distribution, $P_i$, over all of the other datapoints.</span>
<span id="cb16-203"><a href="#cb16-203" aria-hidden="true" tabindex="-1"></a>This distribution has an entropy which increases as $\sigma_i$ increases. SNE</span>
<span id="cb16-204"><a href="#cb16-204" aria-hidden="true" tabindex="-1"></a>performs a binary search for the value of $\sigma_i$ that produces a $P_i$</span>
<span id="cb16-205"><a href="#cb16-205" aria-hidden="true" tabindex="-1"></a>with a fixed perplexity that is specified by the user<span class="ot">[^perplexity]</span>. The</span>
<span id="cb16-206"><a href="#cb16-206" aria-hidden="true" tabindex="-1"></a>perplexity is defined as</span>
<span id="cb16-207"><a href="#cb16-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-208"><a href="#cb16-208" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-209"><a href="#cb16-209" aria-hidden="true" tabindex="-1"></a>Perp(P_i) = 2^{H(P_i)}\,,</span>
<span id="cb16-210"><a href="#cb16-210" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-211"><a href="#cb16-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-212"><a href="#cb16-212" aria-hidden="true" tabindex="-1"></a>where $H(P_i)$ is the Shannon entropy of $P_i$ measured in bits</span>
<span id="cb16-213"><a href="#cb16-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-214"><a href="#cb16-214" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-215"><a href="#cb16-215" aria-hidden="true" tabindex="-1"></a>H(P_i) = - \sum_j p_{j|i} \log_2 p_{j|i}\,.</span>
<span id="cb16-216"><a href="#cb16-216" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-217"><a href="#cb16-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-218"><a href="#cb16-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-219"><a href="#cb16-219" aria-hidden="true" tabindex="-1"></a><span class="ot">[^perplexity]: </span>Note that the perplexity increases monotonically with the variance</span>
<span id="cb16-220"><a href="#cb16-220" aria-hidden="true" tabindex="-1"></a>  $\sigma_i$.</span>
<span id="cb16-221"><a href="#cb16-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-222"><a href="#cb16-222" aria-hidden="true" tabindex="-1"></a>The perplexity can be interpreted as a smooth measure of the effective number</span>
<span id="cb16-223"><a href="#cb16-223" aria-hidden="true" tabindex="-1"></a>of neighbors. The performance of SNE is fairly robust to changes in the</span>
<span id="cb16-224"><a href="#cb16-224" aria-hidden="true" tabindex="-1"></a>perplexity, and typical values are between 5 and 50.</span>
<span id="cb16-225"><a href="#cb16-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-226"><a href="#cb16-226" aria-hidden="true" tabindex="-1"></a>The minimization of the cost function in @eq-sne_cost_function is</span>
<span id="cb16-227"><a href="#cb16-227" aria-hidden="true" tabindex="-1"></a>performed using a gradient descent method. The gradient has a surprisingly</span>
<span id="cb16-228"><a href="#cb16-228" aria-hidden="true" tabindex="-1"></a>simple form</span>
<span id="cb16-229"><a href="#cb16-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-230"><a href="#cb16-230" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-231"><a href="#cb16-231" aria-hidden="true" tabindex="-1"></a>\frac{\partial C}{\partial y_i} = 2 \sum_j (p_{j|i} - q_{j|i} + p_{i|j} -</span>
<span id="cb16-232"><a href="#cb16-232" aria-hidden="true" tabindex="-1"></a>q_{i|j})(y_i - y_j)\,.</span>
<span id="cb16-233"><a href="#cb16-233" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-234"><a href="#cb16-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-235"><a href="#cb16-235" aria-hidden="true" tabindex="-1"></a>Physically, the gradient may be interpreted as the resultant force created by</span>
<span id="cb16-236"><a href="#cb16-236" aria-hidden="true" tabindex="-1"></a>a set of springs between the map point $y_i$ and all other map points $y_j$.</span>
<span id="cb16-237"><a href="#cb16-237" aria-hidden="true" tabindex="-1"></a>All springs exert a force along the direction $(y_i - y_j)$. The spring</span>
<span id="cb16-238"><a href="#cb16-238" aria-hidden="true" tabindex="-1"></a>between $y_i$ and $y_j$ repels or attracts the map points depending on whether</span>
<span id="cb16-239"><a href="#cb16-239" aria-hidden="true" tabindex="-1"></a>the distance between the two in the map is too small or too large to represent</span>
<span id="cb16-240"><a href="#cb16-240" aria-hidden="true" tabindex="-1"></a>the similarities between the two high-dimensional datapoints. The force</span>
<span id="cb16-241"><a href="#cb16-241" aria-hidden="true" tabindex="-1"></a>exerted by the spring between $y_i$ and $y_j$ is proportional to its length,</span>
<span id="cb16-242"><a href="#cb16-242" aria-hidden="true" tabindex="-1"></a>and also proportional to its stiffness, which is the mismatch $(p_{j|i} -</span>
<span id="cb16-243"><a href="#cb16-243" aria-hidden="true" tabindex="-1"></a>q_{j|i} + p_{i|j} + q_{i|j})$ between the pairwise similarities of the data</span>
<span id="cb16-244"><a href="#cb16-244" aria-hidden="true" tabindex="-1"></a>points.</span>
<span id="cb16-245"><a href="#cb16-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-246"><a href="#cb16-246" aria-hidden="true" tabindex="-1"></a>The gradient descent is initialized by sampling map points randomly from an</span>
<span id="cb16-247"><a href="#cb16-247" aria-hidden="true" tabindex="-1"></a>isotropic Gaussian with small variance that is centered around the origin. In</span>
<span id="cb16-248"><a href="#cb16-248" aria-hidden="true" tabindex="-1"></a>order to speed up the optimization and to avoid poor local minima, a</span>
<span id="cb16-249"><a href="#cb16-249" aria-hidden="true" tabindex="-1"></a>relatively large momentum term is added to the gradient. In other words, the</span>
<span id="cb16-250"><a href="#cb16-250" aria-hidden="true" tabindex="-1"></a>current gradient is added to an exponentially decaying sum of previous</span>
<span id="cb16-251"><a href="#cb16-251" aria-hidden="true" tabindex="-1"></a>gradients in order to determine the changes in the coordinates of the map</span>
<span id="cb16-252"><a href="#cb16-252" aria-hidden="true" tabindex="-1"></a>points at each iteration of the gradient search. Mathematically, the gradient</span>
<span id="cb16-253"><a href="#cb16-253" aria-hidden="true" tabindex="-1"></a>update with a momentum term is given by</span>
<span id="cb16-254"><a href="#cb16-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-255"><a href="#cb16-255" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-256"><a href="#cb16-256" aria-hidden="true" tabindex="-1"></a>\mathcal{Y}^{(t)} = \mathcal{Y}^{(t-1)} + \eta \frac{\partial C}{\partial</span>
<span id="cb16-257"><a href="#cb16-257" aria-hidden="true" tabindex="-1"></a>\mathcal{Y}} + \alpha(t) \left( \mathcal{Y}^{(t-1)} - \mathcal{Y}^{(t-2)}\right)\,,</span>
<span id="cb16-258"><a href="#cb16-258" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-259"><a href="#cb16-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-260"><a href="#cb16-260" aria-hidden="true" tabindex="-1"></a>where $\mathcal{Y}^{(t)}$ indicates the solution at iteration $t$, $\eta$</span>
<span id="cb16-261"><a href="#cb16-261" aria-hidden="true" tabindex="-1"></a>indicates the learning rate, and $\alpha(t)$ represents the momentum at</span>
<span id="cb16-262"><a href="#cb16-262" aria-hidden="true" tabindex="-1"></a>iteration $t$.</span>
<span id="cb16-263"><a href="#cb16-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-264"><a href="#cb16-264" aria-hidden="true" tabindex="-1"></a>In addition, in the early stages of the optimization, Gaussian noise is added</span>
<span id="cb16-265"><a href="#cb16-265" aria-hidden="true" tabindex="-1"></a>to the map points after each iteration. Gradually reducing the variance of</span>
<span id="cb16-266"><a href="#cb16-266" aria-hidden="true" tabindex="-1"></a>this noise performs a type of simulated annealing that helps the optimization</span>
<span id="cb16-267"><a href="#cb16-267" aria-hidden="true" tabindex="-1"></a>to escape from poor local minima in the cost function. If the variance of the</span>
<span id="cb16-268"><a href="#cb16-268" aria-hidden="true" tabindex="-1"></a>noise changes very slowly at the critical point at which the global structure</span>
<span id="cb16-269"><a href="#cb16-269" aria-hidden="true" tabindex="-1"></a>of the map starts to form, SNE tends to find maps with a better global</span>
<span id="cb16-270"><a href="#cb16-270" aria-hidden="true" tabindex="-1"></a>organization. Unfortunately, this requires sensible choices of the initial</span>
<span id="cb16-271"><a href="#cb16-271" aria-hidden="true" tabindex="-1"></a>amount of Gaussian noise and the rate at which it decays. Moreover, these</span>
<span id="cb16-272"><a href="#cb16-272" aria-hidden="true" tabindex="-1"></a>choices interact with the amount of momentum and the step size that are</span>
<span id="cb16-273"><a href="#cb16-273" aria-hidden="true" tabindex="-1"></a>employed in the gradient descent. It is therefore common to run the</span>
<span id="cb16-274"><a href="#cb16-274" aria-hidden="true" tabindex="-1"></a>optimization several times on a data set to find appropriate values for the</span>
<span id="cb16-275"><a href="#cb16-275" aria-hidden="true" tabindex="-1"></a>parameters.<span class="ot">[^global_optimization]</span> In this respect, SNE is inferior to methods</span>
<span id="cb16-276"><a href="#cb16-276" aria-hidden="true" tabindex="-1"></a>that allow convex optimization and it would be useful to find an optimization</span>
<span id="cb16-277"><a href="#cb16-277" aria-hidden="true" tabindex="-1"></a>method that gives good results without requiring the extra computation time</span>
<span id="cb16-278"><a href="#cb16-278" aria-hidden="true" tabindex="-1"></a>and parameter choices introduced by the simulated annealing.</span>
<span id="cb16-279"><a href="#cb16-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-280"><a href="#cb16-280" aria-hidden="true" tabindex="-1"></a><span class="ot">[^global_optimization]: </span>Picking the best map after several runs as</span>
<span id="cb16-281"><a href="#cb16-281" aria-hidden="true" tabindex="-1"></a>  visualization of the data is not nearly as problematic as picking the model</span>
<span id="cb16-282"><a href="#cb16-282" aria-hidden="true" tabindex="-1"></a>  that does best on a test set during supervised learning. In visualization,</span>
<span id="cb16-283"><a href="#cb16-283" aria-hidden="true" tabindex="-1"></a>  the aim is to see the structure in the training data, not to generalize to</span>
<span id="cb16-284"><a href="#cb16-284" aria-hidden="true" tabindex="-1"></a>  held out test data.</span>
<span id="cb16-285"><a href="#cb16-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-286"><a href="#cb16-286" aria-hidden="true" tabindex="-1"></a><span class="fu"># t-Distributed Stochastic Neighbor Embedding {#sec-tsne}</span></span>
<span id="cb16-287"><a href="#cb16-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-288"><a href="#cb16-288" aria-hidden="true" tabindex="-1"></a>@sec-sne discussed SNE as it was presented by</span>
<span id="cb16-289"><a href="#cb16-289" aria-hidden="true" tabindex="-1"></a>@hinton:stochastic. Although SNE constructs reasonably good</span>
<span id="cb16-290"><a href="#cb16-290" aria-hidden="true" tabindex="-1"></a>visualizations, it is hampered by a cost function that is difficult to</span>
<span id="cb16-291"><a href="#cb16-291" aria-hidden="true" tabindex="-1"></a>optimize and by a problem we refer to as the "crowding problem." In this</span>
<span id="cb16-292"><a href="#cb16-292" aria-hidden="true" tabindex="-1"></a>section, we present a new technique called "t-Distributed Stochastic Neighbor</span>
<span id="cb16-293"><a href="#cb16-293" aria-hidden="true" tabindex="-1"></a>Embedding" or "t-SNE" that aims to alleviate these problems. The cost</span>
<span id="cb16-294"><a href="#cb16-294" aria-hidden="true" tabindex="-1"></a>function used by t-SNE differs from the one used by SNE in two ways: (1) it</span>
<span id="cb16-295"><a href="#cb16-295" aria-hidden="true" tabindex="-1"></a>uses a symmetrized version of the SNE cost function with simpler gradients</span>
<span id="cb16-296"><a href="#cb16-296" aria-hidden="true" tabindex="-1"></a>that was briefly introduced by @cook:visualizing and (2) it uses a</span>
<span id="cb16-297"><a href="#cb16-297" aria-hidden="true" tabindex="-1"></a>Student-t distribution rather than a Gaussian to compute the similarity</span>
<span id="cb16-298"><a href="#cb16-298" aria-hidden="true" tabindex="-1"></a>between two points *in the low dimensional space*. t-SNE employs a</span>
<span id="cb16-299"><a href="#cb16-299" aria-hidden="true" tabindex="-1"></a>heavy-tailed distribution in the low-dimensional space to alleviate both the</span>
<span id="cb16-300"><a href="#cb16-300" aria-hidden="true" tabindex="-1"></a>crowding problem and the optimization problems of SNE.</span>
<span id="cb16-301"><a href="#cb16-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-302"><a href="#cb16-302" aria-hidden="true" tabindex="-1"></a>In this section, we first discuss the symmetric version of SNE (@sec-symmetric_sne).</span>
<span id="cb16-303"><a href="#cb16-303" aria-hidden="true" tabindex="-1"></a>Subsequently, we discuss the crowding problem</span>
<span id="cb16-304"><a href="#cb16-304" aria-hidden="true" tabindex="-1"></a>(@sec-crowding), and the use of heavy-tailed distributions to</span>
<span id="cb16-305"><a href="#cb16-305" aria-hidden="true" tabindex="-1"></a>address this problem (@sec-heavy_tail). We conclude the</span>
<span id="cb16-306"><a href="#cb16-306" aria-hidden="true" tabindex="-1"></a> by describing our approach to the optimization of the t-SNE cost</span>
<span id="cb16-307"><a href="#cb16-307" aria-hidden="true" tabindex="-1"></a>function (@sec-optimization_methods_for_tsne).</span>
<span id="cb16-308"><a href="#cb16-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-309"><a href="#cb16-309" aria-hidden="true" tabindex="-1"></a><span class="fu">## Symmetric SNE {#sec-symmetric_sne}</span></span>
<span id="cb16-310"><a href="#cb16-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-311"><a href="#cb16-311" aria-hidden="true" tabindex="-1"></a>As an alternative to minimizing the sum of the Kullback-Leibler divergences</span>
<span id="cb16-312"><a href="#cb16-312" aria-hidden="true" tabindex="-1"></a>between the conditional probabilities $p_{j|i}$ and $q_{j|i}$, it is also</span>
<span id="cb16-313"><a href="#cb16-313" aria-hidden="true" tabindex="-1"></a>possible to minimize a single Kullback-Leibler divergence between a joint</span>
<span id="cb16-314"><a href="#cb16-314" aria-hidden="true" tabindex="-1"></a>probability distribution, $P$, in the high-dimensional space and a joint</span>
<span id="cb16-315"><a href="#cb16-315" aria-hidden="true" tabindex="-1"></a>probability distribution, $Q$, in the low-dimensional space:</span>
<span id="cb16-316"><a href="#cb16-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-317"><a href="#cb16-317" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-318"><a href="#cb16-318" aria-hidden="true" tabindex="-1"></a>C = KL(P\|Q) = \sum_i \sum_j p_{ij} \log \frac{p_{ij}}{q_{ij}}\,,</span>
<span id="cb16-319"><a href="#cb16-319" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-320"><a href="#cb16-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-321"><a href="#cb16-321" aria-hidden="true" tabindex="-1"></a>where again, we set $p_{ij}$ and $q_{ii}$ to zero. We refer to this type of</span>
<span id="cb16-322"><a href="#cb16-322" aria-hidden="true" tabindex="-1"></a>SNE as symmetric SNE, because it has the property that $p_{ij} = p_{ji}$ and</span>
<span id="cb16-323"><a href="#cb16-323" aria-hidden="true" tabindex="-1"></a>$q_{ij} = q_{ji}$ for all $i, j$. In symmetric SNE, the pairwise</span>
<span id="cb16-324"><a href="#cb16-324" aria-hidden="true" tabindex="-1"></a>similarities in the low-dimensional map $q_{ij}$ are given by</span>
<span id="cb16-325"><a href="#cb16-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-326"><a href="#cb16-326" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-327"><a href="#cb16-327" aria-hidden="true" tabindex="-1"></a>q_{ij} = \frac{\exp(-\|x_i - x_j\|^2)}{\sum_{k\neq l} \exp(-\|x_k -</span>
<span id="cb16-328"><a href="#cb16-328" aria-hidden="true" tabindex="-1"></a>x_l\|^2)}\,.</span>
<span id="cb16-329"><a href="#cb16-329" aria-hidden="true" tabindex="-1"></a>$$ {#eq-pairwise_similarities}</span>
<span id="cb16-330"><a href="#cb16-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-331"><a href="#cb16-331" aria-hidden="true" tabindex="-1"></a>The obvious way to define the pairwise similarities in the high-dimensional</span>
<span id="cb16-332"><a href="#cb16-332" aria-hidden="true" tabindex="-1"></a>space $p_{ij}$ is</span>
<span id="cb16-333"><a href="#cb16-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-334"><a href="#cb16-334" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-335"><a href="#cb16-335" aria-hidden="true" tabindex="-1"></a>p_{ij} = \frac{\exp(-\|x_i - x_j\|^2/2\sigma^2)}{\sum_{k\neq l} \exp(-\|x_k -</span>
<span id="cb16-336"><a href="#cb16-336" aria-hidden="true" tabindex="-1"></a>x_l\|^2/2\sigma^2)}\,</span>
<span id="cb16-337"><a href="#cb16-337" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-338"><a href="#cb16-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-339"><a href="#cb16-339" aria-hidden="true" tabindex="-1"></a>but this causes problems when a high-dimensional datapoint $x_i$ is an outlier</span>
<span id="cb16-340"><a href="#cb16-340" aria-hidden="true" tabindex="-1"></a>(i.e., all pairwise distances $\|x_i - x_j\|^2$ are large for $x_i$). For such</span>
<span id="cb16-341"><a href="#cb16-341" aria-hidden="true" tabindex="-1"></a>an outlier, the values of $p_{ij}$ are extremely small for all $j$, so the</span>
<span id="cb16-342"><a href="#cb16-342" aria-hidden="true" tabindex="-1"></a>location of its low-dimensional map point $y_i$ has very little effect on the</span>
<span id="cb16-343"><a href="#cb16-343" aria-hidden="true" tabindex="-1"></a>cost function. As a result, the position of the map point is not well</span>
<span id="cb16-344"><a href="#cb16-344" aria-hidden="true" tabindex="-1"></a>determined by the positions of the other map points. We circumvent this</span>
<span id="cb16-345"><a href="#cb16-345" aria-hidden="true" tabindex="-1"></a>problem by defining the joint probabilities $p_{ij}$ in the high dimensional</span>
<span id="cb16-346"><a href="#cb16-346" aria-hidden="true" tabindex="-1"></a>space to be symmetrized conditional probabilities, that is, we set $p_{ij} =</span>
<span id="cb16-347"><a href="#cb16-347" aria-hidden="true" tabindex="-1"></a>\frac{p_{j|i} + p_{i|j}}{2n}$. This ensures that $\sum_j p_{ij} &gt;</span>
<span id="cb16-348"><a href="#cb16-348" aria-hidden="true" tabindex="-1"></a>\frac{1}{2n}$ for all datapoints $x_i$, as a result of which each datapoint</span>
<span id="cb16-349"><a href="#cb16-349" aria-hidden="true" tabindex="-1"></a>$x_i$ makes a significant contribution to the cost function. In the</span>
<span id="cb16-350"><a href="#cb16-350" aria-hidden="true" tabindex="-1"></a>low-dimensional space, symmetric SNE simply uses @eq-pairwise_similarities. </span>
<span id="cb16-351"><a href="#cb16-351" aria-hidden="true" tabindex="-1"></a>The main advantage of the symmetric version of</span>
<span id="cb16-352"><a href="#cb16-352" aria-hidden="true" tabindex="-1"></a>SNE is the simpler form of its gradient, which is faster to compute. The</span>
<span id="cb16-353"><a href="#cb16-353" aria-hidden="true" tabindex="-1"></a>gradient of symmetric SNE is fairly similar to that of asymmetric SNE, and is</span>
<span id="cb16-354"><a href="#cb16-354" aria-hidden="true" tabindex="-1"></a>given by</span>
<span id="cb16-355"><a href="#cb16-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-356"><a href="#cb16-356" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-357"><a href="#cb16-357" aria-hidden="true" tabindex="-1"></a>\frac{\partial C}{\partial y_i} = 4 \sum_j (p_{ij} - q_{ij})(y_i - y_j)\,.</span>
<span id="cb16-358"><a href="#cb16-358" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-359"><a href="#cb16-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-360"><a href="#cb16-360" aria-hidden="true" tabindex="-1"></a>In preliminary experiments, we observed that symmetric SNE seems to produce</span>
<span id="cb16-361"><a href="#cb16-361" aria-hidden="true" tabindex="-1"></a>maps that are just as good as asymmetric SNE, and sometimes even a little</span>
<span id="cb16-362"><a href="#cb16-362" aria-hidden="true" tabindex="-1"></a>better.</span>
<span id="cb16-363"><a href="#cb16-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-364"><a href="#cb16-364" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Crowding Problem {#sec-crowding}</span></span>
<span id="cb16-365"><a href="#cb16-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-366"><a href="#cb16-366" aria-hidden="true" tabindex="-1"></a>Consider a set of datapoints that lie on a two-dimensional curved manifold</span>
<span id="cb16-367"><a href="#cb16-367" aria-hidden="true" tabindex="-1"></a>which is approximately linear on a small scale, and which is embedded within a</span>
<span id="cb16-368"><a href="#cb16-368" aria-hidden="true" tabindex="-1"></a>higher-dimensional space. It is possible to model the small pairwise distances</span>
<span id="cb16-369"><a href="#cb16-369" aria-hidden="true" tabindex="-1"></a>between datapoints fairly well in a two-dimensional map, which is often</span>
<span id="cb16-370"><a href="#cb16-370" aria-hidden="true" tabindex="-1"></a>illustrated on toy examples such as the "Swiss roll" data set. Now suppose that</span>
<span id="cb16-371"><a href="#cb16-371" aria-hidden="true" tabindex="-1"></a>the manifold has ten intrinsic dimensions<span class="ot">[^dataset]</span> and is embedded within a</span>
<span id="cb16-372"><a href="#cb16-372" aria-hidden="true" tabindex="-1"></a>space of much higher dimensionality. There are several reasons why the</span>
<span id="cb16-373"><a href="#cb16-373" aria-hidden="true" tabindex="-1"></a>pairwise distances in a two-dimensional map cannot faithfully model distances</span>
<span id="cb16-374"><a href="#cb16-374" aria-hidden="true" tabindex="-1"></a>between points on the ten-dimensional manifold. For instance, in ten</span>
<span id="cb16-375"><a href="#cb16-375" aria-hidden="true" tabindex="-1"></a>dimensions, it is possible to have 11 datapoints that are mutually</span>
<span id="cb16-376"><a href="#cb16-376" aria-hidden="true" tabindex="-1"></a>equidistant and there is no way to model this faithfully in a two-dimensional</span>
<span id="cb16-377"><a href="#cb16-377" aria-hidden="true" tabindex="-1"></a>map. A related problem is the very different distribution of pairwise</span>
<span id="cb16-378"><a href="#cb16-378" aria-hidden="true" tabindex="-1"></a>distances in the two spaces. The volume of a sphere centered on datapoint $i$</span>
<span id="cb16-379"><a href="#cb16-379" aria-hidden="true" tabindex="-1"></a>scales as $r^m$, where $r$ is the radius and $m$ the dimensionality of the</span>
<span id="cb16-380"><a href="#cb16-380" aria-hidden="true" tabindex="-1"></a>sphere. So if the datapoints are approximately uniformly distributed in the</span>
<span id="cb16-381"><a href="#cb16-381" aria-hidden="true" tabindex="-1"></a>region around $i$ on the ten-dimensional manifold, we get the following</span>
<span id="cb16-382"><a href="#cb16-382" aria-hidden="true" tabindex="-1"></a>"crowding problem:" the area of the two-dimensional map that is available to</span>
<span id="cb16-383"><a href="#cb16-383" aria-hidden="true" tabindex="-1"></a>accommodate moderately distant datapoints will not be nearly large enough</span>
<span id="cb16-384"><a href="#cb16-384" aria-hidden="true" tabindex="-1"></a>compared with the area available to accommodate nearby datapoints. Hence, if we</span>
<span id="cb16-385"><a href="#cb16-385" aria-hidden="true" tabindex="-1"></a>want to model the small distances accurately in the map, most of the points</span>
<span id="cb16-386"><a href="#cb16-386" aria-hidden="true" tabindex="-1"></a>that are at a moderate distance from datapoint $i$ will have to be placed much</span>
<span id="cb16-387"><a href="#cb16-387" aria-hidden="true" tabindex="-1"></a>too far away in the two-dimensional map. In SNE, the spring connecting</span>
<span id="cb16-388"><a href="#cb16-388" aria-hidden="true" tabindex="-1"></a>datapoint $i$ to each of these too-distant map points will thus exert a very</span>
<span id="cb16-389"><a href="#cb16-389" aria-hidden="true" tabindex="-1"></a>small attractive force. Although these attractive forces are very small, the</span>
<span id="cb16-390"><a href="#cb16-390" aria-hidden="true" tabindex="-1"></a>very large number of such forces crushes together the points in the center of</span>
<span id="cb16-391"><a href="#cb16-391" aria-hidden="true" tabindex="-1"></a>the map, which prevents gaps from forming between the natural clusters. Note</span>
<span id="cb16-392"><a href="#cb16-392" aria-hidden="true" tabindex="-1"></a>that the crowding problem is not specific to SNE, but that it occurs in other</span>
<span id="cb16-393"><a href="#cb16-393" aria-hidden="true" tabindex="-1"></a>local techniques for multidimensional scaling such as Sammon mapping.</span>
<span id="cb16-394"><a href="#cb16-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-395"><a href="#cb16-395" aria-hidden="true" tabindex="-1"></a>An attempt to address the crowding problem by adding a slight repulsion to all</span>
<span id="cb16-396"><a href="#cb16-396" aria-hidden="true" tabindex="-1"></a>springs was presented by @cook:visualizing. The slight repulsion is</span>
<span id="cb16-397"><a href="#cb16-397" aria-hidden="true" tabindex="-1"></a>created by introducing a uniform background model with a small mixing</span>
<span id="cb16-398"><a href="#cb16-398" aria-hidden="true" tabindex="-1"></a>proportion, $\rho$. So however far apart two map points are, $q_{ij}$ can</span>
<span id="cb16-399"><a href="#cb16-399" aria-hidden="true" tabindex="-1"></a>never fall below $\frac{2\rho}{n(n-1)}$ (because the uniform background</span>
<span id="cb16-400"><a href="#cb16-400" aria-hidden="true" tabindex="-1"></a>distribution is over $n(n-1)/2$ pairs). As a result, for datapoints that are</span>
<span id="cb16-401"><a href="#cb16-401" aria-hidden="true" tabindex="-1"></a>far apart in the high-dimensional space, $q_{ij}$ will always be larger than</span>
<span id="cb16-402"><a href="#cb16-402" aria-hidden="true" tabindex="-1"></a>$p_{ij}$, leading to a slight repulsion. This technique is called UNI-SNE and</span>
<span id="cb16-403"><a href="#cb16-403" aria-hidden="true" tabindex="-1"></a>although it usually outperforms standard SNE, the optimization of the UNI-SNE</span>
<span id="cb16-404"><a href="#cb16-404" aria-hidden="true" tabindex="-1"></a>cost function is tedious. The best optimization method known is to start by</span>
<span id="cb16-405"><a href="#cb16-405" aria-hidden="true" tabindex="-1"></a>setting the background mixing proportion to zero (i.e., by performing standard</span>
<span id="cb16-406"><a href="#cb16-406" aria-hidden="true" tabindex="-1"></a>SNE). Once the SNE cost function has been optimized using simulated annealing,</span>
<span id="cb16-407"><a href="#cb16-407" aria-hidden="true" tabindex="-1"></a>the background mixing proportion can be increased to allow some gaps to form</span>
<span id="cb16-408"><a href="#cb16-408" aria-hidden="true" tabindex="-1"></a>between natural clusters as shown by @cook:visualizing. Optimizing the</span>
<span id="cb16-409"><a href="#cb16-409" aria-hidden="true" tabindex="-1"></a>UNI-SNE cost function directly does not work because two map points that are</span>
<span id="cb16-410"><a href="#cb16-410" aria-hidden="true" tabindex="-1"></a>far apart will get almost all of their $q_{i}$ from the uniform background. So</span>
<span id="cb16-411"><a href="#cb16-411" aria-hidden="true" tabindex="-1"></a>even if their $p_{ij}$ is large, there will be no attractive force between</span>
<span id="cb16-412"><a href="#cb16-412" aria-hidden="true" tabindex="-1"></a>them, because a small change in their separation will have a vanishingly small</span>
<span id="cb16-413"><a href="#cb16-413" aria-hidden="true" tabindex="-1"></a>*proportional* effect on $q_{ij}$. This means that if two parts of a cluster</span>
<span id="cb16-414"><a href="#cb16-414" aria-hidden="true" tabindex="-1"></a>get separated early on in the optimization, there is no force to pull them</span>
<span id="cb16-415"><a href="#cb16-415" aria-hidden="true" tabindex="-1"></a>back together.</span>
<span id="cb16-416"><a href="#cb16-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-417"><a href="#cb16-417" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mismatched tails can compensate for mismatched dimensionalities {#sec-heavy_tail}</span></span>
<span id="cb16-418"><a href="#cb16-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-419"><a href="#cb16-419" aria-hidden="true" tabindex="-1"></a>Since symmetric SNE is actually matching the joint probabilities of pairs of</span>
<span id="cb16-420"><a href="#cb16-420" aria-hidden="true" tabindex="-1"></a>datapoints in the high-dimensional and the low-dimensional spaces rather than</span>
<span id="cb16-421"><a href="#cb16-421" aria-hidden="true" tabindex="-1"></a>their distances, we have a natural way of alleviating the crowing problem that</span>
<span id="cb16-422"><a href="#cb16-422" aria-hidden="true" tabindex="-1"></a>works as follows. In the high-dimensional space, we convert distances into</span>
<span id="cb16-423"><a href="#cb16-423" aria-hidden="true" tabindex="-1"></a>probabilities using a Gaussian distribution. In the low-dimensional map, we</span>
<span id="cb16-424"><a href="#cb16-424" aria-hidden="true" tabindex="-1"></a>can use a probability distribution that has a much heavier tails than a</span>
<span id="cb16-425"><a href="#cb16-425" aria-hidden="true" tabindex="-1"></a>Gaussian to convert distances into probabilities. This allows a moderate</span>
<span id="cb16-426"><a href="#cb16-426" aria-hidden="true" tabindex="-1"></a>distance in the high-dimensional space to be faithfully modeled by a much</span>
<span id="cb16-427"><a href="#cb16-427" aria-hidden="true" tabindex="-1"></a>larger distance in the map and, as a result, it eliminates the unwanted</span>
<span id="cb16-428"><a href="#cb16-428" aria-hidden="true" tabindex="-1"></a>attractive forces between map points that represent moderately dissimilar</span>
<span id="cb16-429"><a href="#cb16-429" aria-hidden="true" tabindex="-1"></a>datapoints.</span>
<span id="cb16-430"><a href="#cb16-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-431"><a href="#cb16-431" aria-hidden="true" tabindex="-1"></a>In t-SNE, we employ a Student $t$-distribution with a single degree of</span>
<span id="cb16-432"><a href="#cb16-432" aria-hidden="true" tabindex="-1"></a>freedom (which is the same as a Cauchy distribution) as the heavy-tailed</span>
<span id="cb16-433"><a href="#cb16-433" aria-hidden="true" tabindex="-1"></a>distribution in the low-dimensional map.  Using this distribution, the joint</span>
<span id="cb16-434"><a href="#cb16-434" aria-hidden="true" tabindex="-1"></a>probabilities $q_{ij}$ are defined as</span>
<span id="cb16-435"><a href="#cb16-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-436"><a href="#cb16-436" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-437"><a href="#cb16-437" aria-hidden="true" tabindex="-1"></a>q_{ij} = \frac{(1+\|y_i - y_j\|^2)^{-1}}{\sum_{k \neq l}(1+\|y_k -</span>
<span id="cb16-438"><a href="#cb16-438" aria-hidden="true" tabindex="-1"></a>y_t\|^2)^{-1}}</span>
<span id="cb16-439"><a href="#cb16-439" aria-hidden="true" tabindex="-1"></a>$$ {#eq-joint_probabilities}</span>
<span id="cb16-440"><a href="#cb16-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-441"><a href="#cb16-441" aria-hidden="true" tabindex="-1"></a>We use a Student t-distribution with a single degree of freedom, because it</span>
<span id="cb16-442"><a href="#cb16-442" aria-hidden="true" tabindex="-1"></a>has the particularly nice property that $\left(1+\|y_i - y_j\|^2\right)^{-1}$</span>
<span id="cb16-443"><a href="#cb16-443" aria-hidden="true" tabindex="-1"></a>approaches an inverse square law for large pairwise distances $\|y_i - y_j\|$</span>
<span id="cb16-444"><a href="#cb16-444" aria-hidden="true" tabindex="-1"></a>in the low-dimensional map. This makes the map's representation of joint</span>
<span id="cb16-445"><a href="#cb16-445" aria-hidden="true" tabindex="-1"></a>probabilities (almost) invariant to changes in the scale of the map for map</span>
<span id="cb16-446"><a href="#cb16-446" aria-hidden="true" tabindex="-1"></a>points that are far apart. It also means that large clusters of points that</span>
<span id="cb16-447"><a href="#cb16-447" aria-hidden="true" tabindex="-1"></a>are far apart interact in just the same way as individual points, so the</span>
<span id="cb16-448"><a href="#cb16-448" aria-hidden="true" tabindex="-1"></a>optimization operates in the same way at all but the finest scales. A</span>
<span id="cb16-449"><a href="#cb16-449" aria-hidden="true" tabindex="-1"></a>theoretical justification for our selection of the Student $t$-distribution is</span>
<span id="cb16-450"><a href="#cb16-450" aria-hidden="true" tabindex="-1"></a>that it is closely related to the Gaussian distribution, as the Student</span>
<span id="cb16-451"><a href="#cb16-451" aria-hidden="true" tabindex="-1"></a>$t$-distribution is an infinite mixture of Gaussians. A computationally</span>
<span id="cb16-452"><a href="#cb16-452" aria-hidden="true" tabindex="-1"></a>convenient property is that it is much faster to evaluate the density of a</span>
<span id="cb16-453"><a href="#cb16-453" aria-hidden="true" tabindex="-1"></a>point under a Student $t$-distribution than under a Gaussian because it does</span>
<span id="cb16-454"><a href="#cb16-454" aria-hidden="true" tabindex="-1"></a>not involve an exponential, even though the Student $t$-distribution is</span>
<span id="cb16-455"><a href="#cb16-455" aria-hidden="true" tabindex="-1"></a>equivalent to an infinite mixture of Gaussians with different variances.</span>
<span id="cb16-456"><a href="#cb16-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-457"><a href="#cb16-457" aria-hidden="true" tabindex="-1"></a>The gradient of the Kullback-Leibler divergence between $P$ and the Student-$t$</span>
<span id="cb16-458"><a href="#cb16-458" aria-hidden="true" tabindex="-1"></a>based joint probability distribution $Q$ (computed using @eq-joint_probabilities)</span>
<span id="cb16-459"><a href="#cb16-459" aria-hidden="true" tabindex="-1"></a>is derived in Appendix A, and is given by</span>
<span id="cb16-460"><a href="#cb16-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-461"><a href="#cb16-461" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-462"><a href="#cb16-462" aria-hidden="true" tabindex="-1"></a>\frac{\partial C}{\partial y_i} = 4 \sum_j (p_{ij} - q_{ij})(y_i -</span>
<span id="cb16-463"><a href="#cb16-463" aria-hidden="true" tabindex="-1"></a>y_j)(1+\|y_i - y_j\|^2)^{-1}\,.</span>
<span id="cb16-464"><a href="#cb16-464" aria-hidden="true" tabindex="-1"></a>$$ {#eq-gradient-tsne}</span>
<span id="cb16-465"><a href="#cb16-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-466"><a href="#cb16-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-467"><a href="#cb16-467" aria-hidden="true" tabindex="-1"></a>![Gradients of three types of SNE as a function of the pairwise Euclidean</span>
<span id="cb16-468"><a href="#cb16-468" aria-hidden="true" tabindex="-1"></a>distance between two points in the high-dimensional and the pairwise distance</span>
<span id="cb16-469"><a href="#cb16-469" aria-hidden="true" tabindex="-1"></a>between the points in the low-dimensional data representation.](figures/gradients.png){#fig-gradients}</span>
<span id="cb16-470"><a href="#cb16-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-471"><a href="#cb16-471" aria-hidden="true" tabindex="-1"></a>In @fig-gradients, we show the gradients between the</span>
<span id="cb16-472"><a href="#cb16-472" aria-hidden="true" tabindex="-1"></a>low-dimensional datapoints $y_i$ and $y_j$ as a function of their</span>
<span id="cb16-473"><a href="#cb16-473" aria-hidden="true" tabindex="-1"></a>pairwise Euclidean distances in the high-dimensional and the</span>
<span id="cb16-474"><a href="#cb16-474" aria-hidden="true" tabindex="-1"></a>low-dimensional space (i.e., as a function of $\|x_i-x_j\|$ and</span>
<span id="cb16-475"><a href="#cb16-475" aria-hidden="true" tabindex="-1"></a>$\|y_i-y_j\|$) for the symmetric versions of SNE, UNI-SNE, and</span>
<span id="cb16-476"><a href="#cb16-476" aria-hidden="true" tabindex="-1"></a>t-SNE. In the figures, positive values of the gradient represent an</span>
<span id="cb16-477"><a href="#cb16-477" aria-hidden="true" tabindex="-1"></a>attraction between the low-dimensional datapoints $y_i$ and $y_j$,</span>
<span id="cb16-478"><a href="#cb16-478" aria-hidden="true" tabindex="-1"></a>whereas negative values represent a repulsion between the two</span>
<span id="cb16-479"><a href="#cb16-479" aria-hidden="true" tabindex="-1"></a>datapoints. From the figures, we observe two main advantages of the</span>
<span id="cb16-480"><a href="#cb16-480" aria-hidden="true" tabindex="-1"></a>t-SNE gradient over the gradients of SNE and UNI-SNE.</span>
<span id="cb16-481"><a href="#cb16-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-482"><a href="#cb16-482" aria-hidden="true" tabindex="-1"></a>First, the t-SNE gradient strongly repels dissimilar datapoints that are</span>
<span id="cb16-483"><a href="#cb16-483" aria-hidden="true" tabindex="-1"></a>modeled by a small pairwise distance in the low-dimensional representation.</span>
<span id="cb16-484"><a href="#cb16-484" aria-hidden="true" tabindex="-1"></a>SNE has such repulsion as well, but its effect is minimal compared to the</span>
<span id="cb16-485"><a href="#cb16-485" aria-hidden="true" tabindex="-1"></a>strong attractions elsewhere in the gradient (the largest attraction in our</span>
<span id="cb16-486"><a href="#cb16-486" aria-hidden="true" tabindex="-1"></a>graphical representation of the gradient is approximately 19, whereas the</span>
<span id="cb16-487"><a href="#cb16-487" aria-hidden="true" tabindex="-1"></a>largest repulsion is approximately 1). In UNI-SNE, the amount of repulsion</span>
<span id="cb16-488"><a href="#cb16-488" aria-hidden="true" tabindex="-1"></a>between dissimilar datapoints is slightly larger, however, this repulsion is</span>
<span id="cb16-489"><a href="#cb16-489" aria-hidden="true" tabindex="-1"></a>only strong when the pairwise distance between the points in the</span>
<span id="cb16-490"><a href="#cb16-490" aria-hidden="true" tabindex="-1"></a>low-dimensional representation is already large (which is often not the case,</span>
<span id="cb16-491"><a href="#cb16-491" aria-hidden="true" tabindex="-1"></a>since the low-dimensional representation is initialized by sampling from a</span>
<span id="cb16-492"><a href="#cb16-492" aria-hidden="true" tabindex="-1"></a>Gaussian with a very small variance that is centered around the origin).</span>
<span id="cb16-493"><a href="#cb16-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-494"><a href="#cb16-494" aria-hidden="true" tabindex="-1"></a>Second, although t-SNE introduces strong repulsions between dissimilar</span>
<span id="cb16-495"><a href="#cb16-495" aria-hidden="true" tabindex="-1"></a>datapoints that are modeled by small pairwise distances, these repulsions do</span>
<span id="cb16-496"><a href="#cb16-496" aria-hidden="true" tabindex="-1"></a>not go to infinity. In this respect, t-SNE differs from UNI-SNE, in which the</span>
<span id="cb16-497"><a href="#cb16-497" aria-hidden="true" tabindex="-1"></a>strength of the repulsion between very dissimilar datapoints is proportional</span>
<span id="cb16-498"><a href="#cb16-498" aria-hidden="true" tabindex="-1"></a>to their pairwise distance in the low-dimensional map, which may cause</span>
<span id="cb16-499"><a href="#cb16-499" aria-hidden="true" tabindex="-1"></a>dissimilar datapoints to move much too far away from each other.</span>
<span id="cb16-500"><a href="#cb16-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-501"><a href="#cb16-501" aria-hidden="true" tabindex="-1"></a>Taken together, t-SNE puts emphasis on (1) modeling dissimilar</span>
<span id="cb16-502"><a href="#cb16-502" aria-hidden="true" tabindex="-1"></a>datapoints by means of large pairwise distances, and (2) modeling</span>
<span id="cb16-503"><a href="#cb16-503" aria-hidden="true" tabindex="-1"></a>similar datapoints by means of small pairwise distances. Moreover, as</span>
<span id="cb16-504"><a href="#cb16-504" aria-hidden="true" tabindex="-1"></a>a result of these characteristics of the t-SNE cost function (and as a</span>
<span id="cb16-505"><a href="#cb16-505" aria-hidden="true" tabindex="-1"></a>result of the approximate scale invariance of the Student</span>
<span id="cb16-506"><a href="#cb16-506" aria-hidden="true" tabindex="-1"></a>t-distribution), the optimization of the t-SNE cost function is much</span>
<span id="cb16-507"><a href="#cb16-507" aria-hidden="true" tabindex="-1"></a>easier than the optimization of the cost functions of SNE and</span>
<span id="cb16-508"><a href="#cb16-508" aria-hidden="true" tabindex="-1"></a>UNI-SNE. Specifically, t-SNE introduces long-range forces in the</span>
<span id="cb16-509"><a href="#cb16-509" aria-hidden="true" tabindex="-1"></a>low-dimensional map that can pull back together two (clusters of)</span>
<span id="cb16-510"><a href="#cb16-510" aria-hidden="true" tabindex="-1"></a>similar points that get separated early on in the optimization. SNE</span>
<span id="cb16-511"><a href="#cb16-511" aria-hidden="true" tabindex="-1"></a>and UNI-SNE do not have such long-range forces, as a result of which</span>
<span id="cb16-512"><a href="#cb16-512" aria-hidden="true" tabindex="-1"></a>SNE and UNI-SNE need to use simulated annealing to obtain reasonable</span>
<span id="cb16-513"><a href="#cb16-513" aria-hidden="true" tabindex="-1"></a>solutions. Instead, the long-range forces in t-SNE facilitate the</span>
<span id="cb16-514"><a href="#cb16-514" aria-hidden="true" tabindex="-1"></a>identification of good local optima without resorting to simulated</span>
<span id="cb16-515"><a href="#cb16-515" aria-hidden="true" tabindex="-1"></a>annealing</span>
<span id="cb16-516"><a href="#cb16-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-517"><a href="#cb16-517" aria-hidden="true" tabindex="-1"></a><span class="in">```{.pseudocode}</span></span>
<span id="cb16-518"><a href="#cb16-518" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithm}</span></span>
<span id="cb16-519"><a href="#cb16-519" aria-hidden="true" tabindex="-1"></a><span class="in">\caption{Simple version of t-Distributed Stochastic Neighbor Embeding}</span></span>
<span id="cb16-520"><a href="#cb16-520" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithmic}</span></span>
<span id="cb16-521"><a href="#cb16-521" aria-hidden="true" tabindex="-1"></a><span class="in">\State \textbf{Data}: high-dimensional representation $\mathcal{X} = \{x_1, \dots, x_n\}$</span></span>
<span id="cb16-522"><a href="#cb16-522" aria-hidden="true" tabindex="-1"></a><span class="in">\State cost function parameters: perplexity  $Perp$</span></span>
<span id="cb16-523"><a href="#cb16-523" aria-hidden="true" tabindex="-1"></a><span class="in">\State optimization parameters: number of iterations $T$, learning rate $\eta$, momentum $\alpha(t)$</span></span>
<span id="cb16-524"><a href="#cb16-524" aria-hidden="true" tabindex="-1"></a><span class="in">\State \textbf{Result}: low-dimensional data representation $\mathcal{Y}^{(T)} = \{y_1, \dots, y_n\}$</span></span>
<span id="cb16-525"><a href="#cb16-525" aria-hidden="true" tabindex="-1"></a><span class="in">\Procedure{t-sne}{$Perp, T, \eta, \alpha(t)$}</span></span>
<span id="cb16-526"><a href="#cb16-526" aria-hidden="true" tabindex="-1"></a><span class="in">    \State compute pairwise affinities $p_{j|i}$ with perplexity $Perp$ (using Equation 1)</span></span>
<span id="cb16-527"><a href="#cb16-527" aria-hidden="true" tabindex="-1"></a><span class="in">    \State set $p_{ij} = \frac{1}{2n} (p_{j|i} + p_{i|j})$</span></span>
<span id="cb16-528"><a href="#cb16-528" aria-hidden="true" tabindex="-1"></a><span class="in">    \State sample initial solution $\mathcal{Y}^{(0)} = \{y_1,\dots,y_n\}$ from $\mathcal{N}(0,1e^{-4} I)$</span></span>
<span id="cb16-529"><a href="#cb16-529" aria-hidden="true" tabindex="-1"></a><span class="in">    \For{$t = 0$\dots$T$}</span></span>
<span id="cb16-530"><a href="#cb16-530" aria-hidden="true" tabindex="-1"></a><span class="in">        \State compute low-dimensional affinities $q_{ij}$ (using Equation 4)</span></span>
<span id="cb16-531"><a href="#cb16-531" aria-hidden="true" tabindex="-1"></a><span class="in">        \State compute gradient $\frac{\partial C}{\partial \mathcal{Y}}$ (using Equation 5)</span></span>
<span id="cb16-532"><a href="#cb16-532" aria-hidden="true" tabindex="-1"></a><span class="in">        \State et $\mathcal{Y}^{t} = \mathcal{Y}^{t-1} + \eta \frac{\partial C}{\partial \mathcal{Y}} + \alpha(t) \left(\mathcal{Y}^{t-1} - \mathcal{Y}^{t-2}\right)$ </span></span>
<span id="cb16-533"><a href="#cb16-533" aria-hidden="true" tabindex="-1"></a><span class="in">    \EndFor</span></span>
<span id="cb16-534"><a href="#cb16-534" aria-hidden="true" tabindex="-1"></a><span class="in">\EndProcedure</span></span>
<span id="cb16-535"><a href="#cb16-535" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithmic}</span></span>
<span id="cb16-536"><a href="#cb16-536" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithm}</span></span>
<span id="cb16-537"><a href="#cb16-537" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-538"><a href="#cb16-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-539"><a href="#cb16-539" aria-hidden="true" tabindex="-1"></a><span class="fu">## Optimization methods for t-SNE {#sec-optimization_methods_for_tsne}</span></span>
<span id="cb16-540"><a href="#cb16-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-541"><a href="#cb16-541" aria-hidden="true" tabindex="-1"></a>We start by presenting a relatively simple, gradient descent procedure for</span>
<span id="cb16-542"><a href="#cb16-542" aria-hidden="true" tabindex="-1"></a>optimizing the t-SNE cost function. This simple procedure uses a momentum term</span>
<span id="cb16-543"><a href="#cb16-543" aria-hidden="true" tabindex="-1"></a>to reduce the number of iterations required and it works best if the momentum</span>
<span id="cb16-544"><a href="#cb16-544" aria-hidden="true" tabindex="-1"></a>term is small until the map points have become moderately well organized.</span>
<span id="cb16-545"><a href="#cb16-545" aria-hidden="true" tabindex="-1"></a>Pseudocode for this simple algorithm is presented in Algorithm 1 (FIXME: ref not working). The</span>
<span id="cb16-546"><a href="#cb16-546" aria-hidden="true" tabindex="-1"></a>simple algorithm can be sped up using the adaptive learning rate scheme that</span>
<span id="cb16-547"><a href="#cb16-547" aria-hidden="true" tabindex="-1"></a>is described by @jacobs:rates, which gradually increases the learning</span>
<span id="cb16-548"><a href="#cb16-548" aria-hidden="true" tabindex="-1"></a>rate in directions in which the gradient is stable.</span>
<span id="cb16-549"><a href="#cb16-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-550"><a href="#cb16-550" aria-hidden="true" tabindex="-1"></a>Although the simple algorithm produces visualizations that are often much</span>
<span id="cb16-551"><a href="#cb16-551" aria-hidden="true" tabindex="-1"></a>better than those produced by other non-parametric dimensionality reduction</span>
<span id="cb16-552"><a href="#cb16-552" aria-hidden="true" tabindex="-1"></a>techniques, the results can be improved further by using either of two tricks.</span>
<span id="cb16-553"><a href="#cb16-553" aria-hidden="true" tabindex="-1"></a>The first trick, which we call "early compression," is to force the map points</span>
<span id="cb16-554"><a href="#cb16-554" aria-hidden="true" tabindex="-1"></a>to stay close together at the start of the optimization. When the distances</span>
<span id="cb16-555"><a href="#cb16-555" aria-hidden="true" tabindex="-1"></a>between map points are small, it is easy for clusters to move through one</span>
<span id="cb16-556"><a href="#cb16-556" aria-hidden="true" tabindex="-1"></a>another so it is much easier to explore the space of possible global</span>
<span id="cb16-557"><a href="#cb16-557" aria-hidden="true" tabindex="-1"></a>organizations of the data. Early compression is implemented by adding an</span>
<span id="cb16-558"><a href="#cb16-558" aria-hidden="true" tabindex="-1"></a>additional L2-penalty to the cost function that is proportional to the sum of</span>
<span id="cb16-559"><a href="#cb16-559" aria-hidden="true" tabindex="-1"></a>squared distances of the map points from the origin. The magnitude of this</span>
<span id="cb16-560"><a href="#cb16-560" aria-hidden="true" tabindex="-1"></a>penalty term and the iteration at which it is removed are set by hand, but the</span>
<span id="cb16-561"><a href="#cb16-561" aria-hidden="true" tabindex="-1"></a>behavior is fairly robust across variations in these two additional</span>
<span id="cb16-562"><a href="#cb16-562" aria-hidden="true" tabindex="-1"></a>optimization parameters.</span>
<span id="cb16-563"><a href="#cb16-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-564"><a href="#cb16-564" aria-hidden="true" tabindex="-1"></a>A less obvious way to improve the optimization, which we call "early</span>
<span id="cb16-565"><a href="#cb16-565" aria-hidden="true" tabindex="-1"></a>exaggeration," is to multiply all of the $p_{ij}$'s by, for example, 4, in the</span>
<span id="cb16-566"><a href="#cb16-566" aria-hidden="true" tabindex="-1"></a>initial stages of the optimization. This means that almost all of the</span>
<span id="cb16-567"><a href="#cb16-567" aria-hidden="true" tabindex="-1"></a>$q_{ij}$'s, which still add up to 1, are much too small to model their</span>
<span id="cb16-568"><a href="#cb16-568" aria-hidden="true" tabindex="-1"></a>corresponding $p_{ij}$'s. As a result, the optimization is encouraged to focus</span>
<span id="cb16-569"><a href="#cb16-569" aria-hidden="true" tabindex="-1"></a>on modeling the large $p_{ij}$'s by fairly large $q {ij}$'s. The effect is</span>
<span id="cb16-570"><a href="#cb16-570" aria-hidden="true" tabindex="-1"></a>that the natural clusters in the data tend to form tight widely separated</span>
<span id="cb16-571"><a href="#cb16-571" aria-hidden="true" tabindex="-1"></a>clusters in the map. This creates a lot of relatively empty space in the map,</span>
<span id="cb16-572"><a href="#cb16-572" aria-hidden="true" tabindex="-1"></a>which makes it much easier for the clusters to move around relative to one</span>
<span id="cb16-573"><a href="#cb16-573" aria-hidden="true" tabindex="-1"></a>another in order to find a good global organization.</span>
<span id="cb16-574"><a href="#cb16-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-575"><a href="#cb16-575" aria-hidden="true" tabindex="-1"></a>In all the visualizations presented in this paper and in the</span>
<span id="cb16-576"><a href="#cb16-576" aria-hidden="true" tabindex="-1"></a>supporting material, we used exactly the same optimization</span>
<span id="cb16-577"><a href="#cb16-577" aria-hidden="true" tabindex="-1"></a>procedure. We used the early exaggeration method with an exaggeration</span>
<span id="cb16-578"><a href="#cb16-578" aria-hidden="true" tabindex="-1"></a>of 4 for the first 50 iterations (note that early exaggeration is not</span>
<span id="cb16-579"><a href="#cb16-579" aria-hidden="true" tabindex="-1"></a>included in the pseudocode in Algorithm 1). The number of</span>
<span id="cb16-580"><a href="#cb16-580" aria-hidden="true" tabindex="-1"></a>gradient descent iterations $T$ was set 1000, and the momentum term</span>
<span id="cb16-581"><a href="#cb16-581" aria-hidden="true" tabindex="-1"></a>was set to $\alpha^{(t)} = 0.5$ for $t&lt;250$ and $\alpha^{(t)}=0.8$ for</span>
<span id="cb16-582"><a href="#cb16-582" aria-hidden="true" tabindex="-1"></a>$t \geq 250$. The learning rate $\eta$ is initially set to 100 and it</span>
<span id="cb16-583"><a href="#cb16-583" aria-hidden="true" tabindex="-1"></a>is updated after every iteration by means of the adaptive learning</span>
<span id="cb16-584"><a href="#cb16-584" aria-hidden="true" tabindex="-1"></a>rate scheme described by @jacobs:rates. A Matlab implementation of the</span>
<span id="cb16-585"><a href="#cb16-585" aria-hidden="true" tabindex="-1"></a>resulting algorithm is available at</span>
<span id="cb16-586"><a href="#cb16-586" aria-hidden="true" tabindex="-1"></a><span class="ot">&lt;https://lvdmaaten.github.io/tsne/&gt;</span>.</span>
<span id="cb16-587"><a href="#cb16-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-588"><a href="#cb16-588" aria-hidden="true" tabindex="-1"></a><span class="fu"># Experiments {#sec-experiments}</span></span>
<span id="cb16-589"><a href="#cb16-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-590"><a href="#cb16-590" aria-hidden="true" tabindex="-1"></a>To evaluate t-SNE, we present experiments in which t-SNE is compared to seven</span>
<span id="cb16-591"><a href="#cb16-591" aria-hidden="true" tabindex="-1"></a>other non-parametric techniques for dimensionality reduction. Because of space</span>
<span id="cb16-592"><a href="#cb16-592" aria-hidden="true" tabindex="-1"></a>limitations, in the paper, we only compare t-SNE with: (1) Sammon mapping, (2)</span>
<span id="cb16-593"><a href="#cb16-593" aria-hidden="true" tabindex="-1"></a>Isomap, and (3) LLE. In the supporting material, we also compare t-SNE with:</span>
<span id="cb16-594"><a href="#cb16-594" aria-hidden="true" tabindex="-1"></a>(4) CCA, (5) SNE, (6) MVU, and (7) Laplacian Eigenmaps. We performed</span>
<span id="cb16-595"><a href="#cb16-595" aria-hidden="true" tabindex="-1"></a>experiments on five data sets that represent a variety of application domains.</span>
<span id="cb16-596"><a href="#cb16-596" aria-hidden="true" tabindex="-1"></a>Again, because of space limitations, we restrict ourselves to three data sets</span>
<span id="cb16-597"><a href="#cb16-597" aria-hidden="true" tabindex="-1"></a>in the paper. The results of our experiments on the remaining two data sets</span>
<span id="cb16-598"><a href="#cb16-598" aria-hidden="true" tabindex="-1"></a>are presented in the supplementary material.</span>
<span id="cb16-599"><a href="#cb16-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-600"><a href="#cb16-600" aria-hidden="true" tabindex="-1"></a>In @sec-datasets, the data sets that we employed in our</span>
<span id="cb16-601"><a href="#cb16-601" aria-hidden="true" tabindex="-1"></a>experiments are introduced. The setup of the experiments is presented in</span>
<span id="cb16-602"><a href="#cb16-602" aria-hidden="true" tabindex="-1"></a>@sec-experimental_setup. In @sec-results, we</span>
<span id="cb16-603"><a href="#cb16-603" aria-hidden="true" tabindex="-1"></a>present the results of our experiments.</span>
<span id="cb16-604"><a href="#cb16-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-605"><a href="#cb16-605" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data Sets {#sec-datasets}</span></span>
<span id="cb16-606"><a href="#cb16-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-607"><a href="#cb16-607" aria-hidden="true" tabindex="-1"></a>The five data sets we employed in our experiments are: (1) the MNIST data set,</span>
<span id="cb16-608"><a href="#cb16-608" aria-hidden="true" tabindex="-1"></a>(2) the Olivetti faces data set, (3) the COIL-20 data set, (4) the</span>
<span id="cb16-609"><a href="#cb16-609" aria-hidden="true" tabindex="-1"></a>word-features data set, and (5) the Netflix data set. We only present results</span>
<span id="cb16-610"><a href="#cb16-610" aria-hidden="true" tabindex="-1"></a>on the first three data sets in this section. The results on the remaining two</span>
<span id="cb16-611"><a href="#cb16-611" aria-hidden="true" tabindex="-1"></a>data sets are presented in the supporting material. The first three data sets</span>
<span id="cb16-612"><a href="#cb16-612" aria-hidden="true" tabindex="-1"></a>are introduced below.</span>
<span id="cb16-613"><a href="#cb16-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-614"><a href="#cb16-614" aria-hidden="true" tabindex="-1"></a>The MNIST data set<span class="ot">[^MNIST]</span> contains 60,000 grayscale images of</span>
<span id="cb16-615"><a href="#cb16-615" aria-hidden="true" tabindex="-1"></a>handwritten digits. For our experiments, we randomly selected 6,000 of</span>
<span id="cb16-616"><a href="#cb16-616" aria-hidden="true" tabindex="-1"></a>the images for computational reasons. The digit images have $28 \times</span>
<span id="cb16-617"><a href="#cb16-617" aria-hidden="true" tabindex="-1"></a>28 = 784$ pixels (i.e., dimensions). The Olivetti faces data</span>
<span id="cb16-618"><a href="#cb16-618" aria-hidden="true" tabindex="-1"></a>set<span class="ot">[^Olivetti]</span> consists of images of 40 individuals with small</span>
<span id="cb16-619"><a href="#cb16-619" aria-hidden="true" tabindex="-1"></a>variations in viewpoint, large variations in expression, and</span>
<span id="cb16-620"><a href="#cb16-620" aria-hidden="true" tabindex="-1"></a>occasional addition of glasses. The data set consists of 400 images</span>
<span id="cb16-621"><a href="#cb16-621" aria-hidden="true" tabindex="-1"></a>(10 per individual) of size $92\times 112=10,304$ pixels, and is</span>
<span id="cb16-622"><a href="#cb16-622" aria-hidden="true" tabindex="-1"></a>labeled according to identity. The COIL-20 data set</span>
<span id="cb16-623"><a href="#cb16-623" aria-hidden="true" tabindex="-1"></a>@nene:coil20 contains images of 20 different objects viewed</span>
<span id="cb16-624"><a href="#cb16-624" aria-hidden="true" tabindex="-1"></a>from 72 equally spaced orientations, yielding a total of 1,440</span>
<span id="cb16-625"><a href="#cb16-625" aria-hidden="true" tabindex="-1"></a>images. The images contain $32 \times 32 = 1,024$ pixels.</span>
<span id="cb16-626"><a href="#cb16-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-627"><a href="#cb16-627" aria-hidden="true" tabindex="-1"></a><span class="ot">[^MNIST]: </span>The MNIST data set is publicly available from <span class="ot">&lt;http://yann.lecun.com/exdb/mnist/index.html&gt;</span>.</span>
<span id="cb16-628"><a href="#cb16-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-629"><a href="#cb16-629" aria-hidden="true" tabindex="-1"></a><span class="ot">[^Olivetti]: </span>The Olivetti data set is publicly available from <span class="ot">&lt;http://mambo.ucsc.edu/psl/olivetti.html&gt;</span>.</span>
<span id="cb16-630"><a href="#cb16-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-631"><a href="#cb16-631" aria-hidden="true" tabindex="-1"></a><span class="fu">## Experimental Setup {#sec-experimental_setup}</span></span>
<span id="cb16-632"><a href="#cb16-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-633"><a href="#cb16-633" aria-hidden="true" tabindex="-1"></a>In all of our experiments, we start by using PCA to reduce the dimensionality</span>
<span id="cb16-634"><a href="#cb16-634" aria-hidden="true" tabindex="-1"></a>of the data to 30. This speeds up the computation of pairwise distances</span>
<span id="cb16-635"><a href="#cb16-635" aria-hidden="true" tabindex="-1"></a>between the datapoints and suppresses some noise without severely distorting</span>
<span id="cb16-636"><a href="#cb16-636" aria-hidden="true" tabindex="-1"></a>the interpoint distances. We then use each of the dimensionality reduction</span>
<span id="cb16-637"><a href="#cb16-637" aria-hidden="true" tabindex="-1"></a>techniques to convert the 30-dimensional representation to a two-dimensional</span>
<span id="cb16-638"><a href="#cb16-638" aria-hidden="true" tabindex="-1"></a>map and we show the resulting map as a scatterplot. For all of the data sets,</span>
<span id="cb16-639"><a href="#cb16-639" aria-hidden="true" tabindex="-1"></a>there is information about the class of each datapoint, but the class</span>
<span id="cb16-640"><a href="#cb16-640" aria-hidden="true" tabindex="-1"></a>information is only used to select a color and/or symbol for the map points.</span>
<span id="cb16-641"><a href="#cb16-641" aria-hidden="true" tabindex="-1"></a>The class information is not used to determine the spatial coordinates of the</span>
<span id="cb16-642"><a href="#cb16-642" aria-hidden="true" tabindex="-1"></a>map points. The coloring thus provides a way of evaluating how well the map</span>
<span id="cb16-643"><a href="#cb16-643" aria-hidden="true" tabindex="-1"></a>preserves the similarities within each class.</span>
<span id="cb16-644"><a href="#cb16-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-645"><a href="#cb16-645" aria-hidden="true" tabindex="-1"></a>The cost function parameter settings we employed in our experiments are listed</span>
<span id="cb16-646"><a href="#cb16-646" aria-hidden="true" tabindex="-1"></a>in @tbl-cost-function-parameters. In the table, $Perp$ represents the</span>
<span id="cb16-647"><a href="#cb16-647" aria-hidden="true" tabindex="-1"></a>perplexity of the conditional probability distribution induced by a Gaussian</span>
<span id="cb16-648"><a href="#cb16-648" aria-hidden="true" tabindex="-1"></a>kernel and $k$ represents the number of nearest neighbors employed in a</span>
<span id="cb16-649"><a href="#cb16-649" aria-hidden="true" tabindex="-1"></a>neighborhood graph. In the experiments with Isomap and LLE, we only visualize</span>
<span id="cb16-650"><a href="#cb16-650" aria-hidden="true" tabindex="-1"></a>datapoints that correspond to vertices in the largest connected component of</span>
<span id="cb16-651"><a href="#cb16-651" aria-hidden="true" tabindex="-1"></a>the neighborhood graph. <span class="ot">[^neighborhood_graph]</span> For the Sammon mapping</span>
<span id="cb16-652"><a href="#cb16-652" aria-hidden="true" tabindex="-1"></a>optimization, we performed Newton's method for 500 iterations.</span>
<span id="cb16-653"><a href="#cb16-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-654"><a href="#cb16-654" aria-hidden="true" tabindex="-1"></a><span class="ot">[^neighborhood_graph]: </span>Isomap and LLE require data that gives rise to a</span>
<span id="cb16-655"><a href="#cb16-655" aria-hidden="true" tabindex="-1"></a>  neighborhood graph that is connected.</span>
<span id="cb16-656"><a href="#cb16-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-657"><a href="#cb16-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-658"><a href="#cb16-658" aria-hidden="true" tabindex="-1"></a>| Technique      | Cost function parameters |</span>
<span id="cb16-659"><a href="#cb16-659" aria-hidden="true" tabindex="-1"></a>|----------------|-------------------------:|</span>
<span id="cb16-660"><a href="#cb16-660" aria-hidden="true" tabindex="-1"></a>| t-SNE          | $Perp = 40$              |</span>
<span id="cb16-661"><a href="#cb16-661" aria-hidden="true" tabindex="-1"></a>| Sammon mapping | none                     |</span>
<span id="cb16-662"><a href="#cb16-662" aria-hidden="true" tabindex="-1"></a>| Isomap         | $k=12$                   |</span>
<span id="cb16-663"><a href="#cb16-663" aria-hidden="true" tabindex="-1"></a>| LLE            | $k=12$                   |</span>
<span id="cb16-664"><a href="#cb16-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-665"><a href="#cb16-665" aria-hidden="true" tabindex="-1"></a>: Cost function parameter settings for the experiments {#tbl-cost-function-parameters}</span>
<span id="cb16-666"><a href="#cb16-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-667"><a href="#cb16-667" aria-hidden="true" tabindex="-1"></a><span class="fu">## Results {#sec-results}</span></span>
<span id="cb16-668"><a href="#cb16-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-671"><a href="#cb16-671" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-672"><a href="#cb16-672" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-tsne-sammon</span></span>
<span id="cb16-673"><a href="#cb16-673" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Visualization by t-SNE and Sammon mapping"</span></span>
<span id="cb16-674"><a href="#cb16-674" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb16-675"><a href="#cb16-675" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb16-676"><a href="#cb16-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-677"><a href="#cb16-677" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb16-678"><a href="#cb16-678" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 2. Visualization by t-SNE and Sammon mapping</span></span>
<span id="cb16-679"><a href="#cb16-679" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb16-680"><a href="#cb16-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-681"><a href="#cb16-681" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_digits</span>
<span id="cb16-682"><a href="#cb16-682" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-683"><a href="#cb16-683" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> manifold</span>
<span id="cb16-684"><a href="#cb16-684" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> joblib</span>
<span id="cb16-685"><a href="#cb16-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-686"><a href="#cb16-686" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb16-687"><a href="#cb16-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-688"><a href="#cb16-688" aria-hidden="true" tabindex="-1"></a>mem <span class="op">=</span> joblib.Memory(<span class="st">".joblib"</span>)</span>
<span id="cb16-689"><a href="#cb16-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-690"><a href="#cb16-690" aria-hidden="true" tabindex="-1"></a>digits <span class="op">=</span> load_digits()</span>
<span id="cb16-691"><a href="#cb16-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-692"><a href="#cb16-692" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> digits[<span class="st">"data"</span>]</span>
<span id="cb16-693"><a href="#cb16-693" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> digits[<span class="st">"target"</span>]</span>
<span id="cb16-694"><a href="#cb16-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-695"><a href="#cb16-695" aria-hidden="true" tabindex="-1"></a>n_components <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb16-696"><a href="#cb16-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-697"><a href="#cb16-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-698"><a href="#cb16-698" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> {</span>
<span id="cb16-699"><a href="#cb16-699" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>: <span class="st">"C0"</span>,</span>
<span id="cb16-700"><a href="#cb16-700" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span>: <span class="st">"C1"</span>,</span>
<span id="cb16-701"><a href="#cb16-701" aria-hidden="true" tabindex="-1"></a>    <span class="dv">2</span>: <span class="st">"C2"</span>,</span>
<span id="cb16-702"><a href="#cb16-702" aria-hidden="true" tabindex="-1"></a>    <span class="dv">3</span>: <span class="st">"C3"</span>,</span>
<span id="cb16-703"><a href="#cb16-703" aria-hidden="true" tabindex="-1"></a>    <span class="dv">4</span>: <span class="st">"C4"</span>,</span>
<span id="cb16-704"><a href="#cb16-704" aria-hidden="true" tabindex="-1"></a>    <span class="dv">5</span>: <span class="st">"C5"</span>,</span>
<span id="cb16-705"><a href="#cb16-705" aria-hidden="true" tabindex="-1"></a>    <span class="dv">6</span>: <span class="st">"C6"</span>,</span>
<span id="cb16-706"><a href="#cb16-706" aria-hidden="true" tabindex="-1"></a>    <span class="dv">7</span>: <span class="st">"C7"</span>,</span>
<span id="cb16-707"><a href="#cb16-707" aria-hidden="true" tabindex="-1"></a>    <span class="dv">8</span>: <span class="st">"C8"</span>,</span>
<span id="cb16-708"><a href="#cb16-708" aria-hidden="true" tabindex="-1"></a>    <span class="dv">9</span>: <span class="st">"C9"</span>,</span>
<span id="cb16-709"><a href="#cb16-709" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-710"><a href="#cb16-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-711"><a href="#cb16-711" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">12</span>))</span>
<span id="cb16-712"><a href="#cb16-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-713"><a href="#cb16-713" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb16-714"><a href="#cb16-714" aria-hidden="true" tabindex="-1"></a><span class="co"># t-SNE manifold learning</span></span>
<span id="cb16-715"><a href="#cb16-715" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb16-716"><a href="#cb16-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-717"><a href="#cb16-717" aria-hidden="true" tabindex="-1"></a>tsne <span class="op">=</span> manifold.TSNE(n_components<span class="op">=</span>n_components, init<span class="op">=</span><span class="st">"pca"</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb16-718"><a href="#cb16-718" aria-hidden="true" tabindex="-1"></a>X_transformed <span class="op">=</span> mem.cache(tsne.fit_transform)(X)</span>
<span id="cb16-719"><a href="#cb16-719" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label <span class="kw">in</span> np.unique(y):</span>
<span id="cb16-720"><a href="#cb16-720" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> y <span class="op">==</span> label</span>
<span id="cb16-721"><a href="#cb16-721" aria-hidden="true" tabindex="-1"></a>    ax.scatter(</span>
<span id="cb16-722"><a href="#cb16-722" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">0</span>],</span>
<span id="cb16-723"><a href="#cb16-723" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">1</span>],</span>
<span id="cb16-724"><a href="#cb16-724" aria-hidden="true" tabindex="-1"></a>        c<span class="op">=</span>colors[label], marker<span class="op">=</span><span class="st">"."</span>,</span>
<span id="cb16-725"><a href="#cb16-725" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span>label)</span>
<span id="cb16-726"><a href="#cb16-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-727"><a href="#cb16-727" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"left"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-728"><a href="#cb16-728" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"right"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-729"><a href="#cb16-729" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"top"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-730"><a href="#cb16-730" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"bottom"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-731"><a href="#cb16-731" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([])</span>
<span id="cb16-732"><a href="#cb16-732" aria-hidden="true" tabindex="-1"></a>ax.set_yticks([])</span>
<span id="cb16-733"><a href="#cb16-733" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"t-SNE"</span>)</span>
<span id="cb16-734"><a href="#cb16-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-735"><a href="#cb16-735" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb16-736"><a href="#cb16-736" aria-hidden="true" tabindex="-1"></a><span class="co"># Sammon mapping</span></span>
<span id="cb16-737"><a href="#cb16-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-738"><a href="#cb16-738" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">FIXME</span><span class="co"> I think Sammon mapping has weights?</span></span>
<span id="cb16-739"><a href="#cb16-739" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb16-740"><a href="#cb16-740" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> manifold.MDS(n_components<span class="op">=</span>n_components)</span>
<span id="cb16-741"><a href="#cb16-741" aria-hidden="true" tabindex="-1"></a>X_transformed <span class="op">=</span> mem.cache(embedding.fit_transform)(X)</span>
<span id="cb16-742"><a href="#cb16-742" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label <span class="kw">in</span> np.unique(y):</span>
<span id="cb16-743"><a href="#cb16-743" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> y <span class="op">==</span> label</span>
<span id="cb16-744"><a href="#cb16-744" aria-hidden="true" tabindex="-1"></a>    ax.scatter(</span>
<span id="cb16-745"><a href="#cb16-745" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">0</span>],</span>
<span id="cb16-746"><a href="#cb16-746" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">1</span>],</span>
<span id="cb16-747"><a href="#cb16-747" aria-hidden="true" tabindex="-1"></a>        c<span class="op">=</span>colors[label], marker<span class="op">=</span><span class="st">"."</span>,</span>
<span id="cb16-748"><a href="#cb16-748" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span>label)</span>
<span id="cb16-749"><a href="#cb16-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-750"><a href="#cb16-750" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"left"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-751"><a href="#cb16-751" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"right"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-752"><a href="#cb16-752" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"top"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-753"><a href="#cb16-753" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"bottom"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-754"><a href="#cb16-754" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([])</span>
<span id="cb16-755"><a href="#cb16-755" aria-hidden="true" tabindex="-1"></a>ax.set_yticks([])</span>
<span id="cb16-756"><a href="#cb16-756" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Sammon mapping"</span>)</span>
<span id="cb16-757"><a href="#cb16-757" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb16-758"><a href="#cb16-758" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-759"><a href="#cb16-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-762"><a href="#cb16-762" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-763"><a href="#cb16-763" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-isomap-lle</span></span>
<span id="cb16-764"><a href="#cb16-764" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Visualization by Isomap and LLE"</span></span>
<span id="cb16-765"><a href="#cb16-765" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb16-766"><a href="#cb16-766" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb16-767"><a href="#cb16-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-768"><a href="#cb16-768" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb16-769"><a href="#cb16-769" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 3. Visualization by Isomap and LLE</span></span>
<span id="cb16-770"><a href="#cb16-770" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb16-771"><a href="#cb16-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-772"><a href="#cb16-772" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">12</span>))</span>
<span id="cb16-773"><a href="#cb16-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-774"><a href="#cb16-774" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb16-775"><a href="#cb16-775" aria-hidden="true" tabindex="-1"></a><span class="co"># ISOMAP</span></span>
<span id="cb16-776"><a href="#cb16-776" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb16-777"><a href="#cb16-777" aria-hidden="true" tabindex="-1"></a>isomap <span class="op">=</span> manifold.Isomap(n_components<span class="op">=</span>n_components, n_neighbors<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb16-778"><a href="#cb16-778" aria-hidden="true" tabindex="-1"></a>X_transformed <span class="op">=</span> mem.cache(isomap.fit_transform)(X)</span>
<span id="cb16-779"><a href="#cb16-779" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label <span class="kw">in</span> np.unique(y):</span>
<span id="cb16-780"><a href="#cb16-780" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> y <span class="op">==</span> label</span>
<span id="cb16-781"><a href="#cb16-781" aria-hidden="true" tabindex="-1"></a>    ax.scatter(</span>
<span id="cb16-782"><a href="#cb16-782" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">0</span>],</span>
<span id="cb16-783"><a href="#cb16-783" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">1</span>],</span>
<span id="cb16-784"><a href="#cb16-784" aria-hidden="true" tabindex="-1"></a>        c<span class="op">=</span>colors[label], marker<span class="op">=</span><span class="st">"."</span>,</span>
<span id="cb16-785"><a href="#cb16-785" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span>label)</span>
<span id="cb16-786"><a href="#cb16-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-787"><a href="#cb16-787" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"left"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-788"><a href="#cb16-788" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"right"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-789"><a href="#cb16-789" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"top"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-790"><a href="#cb16-790" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"bottom"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-791"><a href="#cb16-791" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([])</span>
<span id="cb16-792"><a href="#cb16-792" aria-hidden="true" tabindex="-1"></a>ax.set_yticks([])</span>
<span id="cb16-793"><a href="#cb16-793" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Isomap"</span>)</span>
<span id="cb16-794"><a href="#cb16-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-795"><a href="#cb16-795" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb16-796"><a href="#cb16-796" aria-hidden="true" tabindex="-1"></a><span class="co"># LLE</span></span>
<span id="cb16-797"><a href="#cb16-797" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb16-798"><a href="#cb16-798" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> manifold.LocallyLinearEmbedding(</span>
<span id="cb16-799"><a href="#cb16-799" aria-hidden="true" tabindex="-1"></a>    n_components<span class="op">=</span>n_components,</span>
<span id="cb16-800"><a href="#cb16-800" aria-hidden="true" tabindex="-1"></a>    n_neighbors<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb16-801"><a href="#cb16-801" aria-hidden="true" tabindex="-1"></a>X_transformed <span class="op">=</span> mem.cache(embedding.fit_transform)(X)</span>
<span id="cb16-802"><a href="#cb16-802" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label <span class="kw">in</span> np.unique(y):</span>
<span id="cb16-803"><a href="#cb16-803" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> y <span class="op">==</span> label</span>
<span id="cb16-804"><a href="#cb16-804" aria-hidden="true" tabindex="-1"></a>    ax.scatter(</span>
<span id="cb16-805"><a href="#cb16-805" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">0</span>],</span>
<span id="cb16-806"><a href="#cb16-806" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">1</span>],</span>
<span id="cb16-807"><a href="#cb16-807" aria-hidden="true" tabindex="-1"></a>        c<span class="op">=</span>colors[label], marker<span class="op">=</span><span class="st">"."</span>,</span>
<span id="cb16-808"><a href="#cb16-808" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span>label)</span>
<span id="cb16-809"><a href="#cb16-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-810"><a href="#cb16-810" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"left"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-811"><a href="#cb16-811" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"right"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-812"><a href="#cb16-812" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"top"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-813"><a href="#cb16-813" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"bottom"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-814"><a href="#cb16-814" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([])</span>
<span id="cb16-815"><a href="#cb16-815" aria-hidden="true" tabindex="-1"></a>ax.set_yticks([])</span>
<span id="cb16-816"><a href="#cb16-816" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"LLE"</span>)</span>
<span id="cb16-817"><a href="#cb16-817" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb16-818"><a href="#cb16-818" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-819"><a href="#cb16-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-822"><a href="#cb16-822" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-823"><a href="#cb16-823" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-olivetti</span></span>
<span id="cb16-824"><a href="#cb16-824" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Visualization of the Olivetti faces data set"</span></span>
<span id="cb16-825"><a href="#cb16-825" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb16-826"><a href="#cb16-826" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb16-827"><a href="#cb16-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-828"><a href="#cb16-828" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb16-829"><a href="#cb16-829" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 4. Visualization of the Olivetti faces data set</span></span>
<span id="cb16-830"><a href="#cb16-830" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb16-831"><a href="#cb16-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-832"><a href="#cb16-832" aria-hidden="true" tabindex="-1"></a><span class="co"># First, load the olivetti datasets</span></span>
<span id="cb16-833"><a href="#cb16-833" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_olivetti_faces</span>
<span id="cb16-834"><a href="#cb16-834" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> itertools</span>
<span id="cb16-835"><a href="#cb16-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-836"><a href="#cb16-836" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> fetch_olivetti_faces()</span>
<span id="cb16-837"><a href="#cb16-837" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[<span class="st">"data"</span>]</span>
<span id="cb16-838"><a href="#cb16-838" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">"target"</span>]</span>
<span id="cb16-839"><a href="#cb16-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-840"><a href="#cb16-840" aria-hidden="true" tabindex="-1"></a>markers <span class="op">=</span> [<span class="st">"+"</span>, <span class="st">"."</span>, <span class="st">"v"</span>, <span class="st">"^"</span>, <span class="st">"&gt;"</span>, <span class="st">"&lt;"</span>, <span class="st">"d"</span>, <span class="st">"*"</span>]</span>
<span id="cb16-841"><a href="#cb16-841" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">"black"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span>, <span class="st">"pink"</span>]</span>
<span id="cb16-842"><a href="#cb16-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-843"><a href="#cb16-843" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">2</span>, ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb16-844"><a href="#cb16-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-845"><a href="#cb16-845" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb16-846"><a href="#cb16-846" aria-hidden="true" tabindex="-1"></a><span class="co"># t-SNE manifold learning</span></span>
<span id="cb16-847"><a href="#cb16-847" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb16-848"><a href="#cb16-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-849"><a href="#cb16-849" aria-hidden="true" tabindex="-1"></a>tsne <span class="op">=</span> manifold.TSNE(n_components<span class="op">=</span>n_components, init<span class="op">=</span><span class="st">"pca"</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb16-850"><a href="#cb16-850" aria-hidden="true" tabindex="-1"></a>X_transformed <span class="op">=</span> mem.cache(tsne.fit_transform)(X)</span>
<span id="cb16-851"><a href="#cb16-851" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label, (m, c) <span class="kw">in</span> <span class="bu">zip</span>(np.unique(y), itertools.product(markers, colors)):</span>
<span id="cb16-852"><a href="#cb16-852" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> y <span class="op">==</span> label</span>
<span id="cb16-853"><a href="#cb16-853" aria-hidden="true" tabindex="-1"></a>    ax.scatter(</span>
<span id="cb16-854"><a href="#cb16-854" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">0</span>],</span>
<span id="cb16-855"><a href="#cb16-855" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">1</span>],</span>
<span id="cb16-856"><a href="#cb16-856" aria-hidden="true" tabindex="-1"></a>    marker<span class="op">=</span>m, c<span class="op">=</span>c,</span>
<span id="cb16-857"><a href="#cb16-857" aria-hidden="true" tabindex="-1"></a>    s<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb16-858"><a href="#cb16-858" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span>label)</span>
<span id="cb16-859"><a href="#cb16-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-860"><a href="#cb16-860" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"left"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-861"><a href="#cb16-861" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"right"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-862"><a href="#cb16-862" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"top"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-863"><a href="#cb16-863" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"bottom"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-864"><a href="#cb16-864" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([])</span>
<span id="cb16-865"><a href="#cb16-865" aria-hidden="true" tabindex="-1"></a>ax.set_yticks([])</span>
<span id="cb16-866"><a href="#cb16-866" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"t-SNE"</span>)</span>
<span id="cb16-867"><a href="#cb16-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-868"><a href="#cb16-868" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb16-869"><a href="#cb16-869" aria-hidden="true" tabindex="-1"></a><span class="co"># Sammon mapping</span></span>
<span id="cb16-870"><a href="#cb16-870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-871"><a href="#cb16-871" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb16-872"><a href="#cb16-872" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> manifold.MDS(n_components<span class="op">=</span>n_components)</span>
<span id="cb16-873"><a href="#cb16-873" aria-hidden="true" tabindex="-1"></a>X_transformed <span class="op">=</span> mem.cache(embedding.fit_transform)(X)</span>
<span id="cb16-874"><a href="#cb16-874" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label, (m, c) <span class="kw">in</span> <span class="bu">zip</span>(np.unique(y), itertools.product(markers, colors)):</span>
<span id="cb16-875"><a href="#cb16-875" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> y <span class="op">==</span> label</span>
<span id="cb16-876"><a href="#cb16-876" aria-hidden="true" tabindex="-1"></a>    ax.scatter(</span>
<span id="cb16-877"><a href="#cb16-877" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">0</span>],</span>
<span id="cb16-878"><a href="#cb16-878" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">1</span>],</span>
<span id="cb16-879"><a href="#cb16-879" aria-hidden="true" tabindex="-1"></a>    marker<span class="op">=</span>m, c<span class="op">=</span>c,</span>
<span id="cb16-880"><a href="#cb16-880" aria-hidden="true" tabindex="-1"></a>    s<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb16-881"><a href="#cb16-881" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span>label)</span>
<span id="cb16-882"><a href="#cb16-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-883"><a href="#cb16-883" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"left"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-884"><a href="#cb16-884" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"right"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-885"><a href="#cb16-885" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"top"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-886"><a href="#cb16-886" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"bottom"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-887"><a href="#cb16-887" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([])</span>
<span id="cb16-888"><a href="#cb16-888" aria-hidden="true" tabindex="-1"></a>ax.set_yticks([])</span>
<span id="cb16-889"><a href="#cb16-889" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Sammon mapping"</span>)</span>
<span id="cb16-890"><a href="#cb16-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-891"><a href="#cb16-891" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb16-892"><a href="#cb16-892" aria-hidden="true" tabindex="-1"></a><span class="co"># ISOMAP</span></span>
<span id="cb16-893"><a href="#cb16-893" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>, <span class="dv">0</span>]</span>
<span id="cb16-894"><a href="#cb16-894" aria-hidden="true" tabindex="-1"></a>isomap <span class="op">=</span> manifold.Isomap(n_components<span class="op">=</span>n_components, n_neighbors<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb16-895"><a href="#cb16-895" aria-hidden="true" tabindex="-1"></a>X_transformed <span class="op">=</span> mem.cache(isomap.fit_transform)(X)</span>
<span id="cb16-896"><a href="#cb16-896" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label, (m, c) <span class="kw">in</span> <span class="bu">zip</span>(np.unique(y), itertools.product(markers, colors)):</span>
<span id="cb16-897"><a href="#cb16-897" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> y <span class="op">==</span> label</span>
<span id="cb16-898"><a href="#cb16-898" aria-hidden="true" tabindex="-1"></a>    ax.scatter(</span>
<span id="cb16-899"><a href="#cb16-899" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">0</span>],</span>
<span id="cb16-900"><a href="#cb16-900" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">1</span>],</span>
<span id="cb16-901"><a href="#cb16-901" aria-hidden="true" tabindex="-1"></a>    marker<span class="op">=</span>m, c<span class="op">=</span>c,</span>
<span id="cb16-902"><a href="#cb16-902" aria-hidden="true" tabindex="-1"></a>    s<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb16-903"><a href="#cb16-903" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span>label)</span>
<span id="cb16-904"><a href="#cb16-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-905"><a href="#cb16-905" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"left"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-906"><a href="#cb16-906" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"right"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-907"><a href="#cb16-907" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"top"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-908"><a href="#cb16-908" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"bottom"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-909"><a href="#cb16-909" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([])</span>
<span id="cb16-910"><a href="#cb16-910" aria-hidden="true" tabindex="-1"></a>ax.set_yticks([])</span>
<span id="cb16-911"><a href="#cb16-911" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Isomap"</span>)</span>
<span id="cb16-912"><a href="#cb16-912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-913"><a href="#cb16-913" aria-hidden="true" tabindex="-1"></a><span class="co">###############################################################################</span></span>
<span id="cb16-914"><a href="#cb16-914" aria-hidden="true" tabindex="-1"></a><span class="co"># LLE</span></span>
<span id="cb16-915"><a href="#cb16-915" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb16-916"><a href="#cb16-916" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> manifold.LocallyLinearEmbedding(</span>
<span id="cb16-917"><a href="#cb16-917" aria-hidden="true" tabindex="-1"></a>    n_components<span class="op">=</span>n_components,</span>
<span id="cb16-918"><a href="#cb16-918" aria-hidden="true" tabindex="-1"></a>    n_neighbors<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb16-919"><a href="#cb16-919" aria-hidden="true" tabindex="-1"></a>X_transformed <span class="op">=</span> mem.cache(embedding.fit_transform)(X)</span>
<span id="cb16-920"><a href="#cb16-920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-921"><a href="#cb16-921" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label, (m, c) <span class="kw">in</span> <span class="bu">zip</span>(np.unique(y), itertools.product(markers, colors)):</span>
<span id="cb16-922"><a href="#cb16-922" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> y <span class="op">==</span> label</span>
<span id="cb16-923"><a href="#cb16-923" aria-hidden="true" tabindex="-1"></a>    ax.scatter(</span>
<span id="cb16-924"><a href="#cb16-924" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">0</span>],</span>
<span id="cb16-925"><a href="#cb16-925" aria-hidden="true" tabindex="-1"></a>        X_transformed[mask, <span class="dv">1</span>],</span>
<span id="cb16-926"><a href="#cb16-926" aria-hidden="true" tabindex="-1"></a>    marker<span class="op">=</span>m, c<span class="op">=</span>c,</span>
<span id="cb16-927"><a href="#cb16-927" aria-hidden="true" tabindex="-1"></a>    s<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb16-928"><a href="#cb16-928" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span>label)</span>
<span id="cb16-929"><a href="#cb16-929" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-930"><a href="#cb16-930" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"left"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-931"><a href="#cb16-931" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"right"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-932"><a href="#cb16-932" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"top"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-933"><a href="#cb16-933" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"bottom"</span>].set_linewidth(<span class="dv">0</span>)</span>
<span id="cb16-934"><a href="#cb16-934" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([])</span>
<span id="cb16-935"><a href="#cb16-935" aria-hidden="true" tabindex="-1"></a>ax.set_yticks([])</span>
<span id="cb16-936"><a href="#cb16-936" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"LLE"</span>)</span>
<span id="cb16-937"><a href="#cb16-937" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb16-938"><a href="#cb16-938" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-939"><a href="#cb16-939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-940"><a href="#cb16-940" aria-hidden="true" tabindex="-1"></a>In @fig-tsne-sammon and @fig-isomap-lle, we show the results of our experiments with t-SNE,</span>
<span id="cb16-941"><a href="#cb16-941" aria-hidden="true" tabindex="-1"></a>Sammon mapping, Isomap, and LLE on the MNIST data set. The results</span>
<span id="cb16-942"><a href="#cb16-942" aria-hidden="true" tabindex="-1"></a>reveal the strong performance of t-SNE compared to the other</span>
<span id="cb16-943"><a href="#cb16-943" aria-hidden="true" tabindex="-1"></a>techniques. In particular, Sammon mapping constructs a "ball" in which</span>
<span id="cb16-944"><a href="#cb16-944" aria-hidden="true" tabindex="-1"></a>only three classes (representing the digits 0, 1, and 7) are somewhat</span>
<span id="cb16-945"><a href="#cb16-945" aria-hidden="true" tabindex="-1"></a>separated from the other classes. Isomap and LLE produce solutions in</span>
<span id="cb16-946"><a href="#cb16-946" aria-hidden="true" tabindex="-1"></a>which there are large overlaps between the digit classes. In contrast,</span>
<span id="cb16-947"><a href="#cb16-947" aria-hidden="true" tabindex="-1"></a>tSNE constructs a map in which the separation between the digit</span>
<span id="cb16-948"><a href="#cb16-948" aria-hidden="true" tabindex="-1"></a>classes is almost perfect. Moreover, detailed inspection of the t-SNE</span>
<span id="cb16-949"><a href="#cb16-949" aria-hidden="true" tabindex="-1"></a>map reveals that much of the local structure of the data (such as the</span>
<span id="cb16-950"><a href="#cb16-950" aria-hidden="true" tabindex="-1"></a>orientation of the ones) is captured as well. This is illustrated in</span>
<span id="cb16-951"><a href="#cb16-951" aria-hidden="true" tabindex="-1"></a>more detail in @sec-large-data (see @fig-random_walk_tsne). The map produced by t-SNE</span>
<span id="cb16-952"><a href="#cb16-952" aria-hidden="true" tabindex="-1"></a>contains some points that are clustered with the wrong class, but most</span>
<span id="cb16-953"><a href="#cb16-953" aria-hidden="true" tabindex="-1"></a>of these points correspond to distorted digits many of which are</span>
<span id="cb16-954"><a href="#cb16-954" aria-hidden="true" tabindex="-1"></a>difficult to identify. @fig-olivetti shows the results of applying t-SNE,</span>
<span id="cb16-955"><a href="#cb16-955" aria-hidden="true" tabindex="-1"></a>Sammon mapping, Isomap, and LLE to the Olivetti faces data set. Again,</span>
<span id="cb16-956"><a href="#cb16-956" aria-hidden="true" tabindex="-1"></a>Isomap and LLE produce solutions that provide little insight into the</span>
<span id="cb16-957"><a href="#cb16-957" aria-hidden="true" tabindex="-1"></a>class structure of the data. The map constructed by Sammon mapping is</span>
<span id="cb16-958"><a href="#cb16-958" aria-hidden="true" tabindex="-1"></a>significantly better, since it models many of the members of each</span>
<span id="cb16-959"><a href="#cb16-959" aria-hidden="true" tabindex="-1"></a>class fairly close together, but none of the classes are clearly</span>
<span id="cb16-960"><a href="#cb16-960" aria-hidden="true" tabindex="-1"></a>separated in the Sammon map. In contrast, t-SNE does a much better job</span>
<span id="cb16-961"><a href="#cb16-961" aria-hidden="true" tabindex="-1"></a>of revealing the natural classes in the data. Some individuals have</span>
<span id="cb16-962"><a href="#cb16-962" aria-hidden="true" tabindex="-1"></a>their ten images split into two clusters, usually because a subset of</span>
<span id="cb16-963"><a href="#cb16-963" aria-hidden="true" tabindex="-1"></a>the images have the head facing in a significantly different</span>
<span id="cb16-964"><a href="#cb16-964" aria-hidden="true" tabindex="-1"></a>direction, or because they have a very different expression or</span>
<span id="cb16-965"><a href="#cb16-965" aria-hidden="true" tabindex="-1"></a>glasses. For these individuals, it is not clear that their ten images</span>
<span id="cb16-966"><a href="#cb16-966" aria-hidden="true" tabindex="-1"></a>form a natural class when using Euclidean distance in pixel space.</span>
<span id="cb16-967"><a href="#cb16-967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-968"><a href="#cb16-968" aria-hidden="true" tabindex="-1"></a>Figure 5 shows the results of applying t-SNE, Sammon mapping, Isomap,</span>
<span id="cb16-969"><a href="#cb16-969" aria-hidden="true" tabindex="-1"></a>and LLE to the COIL20 data set. For many of the 20 objects, t-SNE</span>
<span id="cb16-970"><a href="#cb16-970" aria-hidden="true" tabindex="-1"></a>accurately represents the one-dimensional manifold of viewpoints as a</span>
<span id="cb16-971"><a href="#cb16-971" aria-hidden="true" tabindex="-1"></a>closed loop. For objects which look similar from the front and the</span>
<span id="cb16-972"><a href="#cb16-972" aria-hidden="true" tabindex="-1"></a>back, t-SNE distorts the loop so that the images of front and back are</span>
<span id="cb16-973"><a href="#cb16-973" aria-hidden="true" tabindex="-1"></a>mapped to nearby points. For the four types of toy car in the COIL-20</span>
<span id="cb16-974"><a href="#cb16-974" aria-hidden="true" tabindex="-1"></a>data set (the four aligned "sausages" in the bottom-left of the tSNE</span>
<span id="cb16-975"><a href="#cb16-975" aria-hidden="true" tabindex="-1"></a>map), the four rotation manifolds are aligned by the orientation of</span>
<span id="cb16-976"><a href="#cb16-976" aria-hidden="true" tabindex="-1"></a>the cars to capture the high similarity between different cars at the</span>
<span id="cb16-977"><a href="#cb16-977" aria-hidden="true" tabindex="-1"></a>same orientation. This prevents t-SNE from keeping the four manifolds</span>
<span id="cb16-978"><a href="#cb16-978" aria-hidden="true" tabindex="-1"></a>clearly separate. Figure 5 also reveals that the other three</span>
<span id="cb16-979"><a href="#cb16-979" aria-hidden="true" tabindex="-1"></a>techniques are not nearly as good at cleanly separating the manifolds</span>
<span id="cb16-980"><a href="#cb16-980" aria-hidden="true" tabindex="-1"></a>that correspond to very different objects. In addition, Isomap and LLE</span>
<span id="cb16-981"><a href="#cb16-981" aria-hidden="true" tabindex="-1"></a>only visualize a small number of classes from the COIL-20 data set,</span>
<span id="cb16-982"><a href="#cb16-982" aria-hidden="true" tabindex="-1"></a>because the data set comprises a large number of widely separated</span>
<span id="cb16-983"><a href="#cb16-983" aria-hidden="true" tabindex="-1"></a>submanifolds that give rise to small connected components in the</span>
<span id="cb16-984"><a href="#cb16-984" aria-hidden="true" tabindex="-1"></a>neighborhood graph.</span>
<span id="cb16-985"><a href="#cb16-985" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-986"><a href="#cb16-986" aria-hidden="true" tabindex="-1"></a><span class="fu"># Applying t-SNE to Large Data Sets {#sec-large-data}</span></span>
<span id="cb16-987"><a href="#cb16-987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-988"><a href="#cb16-988" aria-hidden="true" tabindex="-1"></a>Like many other visualization techniques, t-SNE has a computational</span>
<span id="cb16-989"><a href="#cb16-989" aria-hidden="true" tabindex="-1"></a>and memory complexity that is quadratic in the number of</span>
<span id="cb16-990"><a href="#cb16-990" aria-hidden="true" tabindex="-1"></a>datapoints. This makes it infeasible to apply the standard version of</span>
<span id="cb16-991"><a href="#cb16-991" aria-hidden="true" tabindex="-1"></a>t-SNE to data sets that contain many more than, say, 10,000</span>
<span id="cb16-992"><a href="#cb16-992" aria-hidden="true" tabindex="-1"></a>points. Obviously, it is possible to pick a random subset of the</span>
<span id="cb16-993"><a href="#cb16-993" aria-hidden="true" tabindex="-1"></a>datapoints and display them using t-SNE, but such an approach fails to</span>
<span id="cb16-994"><a href="#cb16-994" aria-hidden="true" tabindex="-1"></a>make use of the information that the undisplayed datapoints provide</span>
<span id="cb16-995"><a href="#cb16-995" aria-hidden="true" tabindex="-1"></a>about the underlying manifolds.  Suppose, for example, that A, B, and</span>
<span id="cb16-996"><a href="#cb16-996" aria-hidden="true" tabindex="-1"></a>C are all equidistant in the high-dimensional space. If there are many</span>
<span id="cb16-997"><a href="#cb16-997" aria-hidden="true" tabindex="-1"></a>undisplayed datapoints between A and B and none between A and C, it is</span>
<span id="cb16-998"><a href="#cb16-998" aria-hidden="true" tabindex="-1"></a>much more likely that A and B are part of the same cluster than A and</span>
<span id="cb16-999"><a href="#cb16-999" aria-hidden="true" tabindex="-1"></a>C. This is illustrated in @fig-random-walk. In this section, we</span>
<span id="cb16-1000"><a href="#cb16-1000" aria-hidden="true" tabindex="-1"></a>show how t-SNE can be modified to display a random subset of the</span>
<span id="cb16-1001"><a href="#cb16-1001" aria-hidden="true" tabindex="-1"></a>datapoints (so-called landmark points) in a way that uses information</span>
<span id="cb16-1002"><a href="#cb16-1002" aria-hidden="true" tabindex="-1"></a>from the entire (possibly very large) data set.</span>
<span id="cb16-1003"><a href="#cb16-1003" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1004"><a href="#cb16-1004" aria-hidden="true" tabindex="-1"></a>![An illustration of the advantage of the random walk version of t-SNE over a standard</span>
<span id="cb16-1005"><a href="#cb16-1005" aria-hidden="true" tabindex="-1"></a>landmark approach. The shaded points A, B, and C are three (almost) equidistant landmark points, whereas the non-shaded datapoints are non-landmark points. The arrows represent a directed neighborhood graph where $k = 3$. In a standard landmark approach, the pairwise affinity between A and B is approximately equal to the pairwise affinity between A and C. In the random walk version of t-SNE, the pairwise affinity between A and B is much larger than the pairwise affinity between A and C, and therefore, it reflects the structure of the data much better.](figures/random_walk.png){#fig-random-walk}</span>
<span id="cb16-1006"><a href="#cb16-1006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1007"><a href="#cb16-1007" aria-hidden="true" tabindex="-1"></a>We start by choosing a desired number of neighbors and creating a</span>
<span id="cb16-1008"><a href="#cb16-1008" aria-hidden="true" tabindex="-1"></a>neighborhood graph for all of the datapoints. Although this is</span>
<span id="cb16-1009"><a href="#cb16-1009" aria-hidden="true" tabindex="-1"></a>computationally intensive, it is only done once. Then, for each of the</span>
<span id="cb16-1010"><a href="#cb16-1010" aria-hidden="true" tabindex="-1"></a>landmark points, we define a random walk starting at that landmark</span>
<span id="cb16-1011"><a href="#cb16-1011" aria-hidden="true" tabindex="-1"></a>point and terminating as soon as it lands on another landmark</span>
<span id="cb16-1012"><a href="#cb16-1012" aria-hidden="true" tabindex="-1"></a>point. During a random walk, the probability of choosing an edge</span>
<span id="cb16-1013"><a href="#cb16-1013" aria-hidden="true" tabindex="-1"></a>emanating from node xi to node x j is proportional to $e^{-\|x_i−x_j</span>
<span id="cb16-1014"><a href="#cb16-1014" aria-hidden="true" tabindex="-1"></a>\|^2}$ . We define $p_{j|i}$ to be the fraction of random walks</span>
<span id="cb16-1015"><a href="#cb16-1015" aria-hidden="true" tabindex="-1"></a>starting at landmark point $x_i$ that terminate at landmark point</span>
<span id="cb16-1016"><a href="#cb16-1016" aria-hidden="true" tabindex="-1"></a>$x_j$ . This has some resemblance to the way Isomap measures pairwise</span>
<span id="cb16-1017"><a href="#cb16-1017" aria-hidden="true" tabindex="-1"></a>distances between points. However, as in diffusion maps</span>
<span id="cb16-1018"><a href="#cb16-1018" aria-hidden="true" tabindex="-1"></a>@lafon:diffusion,@nadler:diffusion, rather than looking</span>
<span id="cb16-1019"><a href="#cb16-1019" aria-hidden="true" tabindex="-1"></a>for the shortest path through the neighborhood graph, the random</span>
<span id="cb16-1020"><a href="#cb16-1020" aria-hidden="true" tabindex="-1"></a>walk-based affinity measure integrates over all paths through the</span>
<span id="cb16-1021"><a href="#cb16-1021" aria-hidden="true" tabindex="-1"></a>neighborhood graph. As a result, the random walk-based affinity</span>
<span id="cb16-1022"><a href="#cb16-1022" aria-hidden="true" tabindex="-1"></a>measure is much less sensitive to "short-circuits"</span>
<span id="cb16-1023"><a href="#cb16-1023" aria-hidden="true" tabindex="-1"></a>@lee:nonlinear2005, in which a single noisy datapoint provides a</span>
<span id="cb16-1024"><a href="#cb16-1024" aria-hidden="true" tabindex="-1"></a>bridge between two regions of dataspace that should be far apart in</span>
<span id="cb16-1025"><a href="#cb16-1025" aria-hidden="true" tabindex="-1"></a>the map. Similar approaches using random walks have also been</span>
<span id="cb16-1026"><a href="#cb16-1026" aria-hidden="true" tabindex="-1"></a>successfully applied to, for example, semi-supervised learning</span>
<span id="cb16-1027"><a href="#cb16-1027" aria-hidden="true" tabindex="-1"></a>@jaakola:partially,@zhu:semi and image segmentation</span>
<span id="cb16-1028"><a href="#cb16-1028" aria-hidden="true" tabindex="-1"></a>@grady:random.</span>
<span id="cb16-1029"><a href="#cb16-1029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1030"><a href="#cb16-1030" aria-hidden="true" tabindex="-1"></a>The most obvious way to compute the random walk-based similarities</span>
<span id="cb16-1031"><a href="#cb16-1031" aria-hidden="true" tabindex="-1"></a>$p_{j|i}$ is to explicitly perform the random walks on the</span>
<span id="cb16-1032"><a href="#cb16-1032" aria-hidden="true" tabindex="-1"></a>neighborhood graph, which works very well in practice, given that one</span>
<span id="cb16-1033"><a href="#cb16-1033" aria-hidden="true" tabindex="-1"></a>can easily perform one million random walks per second. Alternatively,</span>
<span id="cb16-1034"><a href="#cb16-1034" aria-hidden="true" tabindex="-1"></a>@grady:random presents an analytical solution to compute the</span>
<span id="cb16-1035"><a href="#cb16-1035" aria-hidden="true" tabindex="-1"></a>pairwise similarities $p_{j|i}$ that involves solving a sparse linear</span>
<span id="cb16-1036"><a href="#cb16-1036" aria-hidden="true" tabindex="-1"></a>system. The analytical solution to compute the similarities $p_{j|i}$</span>
<span id="cb16-1037"><a href="#cb16-1037" aria-hidden="true" tabindex="-1"></a>is sketched in Appendix B (FIXME). In preliminary experiments, we did not find</span>
<span id="cb16-1038"><a href="#cb16-1038" aria-hidden="true" tabindex="-1"></a>significant differences between performing the random walks explicitly</span>
<span id="cb16-1039"><a href="#cb16-1039" aria-hidden="true" tabindex="-1"></a>and the analytical solution. In the experiment we present below, we</span>
<span id="cb16-1040"><a href="#cb16-1040" aria-hidden="true" tabindex="-1"></a>explicitly performed the random walks because this is computationally</span>
<span id="cb16-1041"><a href="#cb16-1041" aria-hidden="true" tabindex="-1"></a>less expensive. However, for very large data sets in which the</span>
<span id="cb16-1042"><a href="#cb16-1042" aria-hidden="true" tabindex="-1"></a>landmark points are very sparse, the analytical solution may be more</span>
<span id="cb16-1043"><a href="#cb16-1043" aria-hidden="true" tabindex="-1"></a>appropriate.</span>
<span id="cb16-1044"><a href="#cb16-1044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1045"><a href="#cb16-1045" aria-hidden="true" tabindex="-1"></a>::: {#fig-random_walk_tsne} </span>
<span id="cb16-1046"><a href="#cb16-1046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1047"><a href="#cb16-1047" aria-hidden="true" tabindex="-1"></a><span class="al">![](figures/random_walk_tSNE.png)</span></span>
<span id="cb16-1048"><a href="#cb16-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1049"><a href="#cb16-1049" aria-hidden="true" tabindex="-1"></a>Visualization of 6,000 digits from the MNIST data set produced by the random walk</span>
<span id="cb16-1050"><a href="#cb16-1050" aria-hidden="true" tabindex="-1"></a>version of t-SNE (employing all 60,000 digit images).</span>
<span id="cb16-1051"><a href="#cb16-1051" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-1052"><a href="#cb16-1052" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1053"><a href="#cb16-1053" aria-hidden="true" tabindex="-1"></a>@fig-random_walk_tsne shows the results of an experiment, in which we</span>
<span id="cb16-1054"><a href="#cb16-1054" aria-hidden="true" tabindex="-1"></a>applied the random walk version of t-SNE to 6,000 randomly selected</span>
<span id="cb16-1055"><a href="#cb16-1055" aria-hidden="true" tabindex="-1"></a>digits from the MNIST data set, using all 60,000 digits to compute the</span>
<span id="cb16-1056"><a href="#cb16-1056" aria-hidden="true" tabindex="-1"></a>pairwise affinities $p_{j|i}$. In the experiment, we used a</span>
<span id="cb16-1057"><a href="#cb16-1057" aria-hidden="true" tabindex="-1"></a>neighborhood graph that was constructed using a value of $k = 20$</span>
<span id="cb16-1058"><a href="#cb16-1058" aria-hidden="true" tabindex="-1"></a>nearest neighbors.<span class="ot">[^knn]</span> The inset of the figure shows the same</span>
<span id="cb16-1059"><a href="#cb16-1059" aria-hidden="true" tabindex="-1"></a>visualization as a scatterplot in which the colors represent the</span>
<span id="cb16-1060"><a href="#cb16-1060" aria-hidden="true" tabindex="-1"></a>labels of the digits. In the t-SNE map, all classes are clearly</span>
<span id="cb16-1061"><a href="#cb16-1061" aria-hidden="true" tabindex="-1"></a>separated and the "continental" sevens form a small separate cluster.</span>
<span id="cb16-1062"><a href="#cb16-1062" aria-hidden="true" tabindex="-1"></a>Moreover, t-SNE reveals the main dimensions of variation within each</span>
<span id="cb16-1063"><a href="#cb16-1063" aria-hidden="true" tabindex="-1"></a>class, such as the orientation of the ones, fours, sevens, and nines,</span>
<span id="cb16-1064"><a href="#cb16-1064" aria-hidden="true" tabindex="-1"></a>or the "loopiness" of the twos. The strong performance of t-SNE is</span>
<span id="cb16-1065"><a href="#cb16-1065" aria-hidden="true" tabindex="-1"></a>also reflected in the generalization error of nearest neighbor</span>
<span id="cb16-1066"><a href="#cb16-1066" aria-hidden="true" tabindex="-1"></a>classifiers that are trained on the low-dimensional</span>
<span id="cb16-1067"><a href="#cb16-1067" aria-hidden="true" tabindex="-1"></a>representation. Whereas the generalization error (measured using</span>
<span id="cb16-1068"><a href="#cb16-1068" aria-hidden="true" tabindex="-1"></a>10-fold cross validation) of a 1-nearest neighbor classifier trained</span>
<span id="cb16-1069"><a href="#cb16-1069" aria-hidden="true" tabindex="-1"></a>on the original 784-dimensional datapoints is 5.75%, the</span>
<span id="cb16-1070"><a href="#cb16-1070" aria-hidden="true" tabindex="-1"></a>generalization error of a 1-nearest neighbor classifier trained on the</span>
<span id="cb16-1071"><a href="#cb16-1071" aria-hidden="true" tabindex="-1"></a>two-dimensional data representation produced by t-SNE is only</span>
<span id="cb16-1072"><a href="#cb16-1072" aria-hidden="true" tabindex="-1"></a>5.13%. The computational requirements of random walk t-SNE are</span>
<span id="cb16-1073"><a href="#cb16-1073" aria-hidden="true" tabindex="-1"></a>reasonable: it took only one hour of CPU time to construct the map in</span>
<span id="cb16-1074"><a href="#cb16-1074" aria-hidden="true" tabindex="-1"></a>@fig-random_walk_tsne.</span>
<span id="cb16-1075"><a href="#cb16-1075" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1076"><a href="#cb16-1076" aria-hidden="true" tabindex="-1"></a><span class="ot">[^knn]: </span>In preliminary experiments, we found the performance of random</span>
<span id="cb16-1077"><a href="#cb16-1077" aria-hidden="true" tabindex="-1"></a>    walk t-SNE to be very robust under changes of $k$.</span>
<span id="cb16-1078"><a href="#cb16-1078" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1079"><a href="#cb16-1079" aria-hidden="true" tabindex="-1"></a><span class="fu"># Discussion {#sec-discussion}</span></span>
<span id="cb16-1080"><a href="#cb16-1080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1081"><a href="#cb16-1081" aria-hidden="true" tabindex="-1"></a>The results in the previous two sections (and those in the</span>
<span id="cb16-1082"><a href="#cb16-1082" aria-hidden="true" tabindex="-1"></a>supplemental material) demonstrate the performance of t-SNE on a wide</span>
<span id="cb16-1083"><a href="#cb16-1083" aria-hidden="true" tabindex="-1"></a>variety of data sets. In this section, we discuss the differences</span>
<span id="cb16-1084"><a href="#cb16-1084" aria-hidden="true" tabindex="-1"></a>between t-SNE and other non-parametric techniques</span>
<span id="cb16-1085"><a href="#cb16-1085" aria-hidden="true" tabindex="-1"></a>(<span class="co">[</span><span class="ot"> 6.1</span><span class="co">](sec-comparison)</span>), and we also discuss a number of</span>
<span id="cb16-1086"><a href="#cb16-1086" aria-hidden="true" tabindex="-1"></a>weaknesses and possible improvements of t-SNE</span>
<span id="cb16-1087"><a href="#cb16-1087" aria-hidden="true" tabindex="-1"></a>(<span class="co">[</span><span class="ot"> 6.2</span><span class="co">](sec-weakness)</span>).</span>
<span id="cb16-1088"><a href="#cb16-1088" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1089"><a href="#cb16-1089" aria-hidden="true" tabindex="-1"></a><span class="fu">## Comparison with Related Techniques {#sec-comparison}</span></span>
<span id="cb16-1090"><a href="#cb16-1090" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1091"><a href="#cb16-1091" aria-hidden="true" tabindex="-1"></a>Classical scaling @torgerson:multidimensional, which is closely</span>
<span id="cb16-1092"><a href="#cb16-1092" aria-hidden="true" tabindex="-1"></a>related to PCA @mardia:multivariate</span>
<span id="cb16-1093"><a href="#cb16-1093" aria-hidden="true" tabindex="-1"></a>@williams:connection, finds a linear transformation of the data</span>
<span id="cb16-1094"><a href="#cb16-1094" aria-hidden="true" tabindex="-1"></a>that minimizes the sum of the squared errors between high-dimensional</span>
<span id="cb16-1095"><a href="#cb16-1095" aria-hidden="true" tabindex="-1"></a>pairwise distances and their low-dimensional representatives. A linear</span>
<span id="cb16-1096"><a href="#cb16-1096" aria-hidden="true" tabindex="-1"></a>method such as classical scaling is not good at modeling curved</span>
<span id="cb16-1097"><a href="#cb16-1097" aria-hidden="true" tabindex="-1"></a>manifolds and it focuses on preserving the distances between widely</span>
<span id="cb16-1098"><a href="#cb16-1098" aria-hidden="true" tabindex="-1"></a>separated datapoints rather than on preserving the distances between</span>
<span id="cb16-1099"><a href="#cb16-1099" aria-hidden="true" tabindex="-1"></a>nearby datapoints. An important approach that attempts to address the</span>
<span id="cb16-1100"><a href="#cb16-1100" aria-hidden="true" tabindex="-1"></a>problems of classical scaling is the Sammon mapping</span>
<span id="cb16-1101"><a href="#cb16-1101" aria-hidden="true" tabindex="-1"></a>@sammon:nonlinear which alters the cost function of classical</span>
<span id="cb16-1102"><a href="#cb16-1102" aria-hidden="true" tabindex="-1"></a>scaling by dividing the squared error in the representation of each</span>
<span id="cb16-1103"><a href="#cb16-1103" aria-hidden="true" tabindex="-1"></a>pairwise Euclidean distance by the original Euclidean distance in the</span>
<span id="cb16-1104"><a href="#cb16-1104" aria-hidden="true" tabindex="-1"></a>high-dimensional space. The resulting cost function is given by</span>
<span id="cb16-1105"><a href="#cb16-1105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1106"><a href="#cb16-1106" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1107"><a href="#cb16-1107" aria-hidden="true" tabindex="-1"></a>    C = \frac{1}{\sum_{ij} \|x_i - x_j\|} \sum_{i \neq j} \frac{\left(\|x_i - x_j\| - \|x_i - x_j\|\right)^2}{\|x_i - x_j\|}\,</span>
<span id="cb16-1108"><a href="#cb16-1108" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1109"><a href="#cb16-1109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1110"><a href="#cb16-1110" aria-hidden="true" tabindex="-1"></a>where the constant outside of the sum is added in order to simplify</span>
<span id="cb16-1111"><a href="#cb16-1111" aria-hidden="true" tabindex="-1"></a>the derivation of the gradient.  The main weakness of the Sammon cost</span>
<span id="cb16-1112"><a href="#cb16-1112" aria-hidden="true" tabindex="-1"></a>function is that the importance of retaining small pairwise distances</span>
<span id="cb16-1113"><a href="#cb16-1113" aria-hidden="true" tabindex="-1"></a>in the map is largely dependent on small differences in these pairwise</span>
<span id="cb16-1114"><a href="#cb16-1114" aria-hidden="true" tabindex="-1"></a>distances. In particular, a small error in the model of two</span>
<span id="cb16-1115"><a href="#cb16-1115" aria-hidden="true" tabindex="-1"></a>high-dimensional points that are extremely close together results in a</span>
<span id="cb16-1116"><a href="#cb16-1116" aria-hidden="true" tabindex="-1"></a>large contribution to the cost function. Since all small pairwise</span>
<span id="cb16-1117"><a href="#cb16-1117" aria-hidden="true" tabindex="-1"></a>distances constitute the local structure of the data, it seems more</span>
<span id="cb16-1118"><a href="#cb16-1118" aria-hidden="true" tabindex="-1"></a>appropriate to aim to assign approximately equal importance to all</span>
<span id="cb16-1119"><a href="#cb16-1119" aria-hidden="true" tabindex="-1"></a>small pairwise distances.</span>
<span id="cb16-1120"><a href="#cb16-1120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1121"><a href="#cb16-1121" aria-hidden="true" tabindex="-1"></a>In contrast to Sammon mapping, the Gaussian kernel employed in the</span>
<span id="cb16-1122"><a href="#cb16-1122" aria-hidden="true" tabindex="-1"></a>high-dimensional space by t-SNE defines a soft border between the</span>
<span id="cb16-1123"><a href="#cb16-1123" aria-hidden="true" tabindex="-1"></a>local and global structure of the data and for pairs of datapoints</span>
<span id="cb16-1124"><a href="#cb16-1124" aria-hidden="true" tabindex="-1"></a>that are close together relative to the standard deviation of the</span>
<span id="cb16-1125"><a href="#cb16-1125" aria-hidden="true" tabindex="-1"></a>Gaussian, the importance of modeling their separations is almost</span>
<span id="cb16-1126"><a href="#cb16-1126" aria-hidden="true" tabindex="-1"></a>independent of the magnitudes of those separations. Moreover, t-SNE</span>
<span id="cb16-1127"><a href="#cb16-1127" aria-hidden="true" tabindex="-1"></a>determines the local neighborhood size for each datapoint separately</span>
<span id="cb16-1128"><a href="#cb16-1128" aria-hidden="true" tabindex="-1"></a>based on the local density of the data (by forcing each conditional</span>
<span id="cb16-1129"><a href="#cb16-1129" aria-hidden="true" tabindex="-1"></a>probability distribution $P_i$ to have the same perplexity).</span>
<span id="cb16-1130"><a href="#cb16-1130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1131"><a href="#cb16-1131" aria-hidden="true" tabindex="-1"></a>The strong performance of t-SNE compared to Isomap is partly explained</span>
<span id="cb16-1132"><a href="#cb16-1132" aria-hidden="true" tabindex="-1"></a>by Isomap’s susceptibility to “short-circuiting”. Also, Isomap mainly</span>
<span id="cb16-1133"><a href="#cb16-1133" aria-hidden="true" tabindex="-1"></a>focuses on modeling large geodesic distances rather than small ones.</span>
<span id="cb16-1134"><a href="#cb16-1134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1135"><a href="#cb16-1135" aria-hidden="true" tabindex="-1"></a>The strong performance of t-SNE compared to LLE is mainly due to a</span>
<span id="cb16-1136"><a href="#cb16-1136" aria-hidden="true" tabindex="-1"></a>basic weakness of LLE: the only thing that prevents all datapoints</span>
<span id="cb16-1137"><a href="#cb16-1137" aria-hidden="true" tabindex="-1"></a>from collapsing onto a single point is a constraint on the covariance</span>
<span id="cb16-1138"><a href="#cb16-1138" aria-hidden="true" tabindex="-1"></a>of the low-dimensional representation. In practice, this constraint is</span>
<span id="cb16-1139"><a href="#cb16-1139" aria-hidden="true" tabindex="-1"></a>often satisfied by placing most of the map points near the center of</span>
<span id="cb16-1140"><a href="#cb16-1140" aria-hidden="true" tabindex="-1"></a>the map and using a few widely scattered points to create large</span>
<span id="cb16-1141"><a href="#cb16-1141" aria-hidden="true" tabindex="-1"></a>covariance (see Figure FIXME). For neighborhood graphs that are almost</span>
<span id="cb16-1142"><a href="#cb16-1142" aria-hidden="true" tabindex="-1"></a>disconnected, the covariance constraint can also be satisfied by a</span>
<span id="cb16-1143"><a href="#cb16-1143" aria-hidden="true" tabindex="-1"></a>“curdled” map in which there are a few widely separated, collapsed</span>
<span id="cb16-1144"><a href="#cb16-1144" aria-hidden="true" tabindex="-1"></a>subsets corresponding to the almost disconnected components.</span>
<span id="cb16-1145"><a href="#cb16-1145" aria-hidden="true" tabindex="-1"></a>Furthermore, neighborhood-graph based techniques (such as Isomap and</span>
<span id="cb16-1146"><a href="#cb16-1146" aria-hidden="true" tabindex="-1"></a>LLE) are not capable of visualizing data that consists of two or more</span>
<span id="cb16-1147"><a href="#cb16-1147" aria-hidden="true" tabindex="-1"></a>widely separated submanifolds, because such data does not give rise to</span>
<span id="cb16-1148"><a href="#cb16-1148" aria-hidden="true" tabindex="-1"></a>a connected neighborhood graph. It is possible to produce a separate</span>
<span id="cb16-1149"><a href="#cb16-1149" aria-hidden="true" tabindex="-1"></a>map for each connected component, but this loses information about the</span>
<span id="cb16-1150"><a href="#cb16-1150" aria-hidden="true" tabindex="-1"></a>relative similarities of the separate components.</span>
<span id="cb16-1151"><a href="#cb16-1151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1152"><a href="#cb16-1152" aria-hidden="true" tabindex="-1"></a>Like Isomap and LLE, the random walk version of t-SNE employs</span>
<span id="cb16-1153"><a href="#cb16-1153" aria-hidden="true" tabindex="-1"></a>neighborhood graphs, but it does not suffer from short-circuiting</span>
<span id="cb16-1154"><a href="#cb16-1154" aria-hidden="true" tabindex="-1"></a>problems because the pairwise similarities between the highdimensional</span>
<span id="cb16-1155"><a href="#cb16-1155" aria-hidden="true" tabindex="-1"></a>datapoints are computed by integrating over all paths through the</span>
<span id="cb16-1156"><a href="#cb16-1156" aria-hidden="true" tabindex="-1"></a>neighborhood graph.  Because of the diffusion-based interpretation of</span>
<span id="cb16-1157"><a href="#cb16-1157" aria-hidden="true" tabindex="-1"></a>the conditional probabilities underlying the random walk version of</span>
<span id="cb16-1158"><a href="#cb16-1158" aria-hidden="true" tabindex="-1"></a>t-SNE, it is useful to compare t-SNE to diffusion maps. Diffusion maps</span>
<span id="cb16-1159"><a href="#cb16-1159" aria-hidden="true" tabindex="-1"></a>define a "diffusion distance" on the high-dimensional datapoints that</span>
<span id="cb16-1160"><a href="#cb16-1160" aria-hidden="true" tabindex="-1"></a>is given by</span>
<span id="cb16-1161"><a href="#cb16-1161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1162"><a href="#cb16-1162" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb16-1163"><a href="#cb16-1163" aria-hidden="true" tabindex="-1"></a>D^{(t)}(x_i,x_j) = \sqrt{ \sum_{k} \frac{\left(p^{(t)}_{ik} -</span>
<span id="cb16-1164"><a href="#cb16-1164" aria-hidden="true" tabindex="-1"></a>p^{(t)}_{jk}\right)^2)}{\psi(x_k)^{(0)}} }\,</span>
<span id="cb16-1165"><a href="#cb16-1165" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1166"><a href="#cb16-1166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1167"><a href="#cb16-1167" aria-hidden="true" tabindex="-1"></a>where $p^{(t)}_{ij}$ represents the probability of a particle</span>
<span id="cb16-1168"><a href="#cb16-1168" aria-hidden="true" tabindex="-1"></a>traveling from $x_i$ to $x_j$ in $t$ timesteps through a graph on the</span>
<span id="cb16-1169"><a href="#cb16-1169" aria-hidden="true" tabindex="-1"></a>data with Gaussian emission probabilities. The term $\psi(x_k)^{(0)}$</span>
<span id="cb16-1170"><a href="#cb16-1170" aria-hidden="true" tabindex="-1"></a>is a measure for the local density of the points, and serves a similar</span>
<span id="cb16-1171"><a href="#cb16-1171" aria-hidden="true" tabindex="-1"></a>purpose to the fixed perplexity Gaussian kernel that is employed in</span>
<span id="cb16-1172"><a href="#cb16-1172" aria-hidden="true" tabindex="-1"></a>SNE. The diffusion map is formed by the principal non-trivial</span>
<span id="cb16-1173"><a href="#cb16-1173" aria-hidden="true" tabindex="-1"></a>eigenvectors of the Markov matrix of the random walks of length</span>
<span id="cb16-1174"><a href="#cb16-1174" aria-hidden="true" tabindex="-1"></a>$t$. It can be shown that when all $(n−1)$ non-trivial eigenvectors</span>
<span id="cb16-1175"><a href="#cb16-1175" aria-hidden="true" tabindex="-1"></a>are employed, the Euclidean distances in the diffusion map are equal</span>
<span id="cb16-1176"><a href="#cb16-1176" aria-hidden="true" tabindex="-1"></a>to the diffusion distances in the high-dimensional data representation</span>
<span id="cb16-1177"><a href="#cb16-1177" aria-hidden="true" tabindex="-1"></a>@lafon:diffusion. Mathematically, diffusion maps minimize</span>
<span id="cb16-1178"><a href="#cb16-1178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1179"><a href="#cb16-1179" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1180"><a href="#cb16-1180" aria-hidden="true" tabindex="-1"></a>C = \sum_i \sum_j \left(D^{(t)}(x_i,x_j) - \|y_i-y_j\|\right)^2</span>
<span id="cb16-1181"><a href="#cb16-1181" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1182"><a href="#cb16-1182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1183"><a href="#cb16-1183" aria-hidden="true" tabindex="-1"></a>As a result, diffusion maps are susceptible to the same problems as</span>
<span id="cb16-1184"><a href="#cb16-1184" aria-hidden="true" tabindex="-1"></a>classical scaling: they assign much higher importance to modeling the</span>
<span id="cb16-1185"><a href="#cb16-1185" aria-hidden="true" tabindex="-1"></a>large pairwise diffusion distances than the small ones and as a</span>
<span id="cb16-1186"><a href="#cb16-1186" aria-hidden="true" tabindex="-1"></a>result, they are not good at retaining the local structure of the</span>
<span id="cb16-1187"><a href="#cb16-1187" aria-hidden="true" tabindex="-1"></a>data. Moreover, in contrast to the random walk version of t-SNE,</span>
<span id="cb16-1188"><a href="#cb16-1188" aria-hidden="true" tabindex="-1"></a>diffusion maps do not have a natural way of selecting the length, $t$,</span>
<span id="cb16-1189"><a href="#cb16-1189" aria-hidden="true" tabindex="-1"></a>of the random walks.</span>
<span id="cb16-1190"><a href="#cb16-1190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1191"><a href="#cb16-1191" aria-hidden="true" tabindex="-1"></a>In the supplemental material, we present results that reveal that</span>
<span id="cb16-1192"><a href="#cb16-1192" aria-hidden="true" tabindex="-1"></a>t-SNE outperforms CCA @demartines:curvilinear, MVU</span>
<span id="cb16-1193"><a href="#cb16-1193" aria-hidden="true" tabindex="-1"></a>@weinberger:learning, and Laplacian Eigenmaps</span>
<span id="cb16-1194"><a href="#cb16-1194" aria-hidden="true" tabindex="-1"></a>@belkin:laplacian as well. For CCA and the closely related CDA</span>
<span id="cb16-1195"><a href="#cb16-1195" aria-hidden="true" tabindex="-1"></a>@lee:robust, these results can be partially explained by the</span>
<span id="cb16-1196"><a href="#cb16-1196" aria-hidden="true" tabindex="-1"></a>hard border $\lambda$ that these techniques define between local and global</span>
<span id="cb16-1197"><a href="#cb16-1197" aria-hidden="true" tabindex="-1"></a>structure, as opposed to the soft border of t-SNE. Moreover, within</span>
<span id="cb16-1198"><a href="#cb16-1198" aria-hidden="true" tabindex="-1"></a>the range $\lambda$, CCA suffers from the same weakness as Sammon mapping: it</span>
<span id="cb16-1199"><a href="#cb16-1199" aria-hidden="true" tabindex="-1"></a>assigns extremely high importance to modeling the distance between two</span>
<span id="cb16-1200"><a href="#cb16-1200" aria-hidden="true" tabindex="-1"></a>datapoints that are extremely close.</span>
<span id="cb16-1201"><a href="#cb16-1201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1202"><a href="#cb16-1202" aria-hidden="true" tabindex="-1"></a>Like t-SNE, MVU @weinberger:learning tries to model all of the</span>
<span id="cb16-1203"><a href="#cb16-1203" aria-hidden="true" tabindex="-1"></a>small separations well but MVU insists on modeling them perfectly</span>
<span id="cb16-1204"><a href="#cb16-1204" aria-hidden="true" tabindex="-1"></a>(i.e., it treats them as constraints) and a single erroneous</span>
<span id="cb16-1205"><a href="#cb16-1205" aria-hidden="true" tabindex="-1"></a>constraint may severely affect the performance of MVU. This can occur</span>
<span id="cb16-1206"><a href="#cb16-1206" aria-hidden="true" tabindex="-1"></a>when there is a short-circuit between two parts of a curved manifold</span>
<span id="cb16-1207"><a href="#cb16-1207" aria-hidden="true" tabindex="-1"></a>that are far apart in the intrinsic manifold coordinates. Also, MVU</span>
<span id="cb16-1208"><a href="#cb16-1208" aria-hidden="true" tabindex="-1"></a>makes no attempt to model longer range structure: It simply pulls the</span>
<span id="cb16-1209"><a href="#cb16-1209" aria-hidden="true" tabindex="-1"></a>map points as far apart as possible subject to the hard constraints</span>
<span id="cb16-1210"><a href="#cb16-1210" aria-hidden="true" tabindex="-1"></a>so, unlike t-SNE, it cannot be expected to produce sensible</span>
<span id="cb16-1211"><a href="#cb16-1211" aria-hidden="true" tabindex="-1"></a>large-scale structure in the map.</span>
<span id="cb16-1212"><a href="#cb16-1212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1213"><a href="#cb16-1213" aria-hidden="true" tabindex="-1"></a>For Laplacian Eigenmaps, the poor results relative to t-SNE may be</span>
<span id="cb16-1214"><a href="#cb16-1214" aria-hidden="true" tabindex="-1"></a>explained by the fact that Laplacian Eigenmaps have the same</span>
<span id="cb16-1215"><a href="#cb16-1215" aria-hidden="true" tabindex="-1"></a>covariance constraint as LLE, and it is easy to cheat on this</span>
<span id="cb16-1216"><a href="#cb16-1216" aria-hidden="true" tabindex="-1"></a>constraint.</span>
<span id="cb16-1217"><a href="#cb16-1217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1218"><a href="#cb16-1218" aria-hidden="true" tabindex="-1"></a><span class="fu">## Weakness {#sec-weakness}</span></span>
<span id="cb16-1219"><a href="#cb16-1219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1220"><a href="#cb16-1220" aria-hidden="true" tabindex="-1"></a>Although we have shown that t-SNE comparesfavorably to other</span>
<span id="cb16-1221"><a href="#cb16-1221" aria-hidden="true" tabindex="-1"></a>techniquesfor data visualization, tSNE has three potential weaknesses:</span>
<span id="cb16-1222"><a href="#cb16-1222" aria-hidden="true" tabindex="-1"></a>(1) it is unclear how t-SNE performs on general dimensionality</span>
<span id="cb16-1223"><a href="#cb16-1223" aria-hidden="true" tabindex="-1"></a>reduction tasks, (2) the relatively local nature of t-SNE makes it</span>
<span id="cb16-1224"><a href="#cb16-1224" aria-hidden="true" tabindex="-1"></a>sensitive to the curse of the intrinsic dimensionality of the data,</span>
<span id="cb16-1225"><a href="#cb16-1225" aria-hidden="true" tabindex="-1"></a>and (3) t-SNE is not guaranteed to converge to a global optimum of its</span>
<span id="cb16-1226"><a href="#cb16-1226" aria-hidden="true" tabindex="-1"></a>cost function. Below, we discuss the three weaknesses in more detail.</span>
<span id="cb16-1227"><a href="#cb16-1227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1228"><a href="#cb16-1228" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>*Dimensionality reduction for other purposes.* It is not obvious</span>
<span id="cb16-1229"><a href="#cb16-1229" aria-hidden="true" tabindex="-1"></a>how t-SNE will perform on the more general task of dimensionality</span>
<span id="cb16-1230"><a href="#cb16-1230" aria-hidden="true" tabindex="-1"></a>reduction (i.e., when the dimensionality of the data is not reduced to</span>
<span id="cb16-1231"><a href="#cb16-1231" aria-hidden="true" tabindex="-1"></a>two or three, but to $d &gt; 3$ dimensions). To simplify evaluation issues,</span>
<span id="cb16-1232"><a href="#cb16-1232" aria-hidden="true" tabindex="-1"></a>this paper only considers the use of t-SNE for data visualization. The</span>
<span id="cb16-1233"><a href="#cb16-1233" aria-hidden="true" tabindex="-1"></a>behavior of t-SNE when reducing data to two or three dimensions cannot</span>
<span id="cb16-1234"><a href="#cb16-1234" aria-hidden="true" tabindex="-1"></a>readily be extrapolated to $d &gt; 3$ dimensions because of the heavy tails</span>
<span id="cb16-1235"><a href="#cb16-1235" aria-hidden="true" tabindex="-1"></a>of the Student-t distribution. In high-dimensional spaces, the heavy</span>
<span id="cb16-1236"><a href="#cb16-1236" aria-hidden="true" tabindex="-1"></a>tails comprise a relatively large portion of the probability mass</span>
<span id="cb16-1237"><a href="#cb16-1237" aria-hidden="true" tabindex="-1"></a>under the Student-t distribution, which might lead to d-dimensional</span>
<span id="cb16-1238"><a href="#cb16-1238" aria-hidden="true" tabindex="-1"></a>data representations that do not preserve the local structure of the</span>
<span id="cb16-1239"><a href="#cb16-1239" aria-hidden="true" tabindex="-1"></a>data as well. Hence, for tasks in which the dimensionality of the data</span>
<span id="cb16-1240"><a href="#cb16-1240" aria-hidden="true" tabindex="-1"></a>needs to be reduced to a dimensionality higher than three, Student</span>
<span id="cb16-1241"><a href="#cb16-1241" aria-hidden="true" tabindex="-1"></a>t-distributions with more than one degree of freedom10 are likely to</span>
<span id="cb16-1242"><a href="#cb16-1242" aria-hidden="true" tabindex="-1"></a>be more appropriate.</span>
<span id="cb16-1243"><a href="#cb16-1243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1244"><a href="#cb16-1244" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>*Curse of intrinsic dimensionality.* t-SNE reduces the</span>
<span id="cb16-1245"><a href="#cb16-1245" aria-hidden="true" tabindex="-1"></a>dimensionality of data mainly based on local properties of the data,</span>
<span id="cb16-1246"><a href="#cb16-1246" aria-hidden="true" tabindex="-1"></a>which makes t-SNE sensitive to the curse of the intrinsic</span>
<span id="cb16-1247"><a href="#cb16-1247" aria-hidden="true" tabindex="-1"></a>dimensionality of the data @bengio:learning. In data sets with</span>
<span id="cb16-1248"><a href="#cb16-1248" aria-hidden="true" tabindex="-1"></a>a high intrinsic dimensionality and an underlying manifold that is</span>
<span id="cb16-1249"><a href="#cb16-1249" aria-hidden="true" tabindex="-1"></a>highly varying, the local linearity assumption on the manifold that</span>
<span id="cb16-1250"><a href="#cb16-1250" aria-hidden="true" tabindex="-1"></a>t-SNE implicitly makes (by employing Euclidean distances between near</span>
<span id="cb16-1251"><a href="#cb16-1251" aria-hidden="true" tabindex="-1"></a>neighbors) may be violated. As a result, t-SNE might be less</span>
<span id="cb16-1252"><a href="#cb16-1252" aria-hidden="true" tabindex="-1"></a>successful if it is applied on data sets with a very high intrinsic</span>
<span id="cb16-1253"><a href="#cb16-1253" aria-hidden="true" tabindex="-1"></a>dimensionality (for instance, a recent study by @meytlis:face</span>
<span id="cb16-1254"><a href="#cb16-1254" aria-hidden="true" tabindex="-1"></a>estimates the space of images of faces to be constituted of</span>
<span id="cb16-1255"><a href="#cb16-1255" aria-hidden="true" tabindex="-1"></a>approximately 100 dimensions). Manifold learners such as Isomap and</span>
<span id="cb16-1256"><a href="#cb16-1256" aria-hidden="true" tabindex="-1"></a>LLE suffer from exactly the same problems (see, e.g.,</span>
<span id="cb16-1257"><a href="#cb16-1257" aria-hidden="true" tabindex="-1"></a>@bengio:learning; @vandermaaten:comparison ).  A</span>
<span id="cb16-1258"><a href="#cb16-1258" aria-hidden="true" tabindex="-1"></a>possible way to (partially) address this issue is by performing t-SNE</span>
<span id="cb16-1259"><a href="#cb16-1259" aria-hidden="true" tabindex="-1"></a>on a data representation obtained from a model that represents the</span>
<span id="cb16-1260"><a href="#cb16-1260" aria-hidden="true" tabindex="-1"></a>highly varying data manifold efficiently in a number of nonlinear</span>
<span id="cb16-1261"><a href="#cb16-1261" aria-hidden="true" tabindex="-1"></a>layers such as an autoencoder @hinton:reducing. Such deep-layer</span>
<span id="cb16-1262"><a href="#cb16-1262" aria-hidden="true" tabindex="-1"></a>architectures can represent complex nonlinear functions in a much</span>
<span id="cb16-1263"><a href="#cb16-1263" aria-hidden="true" tabindex="-1"></a>simpler way, and as a result, require fewer datapoints to learn an</span>
<span id="cb16-1264"><a href="#cb16-1264" aria-hidden="true" tabindex="-1"></a>appropriate solution (as is illustrated for a d-bits parity task by</span>
<span id="cb16-1265"><a href="#cb16-1265" aria-hidden="true" tabindex="-1"></a>@bengio:learning). Performing t-SNE on a data representation</span>
<span id="cb16-1266"><a href="#cb16-1266" aria-hidden="true" tabindex="-1"></a>produced by, for example, an autoencoder is likely to improve the</span>
<span id="cb16-1267"><a href="#cb16-1267" aria-hidden="true" tabindex="-1"></a>quality of the constructed visualizations, because autoencoders can</span>
<span id="cb16-1268"><a href="#cb16-1268" aria-hidden="true" tabindex="-1"></a>identify highly-varying manifolds better than a local method such as</span>
<span id="cb16-1269"><a href="#cb16-1269" aria-hidden="true" tabindex="-1"></a>t-SNE. However, the reader should note that it is by definition</span>
<span id="cb16-1270"><a href="#cb16-1270" aria-hidden="true" tabindex="-1"></a>impossible to fully represent the structure of intrinsically</span>
<span id="cb16-1271"><a href="#cb16-1271" aria-hidden="true" tabindex="-1"></a>high-dimensional data in two or three dimensions.</span>
<span id="cb16-1272"><a href="#cb16-1272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1273"><a href="#cb16-1273" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>*Non-convexity of the t-SNE cost function.* A nice property of most</span>
<span id="cb16-1274"><a href="#cb16-1274" aria-hidden="true" tabindex="-1"></a>state-of-the-art dimensionality reduction techniques (such as</span>
<span id="cb16-1275"><a href="#cb16-1275" aria-hidden="true" tabindex="-1"></a>classical scaling, Isomap, LLE, and diffusion maps) is the convexity</span>
<span id="cb16-1276"><a href="#cb16-1276" aria-hidden="true" tabindex="-1"></a>of their cost functions. A major weakness of t-SNE is that the cost</span>
<span id="cb16-1277"><a href="#cb16-1277" aria-hidden="true" tabindex="-1"></a>function is not convex, as a result of which several optimization</span>
<span id="cb16-1278"><a href="#cb16-1278" aria-hidden="true" tabindex="-1"></a>parameters need to be chosen. The constructed solutions depend on</span>
<span id="cb16-1279"><a href="#cb16-1279" aria-hidden="true" tabindex="-1"></a>these choices of optimization parameters and may be different each</span>
<span id="cb16-1280"><a href="#cb16-1280" aria-hidden="true" tabindex="-1"></a>time t-SNE is run from an initial random configuration of map</span>
<span id="cb16-1281"><a href="#cb16-1281" aria-hidden="true" tabindex="-1"></a>points. We have demonstrated that the same choice of optimization</span>
<span id="cb16-1282"><a href="#cb16-1282" aria-hidden="true" tabindex="-1"></a>parameters can be used for a variety of different visualization tasks,</span>
<span id="cb16-1283"><a href="#cb16-1283" aria-hidden="true" tabindex="-1"></a>and we found that the quality of the optima does not vary much from</span>
<span id="cb16-1284"><a href="#cb16-1284" aria-hidden="true" tabindex="-1"></a>run to run. Therefore, we think that the weakness of the optimization</span>
<span id="cb16-1285"><a href="#cb16-1285" aria-hidden="true" tabindex="-1"></a>method is insufficient reason to reject t-SNE in favor of methods that</span>
<span id="cb16-1286"><a href="#cb16-1286" aria-hidden="true" tabindex="-1"></a>lead to convex optimization problems but produce noticeably worse</span>
<span id="cb16-1287"><a href="#cb16-1287" aria-hidden="true" tabindex="-1"></a>visualizations. A local optimum of a cost function that accurately</span>
<span id="cb16-1288"><a href="#cb16-1288" aria-hidden="true" tabindex="-1"></a>captures what we want in a visualization is often preferable to the</span>
<span id="cb16-1289"><a href="#cb16-1289" aria-hidden="true" tabindex="-1"></a>global optimum of a cost function that fails to capture important</span>
<span id="cb16-1290"><a href="#cb16-1290" aria-hidden="true" tabindex="-1"></a>aspects of what we want. Moreover, the convexity of cost functions can</span>
<span id="cb16-1291"><a href="#cb16-1291" aria-hidden="true" tabindex="-1"></a>be misleading, because their optimization is often computationally</span>
<span id="cb16-1292"><a href="#cb16-1292" aria-hidden="true" tabindex="-1"></a>infeasible for large real-world data sets, prompting the use of</span>
<span id="cb16-1293"><a href="#cb16-1293" aria-hidden="true" tabindex="-1"></a>approximation techniques @desilva:global;</span>
<span id="cb16-1294"><a href="#cb16-1294" aria-hidden="true" tabindex="-1"></a>@weinberger:graph. Even for LLE and Laplacian Eigenmaps, the</span>
<span id="cb16-1295"><a href="#cb16-1295" aria-hidden="true" tabindex="-1"></a>optimization is performed using iterative Arnoldi</span>
<span id="cb16-1296"><a href="#cb16-1296" aria-hidden="true" tabindex="-1"></a>@arnoldi:principle or Jacobi-Davidson @fokkema:jacobi</span>
<span id="cb16-1297"><a href="#cb16-1297" aria-hidden="true" tabindex="-1"></a>methods, which may fail to find the global optimum due to convergence</span>
<span id="cb16-1298"><a href="#cb16-1298" aria-hidden="true" tabindex="-1"></a>problems.</span>
<span id="cb16-1299"><a href="#cb16-1299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1300"><a href="#cb16-1300" aria-hidden="true" tabindex="-1"></a><span class="fu"># Conclusions {#sec-conclusion}</span></span>
<span id="cb16-1301"><a href="#cb16-1301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1302"><a href="#cb16-1302" aria-hidden="true" tabindex="-1"></a>The paper presents a new technique for the visualization of similarity</span>
<span id="cb16-1303"><a href="#cb16-1303" aria-hidden="true" tabindex="-1"></a>data that is capable of retaining the local structure of the data</span>
<span id="cb16-1304"><a href="#cb16-1304" aria-hidden="true" tabindex="-1"></a>while also revealing some important global structure (such as clusters</span>
<span id="cb16-1305"><a href="#cb16-1305" aria-hidden="true" tabindex="-1"></a>at multiple scales). Both the computational and the memory complexity</span>
<span id="cb16-1306"><a href="#cb16-1306" aria-hidden="true" tabindex="-1"></a>of t-SNE are $\mathcal{O}(n^2)$, but we present a landmark approach</span>
<span id="cb16-1307"><a href="#cb16-1307" aria-hidden="true" tabindex="-1"></a>that makes it possible to successfully visualize large real-world data</span>
<span id="cb16-1308"><a href="#cb16-1308" aria-hidden="true" tabindex="-1"></a>sets with limited computational demands. Our experiments on a variety</span>
<span id="cb16-1309"><a href="#cb16-1309" aria-hidden="true" tabindex="-1"></a>of data sets show that t-SNE outperforms existing state-of-the-art</span>
<span id="cb16-1310"><a href="#cb16-1310" aria-hidden="true" tabindex="-1"></a>techniques for visualizing a variety of real-world data sets. Matlab</span>
<span id="cb16-1311"><a href="#cb16-1311" aria-hidden="true" tabindex="-1"></a>implementations of both the normal and the random walk version of</span>
<span id="cb16-1312"><a href="#cb16-1312" aria-hidden="true" tabindex="-1"></a>t-SNE are available for download at</span>
<span id="cb16-1313"><a href="#cb16-1313" aria-hidden="true" tabindex="-1"></a><span class="ot">&lt;https://lvdmaaten.github.io/tsne/&gt;</span>.  In future work we plan to</span>
<span id="cb16-1314"><a href="#cb16-1314" aria-hidden="true" tabindex="-1"></a>investigate the optimization of the number of degrees of freedom of</span>
<span id="cb16-1315"><a href="#cb16-1315" aria-hidden="true" tabindex="-1"></a>the Student-t distribution used in t-SNE. This may be helpful for</span>
<span id="cb16-1316"><a href="#cb16-1316" aria-hidden="true" tabindex="-1"></a>dimensionality reduction when the low-dimensional representation has</span>
<span id="cb16-1317"><a href="#cb16-1317" aria-hidden="true" tabindex="-1"></a>many dimensions. We will also investigate the extension of t-SNE to</span>
<span id="cb16-1318"><a href="#cb16-1318" aria-hidden="true" tabindex="-1"></a>models in which each high-dimensional datapoint is modeled by several</span>
<span id="cb16-1319"><a href="#cb16-1319" aria-hidden="true" tabindex="-1"></a>low-dimensional map points as in @cook:visualizing. Also, we</span>
<span id="cb16-1320"><a href="#cb16-1320" aria-hidden="true" tabindex="-1"></a>aim to develop a parametric version of t-SNE that allows for</span>
<span id="cb16-1321"><a href="#cb16-1321" aria-hidden="true" tabindex="-1"></a>generalization to held-out test data by using the t-SNE objective</span>
<span id="cb16-1322"><a href="#cb16-1322" aria-hidden="true" tabindex="-1"></a>function to train a multilayer neural network that provides an</span>
<span id="cb16-1323"><a href="#cb16-1323" aria-hidden="true" tabindex="-1"></a>explicit mapping to the low-dimensional space</span>
<span id="cb16-1324"><a href="#cb16-1324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1325"><a href="#cb16-1325" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Acknowledgments {.appendix .unnumbered}</span></span>
<span id="cb16-1326"><a href="#cb16-1326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1327"><a href="#cb16-1327" aria-hidden="true" tabindex="-1"></a>The authors thank Sam Roweis for many helpful discussions, Andriy Mnih</span>
<span id="cb16-1328"><a href="#cb16-1328" aria-hidden="true" tabindex="-1"></a>for supplying the wordfeatures data set, Ruslan Salakhutdinov for help</span>
<span id="cb16-1329"><a href="#cb16-1329" aria-hidden="true" tabindex="-1"></a>with the Netflix data set (results for these data sets are presented</span>
<span id="cb16-1330"><a href="#cb16-1330" aria-hidden="true" tabindex="-1"></a>in the supplemental material), and Guido de Croon for pointing us to</span>
<span id="cb16-1331"><a href="#cb16-1331" aria-hidden="true" tabindex="-1"></a>the analytical solution of the random walk probabilities.</span>
<span id="cb16-1332"><a href="#cb16-1332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1333"><a href="#cb16-1333" aria-hidden="true" tabindex="-1"></a>Laurens van der Maaten is supported by the CATCH-programme of the</span>
<span id="cb16-1334"><a href="#cb16-1334" aria-hidden="true" tabindex="-1"></a>Dutch Scientific Organization (NWO), project RICH (grant 640.002.401),</span>
<span id="cb16-1335"><a href="#cb16-1335" aria-hidden="true" tabindex="-1"></a>and cooperates with RACM. Geoffrey Hinton is a fellow of the Canadian</span>
<span id="cb16-1336"><a href="#cb16-1336" aria-hidden="true" tabindex="-1"></a>Institute for Advanced Research, and is also supported by grants from</span>
<span id="cb16-1337"><a href="#cb16-1337" aria-hidden="true" tabindex="-1"></a>NSERC and CFI and gifts from Google and Microsoft.</span>
<span id="cb16-1338"><a href="#cb16-1338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1339"><a href="#cb16-1339" aria-hidden="true" tabindex="-1"></a><span class="fu"># Appendix A. Derivation of the t-SNE gradient {.appendix .unnumbered}</span></span>
<span id="cb16-1340"><a href="#cb16-1340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1341"><a href="#cb16-1341" aria-hidden="true" tabindex="-1"></a>t-SNE minimizes the Kullback-Leibler divergence between the joint</span>
<span id="cb16-1342"><a href="#cb16-1342" aria-hidden="true" tabindex="-1"></a>probabilities $p_{ij}$ in the highdimensional space and the joint</span>
<span id="cb16-1343"><a href="#cb16-1343" aria-hidden="true" tabindex="-1"></a>probabilities $q_{ij}$ in the low-dimensional space. The values of</span>
<span id="cb16-1344"><a href="#cb16-1344" aria-hidden="true" tabindex="-1"></a>$p_{ij}$ are defined to be the symmetrized conditional probabilities,</span>
<span id="cb16-1345"><a href="#cb16-1345" aria-hidden="true" tabindex="-1"></a>whereas the values of $q_{ij}$ are obtained by means of a Student-t</span>
<span id="cb16-1346"><a href="#cb16-1346" aria-hidden="true" tabindex="-1"></a>distribution with one degree of freedom</span>
<span id="cb16-1347"><a href="#cb16-1347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1348"><a href="#cb16-1348" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1349"><a href="#cb16-1349" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb16-1350"><a href="#cb16-1350" aria-hidden="true" tabindex="-1"></a>p_{ij} &amp; = \frac{p_{j|i} + p_{i|j}}{2 n} <span class="sc">\\</span></span>
<span id="cb16-1351"><a href="#cb16-1351" aria-hidden="true" tabindex="-1"></a>q_{ij} &amp; = \frac{\left(1 + \|y_i - y_j \|^2\right)^{-1}}{\sum_{k\neq \ell} \left(1 + \|y_k - y_\ell \|^2\right)^{-1}}</span>
<span id="cb16-1352"><a href="#cb16-1352" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb16-1353"><a href="#cb16-1353" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1354"><a href="#cb16-1354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1355"><a href="#cb16-1355" aria-hidden="true" tabindex="-1"></a>where $p_{j|i}$ and $p_{i|j}$ are either obtained from Equation 1 or</span>
<span id="cb16-1356"><a href="#cb16-1356" aria-hidden="true" tabindex="-1"></a>from the random walk procedure described in @sec-large-data. The values of $p_{ii}$ and $q_{ii}$ are set to</span>
<span id="cb16-1357"><a href="#cb16-1357" aria-hidden="true" tabindex="-1"></a>zero. The Kullback-Leibler divergence between the two joint</span>
<span id="cb16-1358"><a href="#cb16-1358" aria-hidden="true" tabindex="-1"></a>probability distributions $P$ and $Q$ is given by</span>
<span id="cb16-1359"><a href="#cb16-1359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1360"><a href="#cb16-1360" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1361"><a href="#cb16-1361" aria-hidden="true" tabindex="-1"></a>C = KL(P \| Q) = \sum_i \sum_j p_{ij} \log \frac{p_{ij}}{q_{ij}}  = \sum_i \sum_j p_{ij} \log p_{ij} - p_{ij} \log q_{ij}.</span>
<span id="cb16-1362"><a href="#cb16-1362" aria-hidden="true" tabindex="-1"></a>$$ {#eq-appendixa1}</span>
<span id="cb16-1363"><a href="#cb16-1363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1364"><a href="#cb16-1364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1365"><a href="#cb16-1365" aria-hidden="true" tabindex="-1"></a>In order to make the derivation less cluttered, we define two</span>
<span id="cb16-1366"><a href="#cb16-1366" aria-hidden="true" tabindex="-1"></a>auxiliary variables $d_{ij}$ and $Z$ as follows</span>
<span id="cb16-1367"><a href="#cb16-1367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1368"><a href="#cb16-1368" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1369"><a href="#cb16-1369" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb16-1370"><a href="#cb16-1370" aria-hidden="true" tabindex="-1"></a>d_{ij} &amp; = \|y_i - y_j\|, <span class="sc">\\</span></span>
<span id="cb16-1371"><a href="#cb16-1371" aria-hidden="true" tabindex="-1"></a>Z &amp; = \sum_{k\neq \ell} \left(1 + d_{k\ell}^2 \right)^{-1}.</span>
<span id="cb16-1372"><a href="#cb16-1372" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb16-1373"><a href="#cb16-1373" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1374"><a href="#cb16-1374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1375"><a href="#cb16-1375" aria-hidden="true" tabindex="-1"></a>Note that if $y_i$ changes, the only pairwise distances that change</span>
<span id="cb16-1376"><a href="#cb16-1376" aria-hidden="true" tabindex="-1"></a>are $d_{ij}$ and $d_{ji}$ for all $j$. Hence, the gradient of the cost</span>
<span id="cb16-1377"><a href="#cb16-1377" aria-hidden="true" tabindex="-1"></a>function $C$ with respect to $y_i$ is given by</span>
<span id="cb16-1378"><a href="#cb16-1378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1379"><a href="#cb16-1379" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1380"><a href="#cb16-1380" aria-hidden="true" tabindex="-1"></a>\frac{\partial C}{\partial y_i} = \sum_j \left(\frac{\partial C}{\partial d_{ij}} + \frac{\partial C}{\partial d_{ji}}\right) (y_i - y_j) = 2 \sum_j \frac{\partial C}{\partial d_{ij}} (y_i - y_j)</span>
<span id="cb16-1381"><a href="#cb16-1381" aria-hidden="true" tabindex="-1"></a>$$ {#eq-appendixa2}</span>
<span id="cb16-1382"><a href="#cb16-1382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1383"><a href="#cb16-1383" aria-hidden="true" tabindex="-1"></a>The gradient $\frac{\partial C}{\partial d_{ji}}$ is computed</span>
<span id="cb16-1384"><a href="#cb16-1384" aria-hidden="true" tabindex="-1"></a>from the definition of the Kullback-Leibler divergence in @eq-appendixa1</span>
<span id="cb16-1385"><a href="#cb16-1385" aria-hidden="true" tabindex="-1"></a>(note that he first part of this equation is a constant).</span>
<span id="cb16-1386"><a href="#cb16-1386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1387"><a href="#cb16-1387" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1388"><a href="#cb16-1388" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb16-1389"><a href="#cb16-1389" aria-hidden="true" tabindex="-1"></a>\frac{\partial C}{\partial d_{ij}} </span>
<span id="cb16-1390"><a href="#cb16-1390" aria-hidden="true" tabindex="-1"></a>&amp; = - \sum_{k\neq\ell} p_{k\ell} \frac{\partial \log q_{k\ell}}{\partial d_{ij}} <span class="sc">\\</span></span>
<span id="cb16-1391"><a href="#cb16-1391" aria-hidden="true" tabindex="-1"></a>&amp; = - \sum_{k\neq\ell} p_{k\ell} \frac{\partial \log q_{k\ell}Q - \log Z}{\partial d_{ij}} <span class="sc">\\</span></span>
<span id="cb16-1392"><a href="#cb16-1392" aria-hidden="true" tabindex="-1"></a>&amp; = - \sum_{k\neq\ell} p_{k\ell} \left(\frac{1}{q_{k\ell}Z} \frac{\partial ((1 - d_{k\ell}^2)^{-1})}{\partial d_{ij}} - \frac{1}{Z}\frac{\partial Z}{\partial d_{ij}} \right) ) </span>
<span id="cb16-1393"><a href="#cb16-1393" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb16-1394"><a href="#cb16-1394" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1395"><a href="#cb16-1395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1396"><a href="#cb16-1396" aria-hidden="true" tabindex="-1"></a>The gradient $\frac{\partial ((1 - d_{k\ell}^2)^{-1})}{\partial d_{ij}}$ is only onzero when $k=i$ and $\ell = j$. Hence, the gradient $\frac{\partial C}{\partial d_{ij}}$ is given by</span>
<span id="cb16-1397"><a href="#cb16-1397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1398"><a href="#cb16-1398" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1399"><a href="#cb16-1399" aria-hidden="true" tabindex="-1"></a>\frac{\partial C}{\partial d_{ij}} + 2 \frac{p{ij}}{q_{ij}Z} (1 = d_{ij}^2)^{-2} - 2 \sum_{k\neq \ell} p_{k\ell} \frac{(1+d_{ij}^2)^{-2}}{Z}.</span>
<span id="cb16-1400"><a href="#cb16-1400" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1401"><a href="#cb16-1401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1402"><a href="#cb16-1402" aria-hidden="true" tabindex="-1"></a>Noting that $\sum_{k\neq \ell} p_{k\ell} = 1$, we see that the gradients simplifies to</span>
<span id="cb16-1403"><a href="#cb16-1403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1404"><a href="#cb16-1404" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1405"><a href="#cb16-1405" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb16-1406"><a href="#cb16-1406" aria-hidden="true" tabindex="-1"></a>\frac{\partial C}{\partial d_{ij}} &amp; = 2 p{ij} (1 + d_{ij}^2)^{-1} - 2 q_{ij}(1 + d_{ij}^2)^{-1} <span class="sc">\\</span></span>
<span id="cb16-1407"><a href="#cb16-1407" aria-hidden="true" tabindex="-1"></a>&amp; = 2 (p{ij} - q_{ij}) (1 + d_{ij}^2)^{-1}.</span>
<span id="cb16-1408"><a href="#cb16-1408" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb16-1409"><a href="#cb16-1409" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1410"><a href="#cb16-1410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1411"><a href="#cb16-1411" aria-hidden="true" tabindex="-1"></a>Substituting this term into @eq-appendixa2, we obtain the gradient</span>
<span id="cb16-1412"><a href="#cb16-1412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1413"><a href="#cb16-1413" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1414"><a href="#cb16-1414" aria-hidden="true" tabindex="-1"></a>\frac{\partial C}{\partial y_i} = 4 \sum_j (p{ij} - q_{ij}) (1 + \|y_i - y_j\|^2)^{-1} (y_i - y_j).</span>
<span id="cb16-1415"><a href="#cb16-1415" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1416"><a href="#cb16-1416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1417"><a href="#cb16-1417" aria-hidden="true" tabindex="-1"></a><span class="fu"># Appendix B. Analytical Solution to Random Walk Probabilities {.appendix .unnumbered}</span></span>
<span id="cb16-1418"><a href="#cb16-1418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1419"><a href="#cb16-1419" aria-hidden="true" tabindex="-1"></a>Below, we describe the analytical solution to the random walk</span>
<span id="cb16-1420"><a href="#cb16-1420" aria-hidden="true" tabindex="-1"></a>probabilities that are employed in the random walk version of t-SNE</span>
<span id="cb16-1421"><a href="#cb16-1421" aria-hidden="true" tabindex="-1"></a>(@sec-large-data). The solution is described in more</span>
<span id="cb16-1422"><a href="#cb16-1422" aria-hidden="true" tabindex="-1"></a>detail @grady:random.</span>
<span id="cb16-1423"><a href="#cb16-1423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1424"><a href="#cb16-1424" aria-hidden="true" tabindex="-1"></a>It can be shown that computing the probability that a random walk</span>
<span id="cb16-1425"><a href="#cb16-1425" aria-hidden="true" tabindex="-1"></a>initiated from a non-landmark point (on a graph that is specified by</span>
<span id="cb16-1426"><a href="#cb16-1426" aria-hidden="true" tabindex="-1"></a>adjacency matrix W) first reaches a specific landmark point is equal</span>
<span id="cb16-1427"><a href="#cb16-1427" aria-hidden="true" tabindex="-1"></a>to computing the solution to the combinatorial Dirichlet problem in</span>
<span id="cb16-1428"><a href="#cb16-1428" aria-hidden="true" tabindex="-1"></a>which the boundary conditions are at the locations of the landmark</span>
<span id="cb16-1429"><a href="#cb16-1429" aria-hidden="true" tabindex="-1"></a>points, the considered landmark point is fixed to unity, and the other</span>
<span id="cb16-1430"><a href="#cb16-1430" aria-hidden="true" tabindex="-1"></a>landmarks points are set to zero @kakutani:dirichlet ;</span>
<span id="cb16-1431"><a href="#cb16-1431" aria-hidden="true" tabindex="-1"></a>@doyle:random.  In practice, the solution can thus be obtained</span>
<span id="cb16-1432"><a href="#cb16-1432" aria-hidden="true" tabindex="-1"></a>by minimizing the combinatorial formulation of the Dirichlet integral</span>
<span id="cb16-1433"><a href="#cb16-1433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1434"><a href="#cb16-1434" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1435"><a href="#cb16-1435" aria-hidden="true" tabindex="-1"></a>D<span class="co">[</span><span class="ot">x</span><span class="co">]</span> = \frac12 x^\top L x,</span>
<span id="cb16-1436"><a href="#cb16-1436" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1437"><a href="#cb16-1437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1438"><a href="#cb16-1438" aria-hidden="true" tabindex="-1"></a>where $L$ represents the graph Laplacian. Mathematically, the graph</span>
<span id="cb16-1439"><a href="#cb16-1439" aria-hidden="true" tabindex="-1"></a>Laplacian is given by $L = D−W$, where $D = \mathrm{diag} (\sum_j</span>
<span id="cb16-1440"><a href="#cb16-1440" aria-hidden="true" tabindex="-1"></a>w_{1j}, \sum_j w_{2j}, \dots, \sum_j w_{nj} )$. Without loss of</span>
<span id="cb16-1441"><a href="#cb16-1441" aria-hidden="true" tabindex="-1"></a>generality, we may reorder the landmark points such that the landmark</span>
<span id="cb16-1442"><a href="#cb16-1442" aria-hidden="true" tabindex="-1"></a>points come first. As a result, the combinatorial Dirichlet integral</span>
<span id="cb16-1443"><a href="#cb16-1443" aria-hidden="true" tabindex="-1"></a>decomposes into</span>
<span id="cb16-1444"><a href="#cb16-1444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1445"><a href="#cb16-1445" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1446"><a href="#cb16-1446" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb16-1447"><a href="#cb16-1447" aria-hidden="true" tabindex="-1"></a>D_{x_N} &amp; = \frac12 </span>
<span id="cb16-1448"><a href="#cb16-1448" aria-hidden="true" tabindex="-1"></a>\left[\begin{array}{cc}</span>
<span id="cb16-1449"><a href="#cb16-1449" aria-hidden="true" tabindex="-1"></a>x_L^\top &amp; x_N^\top \end{array}\right] \, </span>
<span id="cb16-1450"><a href="#cb16-1450" aria-hidden="true" tabindex="-1"></a>\left<span class="co">[</span><span class="ot">\begin{array}{cc}  L_L &amp; B \\ B^\top &amp; L_N \end{array}\right</span><span class="co">]</span> \, </span>
<span id="cb16-1451"><a href="#cb16-1451" aria-hidden="true" tabindex="-1"></a>\left[\begin{array}{c}</span>
<span id="cb16-1452"><a href="#cb16-1452" aria-hidden="true" tabindex="-1"></a>x_L <span class="sc">\\</span> x_N \end{array}\right] <span class="sc">\\</span></span>
<span id="cb16-1453"><a href="#cb16-1453" aria-hidden="true" tabindex="-1"></a>&amp; = \frac12 (x_L^\top L_L x_L + 2 x_N^\top B^\top x_L + x_N^\top L_N x_N),</span>
<span id="cb16-1454"><a href="#cb16-1454" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb16-1455"><a href="#cb16-1455" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1456"><a href="#cb16-1456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1457"><a href="#cb16-1457" aria-hidden="true" tabindex="-1"></a>where we use the subscript ${\cdot}_L$ to indicate the landmark points, and the</span>
<span id="cb16-1458"><a href="#cb16-1458" aria-hidden="true" tabindex="-1"></a>subscript ${\cdot}_N$ to indicate the non-landmark points. Differentiating</span>
<span id="cb16-1459"><a href="#cb16-1459" aria-hidden="true" tabindex="-1"></a>$D<span class="co">[</span><span class="ot">x_N</span><span class="co">]</span>$ with respect to $x_N$ and finding its critical points amounts to</span>
<span id="cb16-1460"><a href="#cb16-1460" aria-hidden="true" tabindex="-1"></a>solving the linear systems </span>
<span id="cb16-1461"><a href="#cb16-1461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1462"><a href="#cb16-1462" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1463"><a href="#cb16-1463" aria-hidden="true" tabindex="-1"></a>L_N x_N = −B^\top.</span>
<span id="cb16-1464"><a href="#cb16-1464" aria-hidden="true" tabindex="-1"></a>$$ {#eq-appendixb1}</span>
<span id="cb16-1465"><a href="#cb16-1465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1466"><a href="#cb16-1466" aria-hidden="true" tabindex="-1"></a>Please note that in this linear system, $B^\top$ is a matrix</span>
<span id="cb16-1467"><a href="#cb16-1467" aria-hidden="true" tabindex="-1"></a>containing the columns from the graph Laplacian $L$ that correspond to</span>
<span id="cb16-1468"><a href="#cb16-1468" aria-hidden="true" tabindex="-1"></a>the landmark points (excluding the rows that correspond to landmark</span>
<span id="cb16-1469"><a href="#cb16-1469" aria-hidden="true" tabindex="-1"></a>points). After normalization of the solutions to the systems $X_N$,</span>
<span id="cb16-1470"><a href="#cb16-1470" aria-hidden="true" tabindex="-1"></a>the column vectors of $X_N$ contain the probability that a random walk</span>
<span id="cb16-1471"><a href="#cb16-1471" aria-hidden="true" tabindex="-1"></a>initiated from a non-landmark point terminates in a landmark</span>
<span id="cb16-1472"><a href="#cb16-1472" aria-hidden="true" tabindex="-1"></a>point. One should note that the linear system in </span>
<span id="cb16-1473"><a href="#cb16-1473" aria-hidden="true" tabindex="-1"></a>@eq-appendixb1 is only nonsingular if the graph is completely</span>
<span id="cb16-1474"><a href="#cb16-1474" aria-hidden="true" tabindex="-1"></a>connected, or if each connected component in the graph contains at</span>
<span id="cb16-1475"><a href="#cb16-1475" aria-hidden="true" tabindex="-1"></a>least one landmark point @biggs:algebraic.</span>
<span id="cb16-1476"><a href="#cb16-1476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1477"><a href="#cb16-1477" aria-hidden="true" tabindex="-1"></a>Because we are interested in the probability of a random walk</span>
<span id="cb16-1478"><a href="#cb16-1478" aria-hidden="true" tabindex="-1"></a>initiated from a landmark point terminating at another landmark point,</span>
<span id="cb16-1479"><a href="#cb16-1479" aria-hidden="true" tabindex="-1"></a>we duplicate all landmark points in the neighborhood graph, and</span>
<span id="cb16-1480"><a href="#cb16-1480" aria-hidden="true" tabindex="-1"></a>initiate the random walks from the duplicate landmarks. Because of</span>
<span id="cb16-1481"><a href="#cb16-1481" aria-hidden="true" tabindex="-1"></a>memory constraints, it is not possible to store the entire matrix</span>
<span id="cb16-1482"><a href="#cb16-1482" aria-hidden="true" tabindex="-1"></a>$X_N$ into memory (note that we are only interested in a small number</span>
<span id="cb16-1483"><a href="#cb16-1483" aria-hidden="true" tabindex="-1"></a>of rows from this matrix, viz., in the rows corresponding to the</span>
<span id="cb16-1484"><a href="#cb16-1484" aria-hidden="true" tabindex="-1"></a>duplicate landmark points).  Hence, we solve the linear systems</span>
<span id="cb16-1485"><a href="#cb16-1485" aria-hidden="true" tabindex="-1"></a>defined by the columns of $−B^\top$ one-by-one, and store only the</span>
<span id="cb16-1486"><a href="#cb16-1486" aria-hidden="true" tabindex="-1"></a>parts of the solutions that correspond to the duplicate landmark</span>
<span id="cb16-1487"><a href="#cb16-1487" aria-hidden="true" tabindex="-1"></a>points. For computational reasons, we first perform a Cholesky</span>
<span id="cb16-1488"><a href="#cb16-1488" aria-hidden="true" tabindex="-1"></a>factorization of $L_N$, such that $L_N = C C^\top$, where $C$ is an</span>
<span id="cb16-1489"><a href="#cb16-1489" aria-hidden="true" tabindex="-1"></a>upper-triangular matrix. Subsequently, the solution to the linear</span>
<span id="cb16-1490"><a href="#cb16-1490" aria-hidden="true" tabindex="-1"></a>system in Equation @eq-appendixb1 is obtained by solving the</span>
<span id="cb16-1491"><a href="#cb16-1491" aria-hidden="true" tabindex="-1"></a>linear systems $Cy = −B^\top$ and $C x_N = y$ using a fast</span>
<span id="cb16-1492"><a href="#cb16-1492" aria-hidden="true" tabindex="-1"></a>backsubstitution method.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<script>
for (const element of document.getElementsByClassName("pseudocode")){
    pseudocode.renderElement(element);
}
</script>



</body></html>